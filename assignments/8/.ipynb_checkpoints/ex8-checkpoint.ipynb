{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import takewhile\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# using nltk tokenizer. not satisfied with it. change in review if needed\n",
    "def tokenize(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "#     return re.findall(\"[a-z]+\", text.replace(\"'\", '').lower())\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'a', 'of', 'in', 'and', 's', 'on', 'for', '39', 'that', 'with', 'as', 'at', 'its', 'is', 'new', 'by', 'it', 'said', 'has', 'from', 'reuters', 'an', 'ap', 'his', 'will', 'after', 'was', 'us', 'be', 'over', 'have', 'up', 'their', 'two', 'first', 'are', 'year', 'quot', 'but', 'more', 'he', 'world', 'u', 'this', '1', 'one', 'company', 'monday', 'oil', 'out', 'wednesday', 'tuesday', 'thursday', 'not', 'against', 'friday', 'inc', 'than', '2', 'into', 'last', 'they', 'about', 'iraq', 'york', 'yesterday', 'who', 'three', 'president', 'no', 'microsoft', 'were', 'game', 'million', 'week', 't', 'been', 'time', 'says', 'had', 'corp', 'united', 'when', 'sunday', 'prices', 'government', 'could', 'would', '3', 'security', 'years', 'group', 'today', 'people', 'off', 'which', 'may', 'second']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'exercise8_corpora/ag_news_csv_cleaned/'\n",
    "train_name = file_path + 'train_cleaned.csv'\n",
    "test_name = file_path + 'test_cleaned.csv'\n",
    "\n",
    "train = pd.read_csv(train_name, header=None)\n",
    "# train = train.iloc[0:100]\n",
    "\n",
    "train.columns = ['class', 'doc']\n",
    "train.doc = train.doc.apply(lambda x: tokenize(x))\n",
    "\n",
    "tokenized_docs = [item for sublist in train.doc.tolist() for item in sublist]\n",
    "total_words = len(tokenized_docs)\n",
    "words = Counter(tokenized_docs)\n",
    "vocab = list(dict(takewhile(lambda i: i[1] > 1, words.most_common())))\n",
    "\n",
    "print(vocab[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205478\n"
     ]
    }
   ],
   "source": [
    "print(words['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "[3 4 2 1]\n",
      "3\n",
      "4\n",
      "2\n",
      "1\n",
      "49041\n"
     ]
    }
   ],
   "source": [
    "D = len(train)\n",
    "print(D)\n",
    "\n",
    "classes = train['class'].unique()\n",
    "print(classes)\n",
    "D_k = {}\n",
    "class_docs = {}\n",
    "class_words = {}\n",
    "for c in classes:\n",
    "    print(c)\n",
    "    class_docs[c] = train[train['class'] == c]\n",
    "    D_k[c] = len(class_docs[c])\n",
    "#     print(D_k[c])\n",
    "    tokenized_words = [item for sublist in class_docs[c].doc.tolist() for item in sublist]\n",
    "#     print(len(tokenized_words))\n",
    "    class_words[c] = Counter(tokenized_words)\n",
    "#     print(list(class_words.items())[0:100])\n",
    "\n",
    "# count of the word 'the' in all docs having class == 1\n",
    "print(class_words[1]['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.25\n",
      "4\n",
      "0.25\n",
      "2\n",
      "0.25\n",
      "1\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "priors = {}\n",
    "for c in D_k:\n",
    "    priors[c] = D_k[c]/ D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03982672802976224\n",
      "0.016860546833015022\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "likelihoods = {}\n",
    "for c in classes:\n",
    "    likelihoods[c] = {}\n",
    "    denominator = len(vocab)\n",
    "    for word in vocab:\n",
    "        denominator += class_words[c][word]\n",
    "    for word in vocab:\n",
    "        likelihoods[c][word] = (class_words[c][word] +1) / denominator\n",
    "        \n",
    "print(likelihoods[1]['the'])\n",
    "print(likelihoods[2]['of'])\n",
    "print(likelihoods[2]['farmer'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(class_words[2]['farmer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB_Classifier():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [item for sublist in train.doc.tolist() for item in sublist]\n",
    "print(tokenized_docs[0:100])\n",
    "total_words = len(tokenized_docs)\n",
    "words = Counter(tokenized_docs)\n",
    "vocab = dict(takewhile(lambda i: i[1] > 1, words.most_common()))\n",
    "\n",
    "classes = train['class'].unique()\n",
    "documents = pd.DataFrame(train.doc.tolist())\n",
    "\n",
    "p_cs = {}\n",
    "class_docs = {}\n",
    "for c in classes:\n",
    "    docs = train[train['class'] == c]\n",
    "    class_docs[c] = docs\n",
    "    p_cs[c] = len(docs) / len(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
