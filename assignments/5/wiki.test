Politics  refers to a set of activities associated with the governance of a country, or an area.
It involves making decisions that apply to members of a group.
It refers to achieving and exercising positions of governance—organized control over a human community, particularly a state.
[2] The academic study focusing on just politics, which is therefore more targeted than general political science, is sometimes referred to as politology(not to be confused with politicology, a synonym for political science).
In modern nation-states, people have formed political parties to represent their ideas.
They agree to take the same position on many issues and agree to support the same changes to law and the same leaders.
An election is usually a competition between different parties.
[4] Some examples of political parties worldwide are: the African National Congress (ANC) in South Africa, the Democratic Party (D) in the United States, the Conservative Party in the United Kingdom, the Christian Democratic Union (CDU) in Germany and the Indian National Congress in India.
Politics is a multifaceted word.
It has a set of fairly specific meanings that are descriptive and nonjudgmental (such as "the art or science of government" and "political principles"), but does often colloquially carry a negative connotation.
[1][5][6] The word has been used negatively for many years: the British national anthem as published in 1745 calls on God to "Confound their politics",[7] and the phrase "play politics", for example, has been in use since at least 1853, when abolitionist Wendell Phillips declared: "We do not play politics; anti-slavery is no half-jest with us.
A variety of methods are deployed in politics, which include promoting one's own political views among people, negotiation with other political subjects, making laws, and exercising force, including warfare against adversaries.
[9][10][11][12][13] Politics is exercised on a wide range of social levels, from clans and tribes of traditional societies, through modern local governments, companies and institutions up to sovereign states, to the international level.
A political system is a framework which defines acceptable political methods within a given society.
The history of political thought can be traced back to early antiquity, with seminal works such as Plato's Republic, Aristotle's Politics and the works of Confucius.
The word comes from the same Greek word from which the title of Aristotle's book Politics (from Ancient Greek: Πολιτικά, romanized: Politiká or Polis, meaning "affairs of the cities").
The book title was rendered in Early Modern English in the mid-15th century as "Polettiques";[14] it became "politics" in Modern English.
The singular politic first attested in English 1430 and comes from Middle French politique, in turn from Latin politicus,[15] which is the Latinization of the Greek πολιτικός (politikos), meaning amongst others "of, for, or relating to citizens", "civil", "civic", "belonging to the state",[16] in turn from πολίτης (polites), "citizen"[16] and that from πόλις (polis), "city".
Formal politics refers to the operation of a constitutional system of government and publicly defined institutions and procedures.
[17] Political parties, public policy or discussions about war and foreign affairs would fall under the category of Formal Politics.
[17] Many people view formal politics as something outside of themselves, but that can still affect their daily lives.
Semi-formal politics is politics in government associations such as neighborhood associations, or student governments where student government political party politics is often important.
Informal politics is understood as forming alliances, exercising power and protecting and advancing particular ideas or goals.
Generally, this includes anything affecting one's daily life, such as the way an office or household is managed, or how one person or group exercises influence over another.
[17] Informal Politics is typically understood as everyday politics, hence the idea that "politics is everywhere".
The history of politics is reflected in the origin, development, and economics of the institutions of government.
The origin of the state is to be found in the development of the art of warfare.
Historically speaking, all political communities of the modern type owe their existence to successful warfare.
Kings, emperors and other types of monarchs in many countries including China and Japan, were considered divine.
Of the institutions that ruled states, that of kingship stood at the forefront until the American  Revolution put an end to the "divine right of kings".
Nevertheless, the monarchy is among the longest-lasting political institutions, dating as early as 2100 BC in Sumeria[19] to the 21st century AD British Monarchy.
Kingship becomes an institution through the institution of hereditary monarchy.
The king often, even in absolute monarchies, ruled his kingdom with the aid of an elite group of advisors, a council without which he could not maintain power.
As these advisors and others outside the monarchy negotiated for power, constitutional monarchies emerged, which may be considered the germ of constitutional government.
The greatest of the king's subordinates, the earls and dukes in England and Scotland, the dukes and counts in the Continent, always sat as a right on the council.
A conqueror wages war upon the vanquished for vengeance or for plunder but an established kingdom exacts tribute.
One of the functions of the council is to keep the coffers of the king full.
Another is the satisfaction of military service and the establishment of lordships by the king to satisfy the task of collecting taxes and soldiers.
There are many forms of political organization, including states, non-government organizations (NGOs) and international organizations such as the United Nations.
States are perhaps the predominant institutional form of political governance, where a state is understood as an institution and a government is understood as the regime in power.
According to Aristotle, states are classified into monarchies, aristocracies, timocracies, democracies, oligarchies, and tyrannies.
Due to changes across the history of politics, this classification has been abandoned.
All states are varieties of a single organizational form, the sovereign state.
All the great powers of the modern world rule on the principle of sovereignty.
Sovereign power may be vested on an individual as in an autocratic government or it may be vested on a group as in a constitutional government.
Constitutions are written documents that specify and limit the powers of the different branches of government.
Although a constitution is a written document, there is also an unwritten constitution.
The unwritten constitution is continually being written by the legislative and judiciary branch of government; this is just one of those cases in which the nature of the circumstances determines the form of government that is most appropriate.
[23] England did set the fashion of written constitutions during the Civil War but after the Restoration abandoned them to be taken up later by the American Colonies after their emancipation and then France after the Revolution and the rest of Europe including the European colonies.
There are many forms of government.
One form is a strong central government as in France and China.
Another form is local government, such as the ancient divisions in England that are comparatively weaker but less bureaucratic.
These two forms helped to shape the practice of federal government, first in Switzerland, then in the United States in 1776, in Canada in 1867 and in Germany in 1871 and in 1901, Australia.
Federal states introduced the new principle of agreement or contract.
Compared to a federation, a confederation has a more dispersed system of judicial power.
[24] In the American Civil War, the argument by the Confederate States that a State could secede from the Union was deemed inconstitutional by the supreme court.
According to professor A. V. Dicey in An Introduction to the Study of the Law of the Constitution, the essential features of a federal constitution are: a) A written supreme constitution in order to prevent disputes between the jurisdictions of the Federal and State authorities; b) A distribution of power between the Federal and State governments and c) A Supreme Court vested with the power to interpret the Constitution and enforce the law of the land remaining independent of both the executive and legislative branches.
Global politics include different practices of political globalization in relation to questions of social power: from global patterns of governance to issues of globalizing conflict.
The 20th century witnessed the outcome of two world wars and not only the rise and fall of the Third Reich but also the rise and relative fall of communism.
The development of the atomic bomb gave the United States a more rapid end to its conflict in Japan in World War II.
Later, the hydrogen bomb became the ultimate weapon of mass destruction.
Global politics also concerns the rise of global and international organizations.
The United Nations has served as a forum for peace in a world threatened by nuclear war, "The invention of nuclear and space weapons has made war unacceptable as an instrument for achieving political ends.
"[27] Although an all-out final nuclear holocaust is radically undesirable for man, "nuclear blackmail" comes into question not only on the issue of world peace but also on the issue of national sovereignty.
[28] On a Sunday in 1962, the world stood still at the brink of nuclear war during the October Cuban Missile Crisis from the implementation of U.S. vs Soviet Union nuclear blackmail policy.
According to political science professor Paul James, global politics is affected by values: norms of human rights, ideas of human development, and beliefs such as cosmopolitanism about how we should relate to each:
Cosmopolitanism can be defined as a global politics that, firstly, projects a sociality of common political engagement among all human beings across the globe, and, secondly, suggests that this sociality should be either ethically or organizationally privileged over other forms of sociality.
William Pitt the Elder, speaking before the British House of Lords, 9 January 1770, observed: "Unlimited power is apt to corrupt the minds of those who possess it.
"[30] This was echoed more famously by John Dalberg-Acton over a century later: "Power tends to corrupt, and absolute power corrupts absolutely.
Political corruption is the use of legislated powers by government officials for illegitimate private gain.
Misuse of government power for other purposes, such as repression of political opponents and general police brutality, is not considered political corruption.
Neither are illegal acts by private persons or corporations not directly involved with the government.
An illegal act by an officeholder constitutes political corruption only if the act is directly related to their official duties and/or power.
[32] The corruption in third World dictatorships is usually more blatant.
For example, government cronies may be given exclusive right to make arbitrage profit by exploiting a fixed rate mechanism in government currency.
In democracies corruption is often more indirect.
Trade union leaders may be given priority in housing queues, giving them indirectly a worth of millions.
Forms of corruption vary, but include corruption, extortion, cronyism, nepotism, patronage, graft, and embezzlement.
While corruption may facilitate criminal enterprise it may be legal but considered immoral.
[34] Worldwide, bribery alone is estimated to involve over 1 trillion US dollars annually.
[35] A state of unrestrained political corruption is known as a kleptocracy, literally meaning "rule by thieves".
A political party is a political organization that typically seeks to attain and maintain political power within government, usually by participating in electoral campaigns, educational outreach or protest actions.
Parties often espouse an expressed ideology or vision bolstered by a written platform with specific goals, forming a coalition among disparate interests.
Political science, the study of politics, examines the acquisition and application of power.
[38] Political scientist Harold Lasswell defined politics as "who gets what, when, and how".
[39] Related areas of study include political philosophy, which seeks a rationale for politics and an ethic of public behaviour, as well as examining the preconditions for the formation of political communities;[40] political economy, which attempts to develop understandings of the relationships between politics and the economy and the governance of the two; and public administration, which examines the practices of governance.
[41] The philosopher Charles Blattberg, who has defined politics as "responding to conflict with dialogue," offers an account which distinguishes political philosophies from political ideologies.
The first academic chair devoted to politics in the United States was the chair of history and political science at Columbia University, first occupied by Prussian émigré Francis Lieber in 1857.
Several different political spectra have been proposed.
Political analysts and politicians divide politics into left wing and right wing politics, often also using the idea of center politics as a middle path of policy between the right and left.
This classification is comparatively recent (it was not used by Aristotle or Hobbes, for instance), and dates from the French Revolution era, when those members of the National Assembly who supported the republic, the common people and a secular society sat on the left and supporters of the monarchy, aristocratic privilege and the Church sat on the right.
The meanings behind the labels have become more complicated over the years.
A particularly influential event was the publication of the Communist Manifesto by Karl Marx and Friedrich Engels in 1848.
The Manifesto suggested a course of action for a proletarian revolution to overthrow the bourgeois society and abolish private property, in the belief that this would lead to a classless and stateless society.
[45][46][page needed]
The meaning of left-wing and right-wing varies considerably between different countries and at different times, but generally speaking, it can be said that the right wing often values tradition and inequality while the left wing often values progress and egalitarianism, with the center seeking a balance between the two such as with social democracy, libertarianism or regulated capitalism.
According to Norberto Bobbio, one of the major exponents of this distinction, the Left believes in attempting to eradicate social inequality – believing it to be unethical or unnatural[48] while the Right regards most social inequality as the result of ineradicable natural inequalities, and sees attempts to enforce social equality as utopian or authoritarian.
Some ideologies, notably Christian Democracy, claim to combine left and right wing politics; according to Geoffrey K. Roberts and Patricia Hogwood, "In terms of ideology, Christian Democracy has incorporated many of the views held by liberals, conservatives and socialists within a wider framework of moral and Christian principles.
"[50] Movements which claim or formerly claimed to be above the left-right divide include Fascist Terza Posizione economic politics in Italy and Peronism in Argentina.
Authoritarianism and libertarianism refer to the amount of individual freedom each person possesses in that society relative to the state.
One author describes authoritarian political systems as those where "individual rights and goals are subjugated to group goals, expectations and conformities",[53] while libertarians generally oppose the state and hold the individual as sovereign.
In their purest form, libertarians are anarchists [54], who argue for the total abolition of the state, of political parties and of other political entities, while the purest authoritarians are, by definition, totalitarians who support state control over all aspects of society.
For instance, classical liberalism (also known as laissez-faire liberalism,[56]) is a doctrine stressing individual freedom and limited government.
This includes the importance of human rationality, individual property rights, free markets, natural rights, the protection of civil liberties, constitutional limitation of government, and individual freedom from restraint as exemplified in the writings of John Locke, Adam Smith, David Hume, David Ricardo, Voltaire, Montesquieu and others.
According to the libertarian Institute for Humane Studies, "the libertarian, or 'classical liberal,' perspective is that individual well-being, prosperity, and social harmony are fostered by 'as much liberty as possible' and 'as little government as necessary.
'"[57] For anarchist political philosopher L. Susan Brown "Liberalism and anarchism are two political philosophies that are fundamentally concerned with individual freedom yet differ from one another in very distinct ways.
Anarchism shares with liberalism a radical commitment to individual freedom while rejecting liberalism's competitive property relations.
A military is a heavily-armed, highly-organised force primarily intended for warfare, also known collectively as armed forces.
It is typically officially authorized and maintained by a sovereign state, with its members identifiable by their distinct military uniform.
It may consist of one or more military branches such as an Army, Navy, Air Force and in certain countries, Marines and Coast Guard.
The main task of the military is usually defined as defence of the state and its interests against external armed threats.
Beyond warfare, the military may be employed in additional sanctioned and non-sanctioned functions within the state, including internal security threats, population control, the promotion of a political agenda, emergency services and reconstruction, protecting corporate economic interests, social ceremonies and national honor guards.
A nation's military may function as a discrete social subculture, with dedicated infrastructure such as military housing, schools, utilities, logistics, hospitals, legal services, food production, finance, and banking services.
In broad usage, the terms "armed forces" and "military" are often treated as synonymous, although in technical usage a distinction is sometimes made in which a country's armed forces may include both its military and other paramilitary forces.
There are various forms of irregular military forces, not belonging to a recognized state; though they share many attributes with regular military forces, they are less often referred to as simply "military".
The profession of soldiering as part of a military is older than recorded history itself.
[1] Some of the most enduring images of  classical antiquity portray the power and feats of its military leaders.
The Battle of Kadesh in 1274 BC was one of the defining points of Pharaoh Ramses II's reign, and his monuments commemorate it in bas-relief.
A thousand years later, the first emperor of unified China, Qin Shi Huang, was so determined to impress the gods with his military might that he had himself buried with an army of  terracotta soldiers.
The  Romans paid considerable attention to military matters, leaving to posterity many treatises and writings on the subject, as well as a large number of lavishly carved triumphal arches and victory columns.
The first recorded use of the word military in English, spelled militarie, was in 1582.
[3] It comes from the Latin militaris (from Latin miles, meaning "soldier") through French, but is of uncertain etymology, one suggestion being derived from *mil-it- – going in a body or mass.
[4][5] The word is now identified as denoting someone that is skilled in use of weapons, or engaged in military service, or in warfare.
As a noun, the military usually refers generally to a country's armed forces, or sometimes, more specifically, to the senior officers who command them.
[3][6] In general, it refers to the physicality of armed forces, their personnel, equipment, and the physical area which they occupy.
As an adjective, military originally referred only to soldiers and soldiering, but it soon broadened to apply to land forces in general, and anything to do with their profession.
[3] The names of both the Royal Military Academy (1741) and United States Military Academy (1802) reflect this.
However, at about the time of the Napoleonic Wars, 'military' began to be used in reference to armed forces as a whole,[3] and in the 21st century expressions like 'military service', 'military intelligence', and 'military history' encompass naval and air force aspects.
As such, it now connotes any activity performed by armed force personnel.
Military history is often considered to be the history of all conflicts, not just the history of the state militaries.
It differs somewhat from the history of war, with military history focusing on the people and institutions of war-making, while the history of war focuses on the evolution of war itself in the face of changing technology, governments, and geography.
Military history has a number of facets.
One main facet is to learn from past accomplishments and mistakes, so as to more effectively wage war in the future.
Another is to create a sense of military tradition, which is used to create cohesive military forces.
Still, another may be to learn to prevent wars more effectively.
Human knowledge about the military is largely based on both recorded and oral history of military conflicts (war), their participating armies and navies and, more recently, air forces.
There are two types of military history, although almost all texts have elements of both: descriptive history, that serves to chronicle conflicts without offering any statements about the causes, nature of conduct, the ending, and effects of a conflict; and analytical history, that seeks to offer statements about the causes, nature, ending, and aftermath of conflicts – as a means of deriving knowledge and understanding of conflicts as a whole, and prevent repetition of mistakes in future, to suggest better concepts or methods in employing forces, or to advocate the need for new technology.
Despite the growing importance of military technology, military activity depends above all on people.
For example, in 2000 the British Army declared: "Man is still the first weapon of war.
The military organization is characterized by a strict hierarchy divided by military rank, with ranks normally grouped (in descending order of authority) as officers (e.g.
Colonel), non-commissioned officers (e.g.
Sergeant), and personnel at the lowest rank (e.g.
Private Soldier).
While senior officers make strategic decisions, subordinated military personnel (soldiers, sailors, marines, or airmen) fulfil them.
Although rank titles vary by military branch and country, the rank hierarchy is common to all state armed forces worldwide.
In addition to their rank, personnel occupy one of many trade roles, which are often grouped according to the nature of the role's military task on combat operations: combat roles (e.g.
infantry), combat support roles (e.g.
combat engineers), and combat service support roles (e.g.
logistical support).
Personnel may be recruited or conscripted, depending on the system chosen by the state.
Most military personnel are males; the minority proportion of female personnel varies internationally (approximately 3% in India,[8] 10% in the UK,[9] 13% in Sweden,[10] 16% in the US,[11] and 27% in South Africa[12]).
While two-thirds of states now recruit or conscript only adults, as of 2017 50 states still relied partly on children under the age of 18 (usually aged 16 or 17) to staff their armed forces.
Whereas recruits who join as officers tend to be upwardly-mobile,[14][15] most enlisted personnel have a childhood background of relative socio-economic deprivation.
[16][17][18] For example, after the US suspended conscription in 1973, "the military disproportionately attracted African American men, men from lower-status socioeconomic backgrounds, men who had been in nonacademic high school programs, and men whose high school grades tended to be low".
The obligations of military employment are many.
Full-time military employment normally requires a minimum period of service of several years; between two and six years is typical of armed forces in Australia, the UK and the US, for example, depending on role, branch, and rank.
[19][20][21] Some armed forces allow a short discharge window, normally during training, when recruits may leave the armed force as of right.
[22] Alternatively, part-time military employment, known as reserve service, allows a recruit to maintain a civilian job while training under military discipline at weekends; he or she may be called out to deploy on operations to supplement the full-time personnel complement.
After leaving the armed forces, recruits may remain liable for compulsory return to full-time military employment in order to train or deploy on operations.
Military law introduces offences not recognised by civilian courts, such as absence without leave (AWOL), desertion, political acts, malingering, behaving disrespectfully, and disobedience (see, for example, Offences against military law in the United Kingdom).
[23] Penalties range from a summary reprimand to imprisonment for several years following a court martial.
[23] Certain fundamental rights are also restricted or suspended, including the freedom of association (e.g.
union organizing) and freedom of speech (speaking to the media).
[23] Military personnel in some countries have a right of conscientious objection if they believe an order is immoral or unlawful, or cannot in good conscience carry it out.
Personnel may be posted to bases in their home country or overseas, according to operational need, and may be deployed from those bases on exercises or operations anywhere in the world.
During peacetime, when military personnel are generally stationed in garrisons or other permanent military facilities, they mostly conduct administrative tasks, training and education activities, technology maintenance, and recruitment.
Initial training conditions recruits for the demands of military life, including preparedness to injure and kill other people, and to face mortal danger without fleeing.
It is a physically and psychologically intensive process which resocializes recruits for the unique nature of military demands.
For example:
The next requirement comes as a fairly basic need for the military to identify possible threats it may be called upon to face.
For this purpose, some of the commanding forces and other military, as well as often civilian personnel participate in identification of these threats.
This is at once an organisation, a system and a process collectively called military intelligence (MI).
The difficulty in using military intelligence concepts and military intelligence methods is in the nature of the secrecy of the information they seek, and the clandestine nature that intelligence operatives work in obtaining what may be plans for a conflict escalation, initiation of combat, or an invasion.
An important part of the military intelligence role is the military analysis performed to assess military capability of potential future aggressors, and provide combat modelling that helps to understand factors on which comparison of forces can be made.
This helps to quantify and qualify such statements as: "China and India maintain the largest armed forces in the World" or that "the U.S. Military is considered to be the world's strongest".
Although some groups engaged in combat, such as militants or resistance movements, refer to themselves using military terminology, notably 'Army' or 'Front', none have had the structure of a national military to justify the reference, and usually have had to rely on support of outside national militaries.
They also use these terms to conceal from the MI their true capabilities, and to impress potential ideological recruits.
Having military intelligence representatives participate in the execution of the national defence policy is important, because it becomes the first respondent and commentator on the policy expected strategic goal, compared to the realities of identified threats.
When the intelligence reporting is compared to the policy, it becomes possible for the national leadership to consider allocating resources over and above the officers and their subordinates military pay, and the expense of maintaining military facilities and military support services for them.
Defense economics is the financial and monetary efforts made to resource and sustain militaries, and to finance military operations, including war.
The process of allocating resources is conducted by determining a military budget, which is administered by a military finance organisation within the military.
Military procurement is then authorised to purchase or contract provision of goods and services to the military, whether in peacetime at a permanent base, or in a combat zone from local population.
Capability development, which is often referred to as the military 'strength', is arguably one of the most complex activities known to humanity; because it requires determining: strategic, operational, and tactical capability requirements to counter the identified threats; strategic, operational, and tactical doctrines by which the acquired capabilities will be used; identifying concepts, methods, and systems involved in executing the doctrines; creating design specifications for the manufacturers who would produce these in adequate quantity and quality for their use in combat; purchase the concepts, methods, and systems; create a forces structure that would use the concepts, methods, and systems most effectively and efficiently; integrate these concepts, methods, and systems into the force structure by providing military education, training, and practice that preferably resembles combat environment of intended use; create military logistics systems to allow continued and uninterrupted performance of military organisations under combat conditions, including provision of health services to the personnel, and maintenance for the equipment; the services to assist recovery of wounded personnel, and repair of damaged equipment; and finally, post-conflict demobilisation, and disposal of war stocks surplus to peacetime requirements.
Development of military doctrine is perhaps the more important of all capability development activities, because it determines how military forces were, and are used in conflicts, the concepts and methods used by the command to employ appropriately military skilled, armed and equipped personnel in achievement of the tangible goals and objectives of the war, campaign, battle, engagement, action or a duel.
[31] The line between strategy and tactics is not easily blurred, although deciding which is being discussed had sometimes been a matter of personal judgement by some commentators, and military historians.
The use of forces at the level of organisation between strategic and tactical is called operational mobility.
There have been attempts to produce a military strength index: this is an example taken from a Credit Suisse report in September 2015.
[32] The factors under consideration for that military strength indicator and their total weights were: number of active personnel in the armed forces (5%), tanks (10%), attack helicopters (15%), aircraft (20%), aircraft carriers (25%), and submarines (25%).
It was practically impossible to make an estimation of the actual training of the armed forces.
These were the results:
Because most of the concepts and methods used by the military, and many of its systems are not found in commercial branches, much of the material is researched, designed, developed, and offered for inclusion in arsenals by military science organisations within the overall structure of the military.
Military scientists are therefore found to interact with all Arms and Services of the armed forces, and at all levels of the military hierarchy of command.
Although concerned with research into military psychology, particularly combat stress and how it affect troop morale, often the bulk of military science activities is directed at military intelligence technology, military communications, and improving military capability through research.
The design, development, and prototyping of weapons, military support equipment, and military technology in general, is also an area in which lots of effort is invested – it includes everything from global communication networks and aircraft carriers to paint and food.
Possessing military capability is not sufficient if this capability cannot be deployed for, and employed in combat operations.
To achieve this, military logistics are used for the logistics management and logistics planning of the forces military supply chain management, the consumables, and capital equipment of the troops.
Although mostly concerned with the military transport, as a means of delivery using different modes of transport; from military trucks, to container ships operating from permanent military base, it also involves creating field supply dumps at the rear of the combat zone, and even forward supply points in specific unit's Tactical Area of Responsibility.
These supply points are also used to provide military engineering services, such as the recovery of defective and derelict vehicles and weapons, maintenance of weapons in the field, the repair and field modification of weapons and equipment; and in peacetime, the life-extension programmes undertaken to allow continued use of equipment.
One of the most important role of logistics is the supply of munitions as a primary type of consumable, their storage, and disposal.
While capability development is about enabling the military to perform its functions and roles in executing the defence policy, how personnel and their equipment are used in engaging the enemy, winning battles, successfully concluding campaigns, and eventually the war – is the responsibility of military operations.
Military operations oversees the policy interpretation into military plans, allocation of capability to specific strategic, operational and tactical goals and objectives, change in posture of the armed forces, the interaction of Combat Arms, Combat Support Arms, and Combat Support Services during combat operations, defining of military missions and tasks during the conduct of combat, management of prisoners of war, military civil affairs, and the military occupation of enemy territory, seizure of captured equipment, and maintenance of civil order in the territory under its responsibility.
Throughout the combat operations process, and during the lulls in combat, combat military intelligence provides reporting on the status of plan completion, and its correlation with desired, expected and achieved satisfaction of policy fulfilment.
The last requirement of the military is for military performance assessment, and learning from it.
These two functions are performed by military historians and military theorists who seek to identify failures and success of the armed force, and integrate corrections into the military reform, with the aim of producing an improved force capable of performing adequately, should there be a national defence policy review.
The primary reason for the existence of the military is to engage in combat, should it be required to do so by the national defence policy, and to win.
This represents an organisational goal of any military, and the primary focus for military thought through military history.
How victory is achieved, and what shape it assumes, is studied by most, if not all, military groups on three levels.
Military strategy is the management of forces in wars and military campaigns by a commander-in-chief, employing large military forces, either national and allied as a whole, or the component elements of armies, navies and air forces; such as army groups, naval fleets, and large numbers of aircraft.
Military strategy is a long-term projection of belligerents' policy, with a broad view of outcome implications, including outside the concerns of military command.
Military strategy is more concerned with the supply of war and planning, than management of field forces and combat between them.
The scope of strategic military planning can span weeks, but is more often months or even years.
Operational mobility is, within warfare and military doctrine, the level of command which coordinates the minute details of tactics with the overarching goals of strategy.
A common synonym is operational art.
The operational level is at a scale bigger than one where line of sight and the time of day are important, and smaller than the strategic level, where production and politics are considerations.
Formations are of the operational level if they are able to conduct operations on their own, and are of sufficient size to be directly handled or have a significant impact at the strategic level.
This concept was pioneered by the German army prior to and during the Second World War.
At this level, planning and duration of activities takes from one week to a month, and are executed by Field Armies and Army Corps and their naval and air equivalents.
Military tactics concerns itself with the methods for engaging and defeating the enemy in direct combat.
Military tactics are usually used by units over hours or days, and are focused on the specific, close proximity tasks and objectives of squadrons, companies, battalions, regiments, brigades, and divisions, and their naval and air force equivalents.
One of the oldest military publications is The Art of War, by the Chinese philosopher Sun Tzu.
[34] Written in the 6th century BCE, the 13-chapter book is intended as military instruction, and not as military theory, but has had a huge influence on Asian military doctrine, and from the late 19th century, on European and United States military planning.
It has even been used to formulate business tactics, and can even be applied in social and political areas.[where?]
The Classical Greeks and the Romans wrote prolifically on military campaigning.
Among the best-known Roman works are Julius Caesar's commentaries on the Gallic Wars, and the Roman Civil war – written about 50 BC.
Two major works on tactics come from the late Roman period: Taktike Theoria by Aelianus Tacticus, and De Re Militari ('On military matters') by Vegetius.
Taktike Theoria examined Greek military tactics, and was most influential in the Byzantine world and during the Golden Age of Islam.
De Re Militari formed the basis of European military tactics until the late 17th century.
Perhaps its most enduring maxim is Igitur qui desiderat pacem, praeparet bellum (let he who desires peace prepare for war).
Due to the changing nature of combat with the introduction of artillery in the European Middle Ages, and infantry firearms in the Renaissance, attempts were made to define and identify those strategies, grand tactics, and tactics that would produce a victory more often than that achieved by the Romans in praying to the gods before the battle.
Later this became known as military science, and later still, would adopt the scientific method approach to the conduct of military operations under the influence of the Industrial Revolution thinking.
In his seminal book On War, the Prussian Major-General and leading expert on modern military strategy, Carl von Clausewitz defined military strategy as 'the employment of battles to gain the end of war'.
[36] According to Clausewitz:
strategy forms the plan of the War, and to this end it links together the series of acts which are to lead to the final decision, that is to say, it makes the plans for the separate campaigns and regulates the combats to be fought in each.
Hence, Clausewitz placed political aims above military goals, ensuring civilian control of the military.
Military strategy was one of a triumvirate of 'arts' or 'sciences' that governed the conduct of warfare, the others being: military tactics, the execution of plans and manoeuvring of forces in battle, and maintenance of an army.
The meaning of military tactics has changed over time; from the deployment and manoeuvring of entire land armies on the fields of ancient battles, and galley fleets; to modern use of small unit ambushes, encirclements, bombardment attacks, frontal assaults, air assaults, hit-and-run tactics used mainly by guerrilla forces, and, in some cases, suicide attacks on land and at sea.
Evolution of aerial warfare introduced its own air combat tactics.
Often, military deception, in the form of military camouflage or misdirection using decoys, is used to confuse the enemy as a tactic.
A major development in infantry tactics came with the increased use of trench warfare in the 19th and 20th centuries.
This was mainly employed in World War I in the Gallipoli campaign, and the Western Front.
Trench warfare often turned to a stalemate, only broken by a large loss of life, because, in order to attack an enemy entrenchment, soldiers had to run through an exposed 'no man's land' under heavy fire from their opposing entrenched enemy.
As with any occupation, since the ancient times, the military has been distinguished from other members of the society by their tools, the military weapons, and military equipment used in combat.
When Stone Age humans first took a sliver of flint to tip the spear, it was the first example of applying technology to improve the weapon.
Since then, the advances made by human societies, and that of weapons, has been irretrievably linked.
Stone weapons gave way to Bronze Age weapons, and later, the Iron Age weapons.
With each technological change, was realised some tangible increase in military capability, such as through greater effectiveness of a sharper edge in defeating leather armour, or improved density of materials used in manufacture of weapons.
On land, the first really significant technological advance in warfare was the development of the ranged weapons, and notably, the sling.
The next significant advance came with the domestication of the horses and mastering of equestrianism.
Arguably, the greatest invention that affected not just the military, but all society, after adoption of fire, was the wheel, and its use in the construction of the chariot.
There were no advances in military technology, until, from the mechanical arm action of a slinger, the Greeks, Egyptians, Romans, Persians, Chinese, etc., developed the siege engines.
The bow was manufactured in increasingly larger and more powerful versions, to increase both the weapon range, and armour penetration performance.
These developed into the powerful composite and recurve bows, and crossbows of Ancient China.
These proved particularly useful during the rise of cavalry, as horsemen encased in ever-more sophisticated armour came to dominate the battlefield.
Somewhat earlier, in medieval China, gunpowder had been invented, and was increasingly used by the military in combat.
The use of gunpowder in the early vase-like mortars in Europe, and advanced versions of the long bow and cross bow, which all had armour-piercing arrowheads, that put an end to the dominance of the armoured knight.
After the long bow, which required great skill and strength to use, the next most significant technological advance was the musket, which could be used effectively, with little training.
In time, the successors to muskets and cannon, in the form of rifles and artillery, would become core battlefield technology.
As the speed of technological advances accelerated in civilian applications, so too warfare became more industrialised.
The newly invented machine gun and repeating rifle redefined firepower on the battlefield, and, in part, explains the high casualty rates of the American Civil War.
The next breakthrough was the conversion of artillery parks from the muzzle loading guns, to the quicker loading breech loading guns with recoiling barrel that allowed quicker aimed fire and use of a shield.
The widespread introduction of low smoke (smokeless) propellant powders since the 1880s also allowed for a great improvement of artillery ranges.
The development of breech loading had the greatest effect on naval warfare, for the first time since the Middle Ages, altering the way weapons are mounted on warships, and therefore naval tactics, now divorced from the reliance on sails with the invention of the internal combustion.
A further advance in military naval technology was the design of the submarine, and its weapon, the torpedo.
Main battle tanks, and other heavy equipment such as armoured fighting vehicles, military aircraft, and ships, are characteristic to organised military forces.
During World War I, the need to break the deadlock of trench warfare saw the rapid development of many new technologies, particularly tanks.
Military aviation was extensively used, and bombers became decisive in many battles of World War II, which marked the most frantic period of weapons development in history.
Many new designs, and concepts were used in combat, and all existing technologies of warfare were improved between 1939 and 1945.
During the war, significant advances were made in military communications through increased use of radio, military intelligence through use of the radar, and in military medicine through use of penicillin, while in the air, the guided missile, jet aircraft, and helicopters were seen for the first time.
Perhaps the most infamous of all military technologies was the creation of the atomic bomb, although the exact effects of its radiation were unknown until the early 1950s.
Far greater use of military vehicles had finally eliminated the cavalry from the military force structure.
After World War II, with the onset of the Cold War, the constant technological development of new weapons was institutionalised, as participants engaged in a constant 'arms race' in capability development.
This constant state of weapons development continues into the present, and remains a constant drain on national resources, which some[who?]
blame on the military-industrial complex.
The most significant technological developments that influenced combat have been the guided missiles, which can be used by all branches of the armed services.
More recently, information technology, and its use in surveillance, including space-based reconnaissance systems, have played an increasing role in military operations.
The impact of information warfare that focuses on attacking command communication systems, and military databases, has been coupled with the new development in military technology, has been the use of robotic systems in intelligence combat, both in hardware and software applications.
Recently, there has also been a particular focus towards the use of renewable fuels for running military vehicles on.
Unlike fossil fuels, renewable fuels can be produced in any country, creating a strategic advantage.
The US military has already committed itself to have 50% of its energy consumption come from alternative sources.
For much of military history, the armed forces were considered to be for use by the heads of their societies, until recently, the crowned heads of states.
In a democracy or other political system run in the public interest, it is a public force.
The relationship between the military and the society it serves is a complicated and ever-evolving one.
Much depends on the nature of the society itself, and whether it sees the military as important, as for example in time of threat or war, or a burdensome expense typified by defence cuts in time of peace.
One difficult matter in the relation between military and society is control and transparency.
In some countries, limited information on military operations and budgeting is accessible for the public.
However transparency in the military sector is crucial to fight corruption.
This showed the Government Defence Anti-corruption Index Transparency International UK published in 2013.
Militaries often function as societies within societies, by having their own military communities, economies, education, medicine, and other aspects of a functioning civilian society.
Although a 'military' is not limited to nations in of itself as many private military companies (or PMC's) can be used or 'hired' by organisations and figures as security, escort, or other means of protection; where police, agencies, or militaries are absent or not trusted.
Militarist ideology is the society's social attitude of being best served, or being a beneficiary of a government, or guided by concepts embodied in the military culture, doctrine, system, or leaders.
Either because of the cultural memory, national history, or the potentiality of a military threat, the militarist argument asserts that a civilian population is dependent upon, and thereby subservient to the needs and goals of its military for continued independence.
Militarism is sometimes contrasted with the concepts of comprehensive national power, soft power and hard power.
Most nations have separate military laws which regulate conduct in war and during peacetime.
An early exponent was Hugo Grotius, whose On the Law of War and Peace (1625) had a major impact of the humanitarian approach to warfare development.
His theme was echoed by Gustavus Adolphus.
Ethics of warfare have developed since 1945, to create constraints on the military treatment of prisoners and civilians, primarily by the Geneva Conventions; but rarely apply to use of the military forces as internal security troops during times of political conflict that results in popular protests and incitement to popular uprising.
International protocols restrict the use, or have even created international bans on some types of weapons, notably weapons of mass destruction (WMD).
International conventions define what constitutes a war crime, and provides for war crimes prosecution.
Individual countries also have elaborate codes of military justice, an example being the United States' Uniform Code of Military Justice that can lead to court martial for military personnel found guilty of war crimes.
Military actions are sometimes argued to be justified by furthering a humanitarian cause, such as disaster relief operations, or in defence of refugees.
The term military humanism is used to refer to such actions.
A military brat is a colloquial term for a child with at least one parent who served as an active duty member (vice reserve) in the armed forces.
Children of armed forces members may move around to different military bases or international postings, which gives them a childhood differing from the norm.
Unlike common usage of the term brat, when it is used in this context, it is not necessarily a derogatory term.
Soldiers and armies have been prominent in popular culture since the beginnings of recorded history.
In addition to the countless images of military leaders in heroic poses from antiquity, they have been an enduring source of inspiration in war literature.
Not all of this has been entirely complementary, and the military have been lampooned or ridiculed as often as they have been idolised.
The classical Greek writer Aristophanes, devoted an entire comedy, Lysistrata, to a strike organised by military wives, where they withhold sex from their husbands to prevent them from going to war.
In Medieval Europe, tales of knighthood and chivalry, the officer class of the period captured the popular imagination.
Writers and poets like Taliesin, Chrétien de Troyes and Thomas Malory wrote tales of derring-do, featuring Arthur, Guinevere, Lancelot and Galahad.
Even in the 21st century, books and films about the Arthurian legend and the Holy Grail continue to appear.
A century or so later, in the hands of writers such as Jean Froissart, Miguel Cervantes and William Shakespeare, the fictional knight Tirant lo Blanch, and the real-life condottieri John Hawkwood would be juxtaposed against the fantastical Don Quixote, and the carousing Sir John Falstaff.
In just one play, Henry V, Shakespeare provides a whole range of military characters, from cool-headed and clear-sighted generals, to captains, and common soldiery.
Emperor Augustus Caesar in a martial pose (1st century)
The Flight of Pompey after Pharsalus, by Jean Fouquet
Medieval view: Richard II of England meets rebels
Sir John Hawkwood (fresco in the Duomo, Florence)
Shakespeare's Sir John Falstaff by Eduard von Grützner
'The Cruel Practices of Prince Rupert' (1643)
The rapid growth of movable type in the late 16th century and early 17th century saw an upsurge in private publication.
Political pamphlets became popular, often lampooning military leaders for political purposes.
A pamphlet directed against Prince Rupert of the Rhine is a typical example.
During the 19th century, irreverence towards authority was at its height, and for every elegant military gentleman painted by the master-portraitists of the European courts, for example, Gainsborough, Goya, and Reynolds, there are the sometimes affectionate and sometimes savage caricatures of Rowland and Hogarth.
This continued in the 19th century, with publications like Punch in the British Empire and Le Père Duchesne in France, poking fun at the military establishment.
This extended to media other print also.
An enduring example is the Major-General's Song from the Gilbert and Sullivan light opera, The Pirates of Penzance, where a senior army officer is satirised for his enormous fund of irrelevant knowledge.
Colonel John Hayes St Leger (detail) by Sir Joshua Reynolds
Rowlandson often satirised the military
'A modern major general' (The Pirates of Penzance)
Punch: war reporter, W H Russell, Crimean War
The increasing importance of cinema in the early 20th century provided a new platform for depictions of military subjects.
During the First World War, although heavily censored, newsreels enabled those at home to see for themselves a heavily sanitised version of life at the front line.
About the same time, both pro-war and anti-war films came to the silver screen.
One of the first films on military aviation, Hell's Angels, broke all box office records on its release in 1929.
Soon, war films of all types were showing throughout the world, notably those of Charlie Chaplin who actively promoted war bonds and voluntary enlistment.
The First World War was also responsible for a new kind of military depiction, through poetry.
Hitherto, poetry had been used mostly to glorify or sanctify war.
The Charge of the Light Brigade by Alfred, Lord Tennyson, with its galloping hoofbeat rhythm, is a prime late Victorian example of this, though Rudyard Kipling had written a scathing reply, The Last of the Light Brigade, criticising the poverty in which many Light Brigade veterans found themselves in old age.
Instead, the new wave of poetry, from the war poets, was written from the point of view of the disenchanted trench soldier.
Leading war poets included Siegfried Sassoon, Wilfred Owen, John McCrae, Rupert Brooke, Isaac Rosenberg, and David Jones.
A similar movement occurred in literature, producing a slew of novels on both sides of the Atlantic, including notably: All Quiet on the Western Front, and Johnny Got His Gun.
The 1963 English stage musical Oh, What a Lovely War!
provided a satirical take on World War I, which was released in a cinematic version directed by Richard Attenborough in 1969.
The propaganda war that accompanied World War II invariably depicted the enemy in unflattering terms.
Examples of this exist not only in posters, but also in the films of Leni Riefenstahl and Sergei Eisenstein.
Alongside this, World War II also inspired films as varied as The Dam Busters, 633 Squadron, Bridge on the River Kwai, The Longest Day, Catch-22, Saving Private Ryan, and The Sea Shall Not Have Them.
The next major event, the Korean War inspired a long-running television series M*A*S*H. With the Vietnam War, the tide of balance turned, and its films, notably Apocalypse Now, Good Morning, Vietnam, Go Tell the Spartans, Born on the Fourth of July, and We Were Soldiers, have tended to contain critical messages.
There is even a nursery rhyme about war, The Grand Old Duke of York, ridiculing a general for his inability to command any further than marching his men up and down a hill.
The huge number of songs focusing on war include And the Band Played Waltzing Matilda and Universal Soldier.
Copyright is a form of intellectual property that grants the creator of an original creative work an exclusive legal right to determine whether and under what conditions this original work may be copied and used by others, usually for a limited term of years.
[1][2][3][4][5] The exclusive rights are not absolute but limited by limitations and exceptions to copyright law, including fair use.
A major limitation on copyright on ideas is that copyright protects only the original expression of ideas, and not the underlying ideas themselves.
[citation needed][6][7]
Copyright is applicable to certain forms of creative work.
Some, but not all jurisdictions require "fixing" copyrighted works in a tangible form.
It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders.
[citation needed][8][9][10][11] These rights frequently include reproduction, control over derivative works, distribution, public performance, and moral rights such as attribution.
Copyrights can be granted by public law and are in that case considered "territorial rights".
This means that copyrights granted by the law of a certain state, do not extend beyond the territory of that specific jurisdiction.
Copyrights of this type vary by country; many countries, and sometimes a large group of countries, have made agreements with other countries on procedures applicable when works "cross" national borders or national rights are inconsistent.
Typically, the public law duration of a copyright expires 50 to 100 years after the creator dies, depending on the jurisdiction.
Some countries require certain copyright formalities to establishing copyright, others recognize copyright in any completed work, without formal registration.
Generally, copyright is enforced as a civil matter, though some jurisdictions do apply criminal sanctions.
Most jurisdictions recognize copyright limitations, allowing "fair" exceptions to the creator's exclusivity of copyright and giving users certain rights.
The development of digital media and computer network technologies have prompted reinterpretation of these exceptions, introduced new difficulties in enforcing copyright, and inspired additional challenges to the philosophical basis of copyright law.
[citation needed] Simultaneously, businesses with great economic dependence upon copyright, such as those in the music business, have advocated the extension and expansion of copyright and sought additional legal and technological enforcement.
[14][additional citation(s) needed]
Copyright licenses can also be granted by those deputized by the original claimant, and private companies may request this as a condition of doing business with them.
Services of internet platform providers like YouTube, Facebook, GitHub, Hotmail, DropBox, Instagram, WhatsApp or Twitter only can be used when users grant the platform provider beforehand the right to co-use all uploaded content, including all material exchanged per email, chat or cloud-storage.
These copyrights only apply for the firm that operates such a platform, no matter in what jurisdiction the platform-services are being offered.
Private companies in general do not recognize exceptions or give users more rights than the right to use the platform according certain rules.
[15][better source needed][additional citation(s) needed]
Copyright came about with the invention of the printing press and with wider literacy.
As a legal concept, its origins in Britain were from a reaction to printers' monopolies at the beginning of the 18th century.
The English Parliament was concerned about the unregulated copying of books and passed the Licensing of the Press Act 1662,[16] which established a register of licensed books and required a copy to be deposited with the Stationers' Company, essentially continuing the licensing of material that had long been in effect.
Copyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized.
Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in Europe and not, for example, in Asia.
In the Middle Ages in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society.
The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property.
However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights.
The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified.
This parallels the ways in which capitalism led to the commodification of many aspects of social life that earlier had no monetary or economic value per se.
Copyright has grown from a legal concept regulating copying rights in the publishing of books and maps to one with a significant effect on nearly every modern industry, covering such items as sound recordings, films, photographs, software, and architectural works.
Often seen as the first real copyright law, the 1709 British Statute of Anne gave the publishers rights for a fixed period, after which the copyright expired.
The act also alluded to individual rights of the artist.
It began, "Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing ... Books, and other Writings, without the Consent of the Authors ... to their very great Detriment, and too often to the Ruin of them and their Families:".
[19] A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved.
An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.
The Copyright Clause of the United States, Constitution (1787) authorized copyright legislation: "To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries."
That is, by guaranteeing them a period of time in which they alone could profit from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole.
A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.
The original length of copyright in the United States was 14 years, and it had to be explicitly applied for.
If the author wished, they could apply for a second 14‑year monopoly grant, but after that the work entered the public domain, so it could be used and built upon by others.
Copyright law was enacted rather late in German states, and the historian Eckhard Höffner argues that the absence of copyright laws in the early 19th century encouraged publishing, was profitable for authors, led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century.
The 1886 Berne Convention first established recognition of copyrights among sovereign nations, rather than merely bilaterally.
Under the Berne Convention, copyrights for creative works do not have to be asserted or declared, as they are automatically in force at creation: an author need not "register" or "apply for" a copyright in countries adhering to the Berne Convention.
[21] As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires.
The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention.
The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100 years later with the passage of the Copyright, Designs and Patents Act 1988.
Specially, for educational and scientific research purposes, the Berne Convention provides the developing countries issue compulsory licenses for the translation or reproduction of copyrighted works within the limits prescribed by the Convention.
This was a special provision that had been added at the time of 1971 revision of the Convention, because of the strong demands of the developing countries.
The United States did not sign the Berne Convention until 1989.
The United States and most Latin American countries instead entered into the Buenos Aires Convention in 1910, which required a copyright notice on the work (such as all rights reserved), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms.
[23][24][25] The Universal Copyright Convention was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the Soviet Union and developing nations.
The regulations of the Berne Convention are incorporated into the World Trade Organization's TRIPS agreement (1995), thus giving the Berne Convention effectively near-global application.
In 1961, the United International Bureaux for the Protection of Intellectual Property signed the Rome Convention for the Protection of Performers, Producers of Phonograms and Broadcasting Organizations.
In 1996, this organization was succeeded by the founding of the World Intellectual Property Organization, which launched the 1996 WIPO Performances and Phonograms Treaty and the 2002 WIPO Copyright Treaty, which enacted greater restrictions on the use of technology to copy works in the nations that ratified it.
The Trans-Pacific Partnership includes intellectual Property Provisions relating to copyright.
Copyright laws are standardized somewhat through these international conventions such as the Berne Convention and Universal Copyright Convention.
These multilateral treaties have been ratified by nearly all countries, and international organizations such as the European Union or World Trade Organization require their member states to comply with them.
The original holder of the copyright may be the employer of the author rather than the author himself if the work is a "work for hire".
[27] For example, in English law the Copyright, Designs and Patents Act 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a "Work for Hire".
Typically, the first owner of a copyright is the person who created the work i.e.
the author.
[28][28] But when more than one person creates the work, then a case of joint authorship can be made provided some criteria are met.
Copyright may apply to a wide range of creative, intellectual, or artistic forms, or "works".
Specifics vary by jurisdiction, but these can include poems, theses, fictional characters plays and other literary works, motion pictures, choreography, musical compositions, sound recordings, paintings, drawings, sculptures, photographs, computer software, radio and television broadcasts, and industrial designs.
Graphic designs and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.
Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed.
[31] For example, the copyright to a Mickey Mouse cartoon restricts others from making copies of the cartoon or creating derivative works based on Disney's particular anthropomorphic mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney's.
[31] Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, Steamboat Willie is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection.
Typically, a work must meet minimal standards of originality in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended).
Different countries impose different tests, although generally the requirements are low; in the United Kingdom there has to be some "skill, labour, and judgment" that has gone into it.
[32] In Australia and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work.
However, single words or a short string of words can sometimes be registered as a trademark instead.
Copyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.
In all countries where the Berne Convention standards apply, copyright is automatic, and need not be obtained through official registration with any government office.
Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce his or her exclusive rights.
[21] However, while registration isn't needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as prima facie evidence of a valid copyright and enables the copyright holder to seek statutory damages and attorney's fees.
[33] (In the US, registering after an infringement only enables one to receive actual damages and lost profits.)
A widely circulated strategy to avoid the cost of copyright registration is referred to as the poor man's copyright.
It proposes that the creator send the work to himself in a sealed envelope by registered mail, using the postmark to establish the date.
This technique has not been recognized in any published opinions of the United States courts.
The United States Copyright Office says the technique is not a substitute for actual registration.
[34] The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original or establish who created the work.
The Berne Convention allows member countries to decide whether creative works must be "fixed" to enjoy copyright.
Article 2, Section 2 of the Berne Convention states: "It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form."
Some countries do not require that a work be produced in a particular form to obtain copyright protection.
For instance, Spain, France, and Australia do not require fixation for copyright protection.
The United States and Canada, on the other hand, require that most works must be "fixed in a tangible medium of expression" to obtain copyright protection.
[37] U.S. law requires that the fixation be stable and permanent enough to be "perceived, reproduced or communicated for a period of more than transitory duration."
Similarly, Canadian courts consider fixation to require that the work be "expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance.
Before 1989, United States law required the use of a copyright notice, consisting of the copyright symbol (©, the letter C inside a circle), the abbreviation "Copr.
", or the word "Copyright", followed by the year of the first publication of the work and the name of the copyright holder.
[38][39] Several years may be noted if the work has gone through substantial revisions.
The proper copyright notice for sound recordings of musical or other audio works is a sound recording copyright symbol (℗, the letter P inside a circle), which indicates a sound recording copyright, with the letter P indicating a "phonorecord".
In addition, the phrase All rights reserved was once required to assert copyright, but that phrase is now legally obsolete.
Almost everything on the Internet has some sort of copyright attached to it.
Whether these things are watermarked, signed, or have any other sort of indication of the copyright is a different story however.
In 1989 the United States enacted the Berne Convention Implementation Act, amending the 1976 Copyright Act to conform to most of the provisions of the Berne Convention.
As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic.
[41] However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit – using notices of this form may reduce the likelihood of a defense of "innocent infringement" being successful.
Copyrights are generally enforced by the holder in a civil law court, but there are also criminal infringement statutes in some jurisdictions.
While central registries are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily prove that copyright was infringed.
Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the RIAA are increasingly targeting the file sharing home Internet user.
Thus far, however, most such cases against file sharers have been settled out of court.
(See: Legal aspects of file sharing)
In most jurisdictions the copyright holder must bear the cost of enforcing copyright.
This will usually involve engaging legal representation, administrative or court costs.
In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.
"...by 1978, the scope was expanded to apply to any 'expression' that has been 'fixed' in any medium, this protection granted automatically whether the maker wants it or not, no registration required.
For a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws or adheres to a bilateral treaty or established international convention such as the Berne Convention or WIPO Copyright Treaty.
Improper use of materials outside of legislation is deemed "unauthorized edition", not copyright infringement.
Statistics regarding the effects of copyright infringement are difficult to determine.
Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available.
[45] Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect.
[46] In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.
According to World Intellectual Property Organisation, copyright protects two types of rights.
Economic rights allow right owners to derive financial reward from the use of their works by others.
Moral rights allow authors and creators to take certain actions to preserve and protect their link with their work.
The author or creator may be the owner of the economic rights or those rights may be transferred to one or more copyright owners.
Many countries do not allow the transfer of moral rights.
Where Economic rights allow right owners to derive financial reward from the use of their works by others, the Moral rights allow authors and creators to take certain actions to preserve and protect their link with their work.
With any kind of property, its owner may decide how it is to be used, and others can use it lawfully only if they have the owner’s permission, often through a license.
The owner’s use of the property must, however, respect the legally recognised rights and interests of other members of society.
So the owner of a copyright-protected work may decide how to use the work, and may prevent others from using it without permission.
National laws usually grant copyright owners exclusive rights to allow third parties to use their works, subject to the legally recognised rights and interests of others.
[48] Most copyright laws state that authors or other right owners have the right to authorise or prevent certain acts in relation to a work.
Right owners can authorise or prohibit:
Moral rights are concerned with the non-economic rights of a creator.
They protect the creator’s connection with a work as well as the integrity of the work.
Moral rights are only accorded to individual authors and in many national laws they remain with the authors even after the authors have transferred their economic rights.
In some EU countries, such as France, moral rights last indefinitely.
In the UK, however, moral rights are finite.
That is, the right of attribution and the right of integrity last only as long as the work is in copyright.
When the copyright term comes to an end, so too do the moral rights in that work.
This is just one reason why the moral rights regime within the UK is often regarded as weaker or inferior to the protection of moral rights in continental Europe and elsewhere in the world.
[49] The Berne Convention, in Article 6bis, requires its members to grant authors the following rights:
These and other similar rights granted in national laws are generally known as the moral rights of authors.
The Berne Convention requires these rights to be independent of authors’ economic rights.
Moral rights are only accorded to individual authors and in many national laws they remain with the authors even after the authors have transferred their economic rights.
This means that even where, for example, a film producer or publisher owns the economic rights in a work, in many jurisdictions the individual author continues to have moral rights.
[48] Recently, as a part of the debates being held at the U.S.
Copyright Office on the question of inclusion of Moral Rights as a part of the framework of the Copyright Law in United States, the Copyright Office concluded that many diverse aspects of the current moral rights patchwork—including copyright law’s derivative work right, state moral rights statutes, and contract law—are generally working well and should not be changed.
Further, the Office concludes that there is no need for the creation of a blanket moral rights statute at this time.
However, there are aspects of the U.S. moral rights patchwork that could be improved to the benefit of individual authors and the copyright system as a whole.
The Copyright Law in the United States, several exclusive rights are granted to the holder of a copyright, as are listed below -
The basic right when a work is protected by copyright is that the holder may determine and decide how and under what conditions the protected work may be used by others.
This includes the right to decide to distribute the work for free.
This part of copyright is often overseen.
The phrase "exclusive right" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission.
Copyright is sometimes called a "negative right", as it serves to prohibit certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than permitting people (e.g., authors) to do something they would otherwise be unable to do.
In this way it is similar to the unregistered design right in English law and European law.
The rights of the copyright holder also permit him/her to not use or exploit their copyright, for some or all of the term.
There is, however, a critique which rejects this assertion as being based on a philosophical interpretation of copyright law that is not universally shared.
There is also debate on whether copyright should be considered a property right or a moral right.
UK copyright law gives creators both economic rights and moral rights.
While ‘copying’ someone else’s work without permission may constitute an infringement of their economic rights, that is, the reproduction right or the right of communication to the public, whereas, ‘mutilating’ it might infringe the creator’s moral rights.
In the UK, moral rights include the right to be identified as the author of the work, which is generally identified as the right of attribution, and the right not to have your work subjected to ‘derogatory treatment’, that is the right of integrity.
Indian copyright law is at parity with the international standards as contained in TRIPS.
The Indian Copyright Act, 1957, pursuant to the amendments in 1999, 2002 and 2012, fully reflects the Berne Convention for Protection of Literary and Artistic Works, 1886 and the Universal Copyrights Convention, to which India is a party.
India is also a party to the Geneva Convention for the Protection of Rights of Producers of Phonograms and is an active member of the World Intellectual Property Organization (WIPO) and United Nations Educational, Scientific and Cultural Organization (UNESCO).
The Indian system provides both the economic and moral rights under different provisions of its Indian Copyright Act of 1957.
Copyright subsists for a variety of lengths in different jurisdictions.
The length of the term can depend on several factors, including the type of work (e.g.
musical composition, novel), whether the work has been published, and whether the work was created by an individual or a corporation.
In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years.
In the United States, the term for most existing works is a fixed number of years after the date of creation or publication.
Under most countries' laws (for example, the United States[54] and the United Kingdom[55]), copyrights expire at the end of the calendar year in which they would otherwise expire.
The length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult.
For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage.
In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France.
Many countries have extended the length of their copyright terms (sometimes retroactively).
International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.
In the United States, all books and other works published before 1923 have expired copyrights and are in the public domain.
[57] In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain.
Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain.
[58] Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.
But if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the U.S., the terms of copyright around the world must be considered.
If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.
In 1998, the length of a copyright in the United States was increased by 20 years under the Copyright Term Extension Act.
This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point.
In many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses.
US copyright does NOT cover names, title, short phrases or Listings (such as ingredients, recipes, labels, or formulas).
[60] However, there are protections available for those areas copyright does not cover – such as trademarks and patents.
There are some exceptions to what copyright will protect.
Copyright will not protect:
The idea–expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves.
This principle, first clarified in the 1879 case of Baker v. Selden, has since been codified by the Copyright Act of 1976 at 17 U.S.C.
§ 102(b).
Copyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder.
It is therefore legal, for example, to resell a copyrighted book or CD.
In the United States this is known as the first-sale doctrine, and was established by the courts to clarify the legality of reselling books in second-hand bookstores.
Some countries may have parallel importation restrictions that allow the copyright holder to control the aftermarket.
This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing.
The first-sale doctrine is known as exhaustion of rights in other countries and is a principle which also applies, though somewhat differently, to patent and trademark rights.
It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved.
It does not permit making or distributing additional copies.
In Kirtsaeng v. John Wiley & Sons, Inc.,[61] in 2013, the United States Supreme Court held in a 6–3 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US without such permission.
The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission.
The defendant, without permission from the publisher, imported the textbooks and resold on eBay.
The Supreme Court's holding severely limits the ability of copyright holders to prevent such importation.
In addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying his or her own legitimately obtained copy of a copyrighted work, so long as duplication is not involved.
However, in countries that implement moral rights, a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.
Copyright does not prohibit all copying or replication.
In the United States, the fair use doctrine, codified by the Copyright Act of 1976 as 17 U.S.C.
Section 107, permits some copying and distribution without permission of the copyright holder or payment to same.
The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis.
Those factors are:
In the United Kingdom and many other Commonwealth countries, a similar notion of fair dealing was established by the courts or through legislation.
The concept is sometimes not well defined; however in Canada, private copying for personal use has been expressly permitted by statute since 1999.
In Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright), 2012 SCC 37, the Supreme Court of Canada concluded that limited copying for educational purposes could also be justified under the fair dealing exemption.
In Australia, the fair dealing exceptions under the Copyright Act 1968 (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent.
Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e.
legal advice).
Under current Australian law, although it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner, owners of a legitimate copy are permitted to "format shift" that work from one medium to another for personal, private use, or to "time shift" a broadcast work for later, once and only once, viewing or listening.
Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.
In the United States the AHRA (Audio Home Recording Act Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.
Later acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying.
Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use.
The Digital Millennium Copyright Act prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner.
[29] An appellate court has held that fair use is not a defense to engaging in such distribution.
The copyright directive allows EU member states to implement a set of exceptions to copyright.
Examples of those exceptions are:
It is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder.
A copyright, or aspects of it (e.g.
reproduction alone, all but moral rights), may be assigned or transferred from one party to another.
[65] For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations.
The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author.
In the digital age of music, music may be copied and distributed at minimal cost through the Internet; however, the record industry attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience.
A copyright holder need not transfer all rights completely, though many publishers will insist.
Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy or distribute the work in a particular region or for a specified period of time.
A transfer or licence may have to meet particular formal requirements in order to be effective,[66] for example under the Australian Copyright Act 1968 the copyright itself must be expressly transferred in writing.
Under the U.S.
Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor.
For that purpose, ownership in copyright includes exclusive licenses of rights.
Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor.
No special form of transfer or grant is required.
A simple document that identifies the work involved and the rights being granted is sufficient.
Non-exclusive grants (often called non-exclusive licenses) need not be in writing under U.S. law.
They can be oral or even implied by the behavior of the parties.
Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S.
Copyright Office.
(Information on recording transfers is available on the Office's web site.)
While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a real estate transaction.
Copyright may also be licensed.
[65] Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed statutory license (e.g.
musical works in the United States used for radio broadcast or performance).
This is also called a compulsory license, because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made.
[67] Failure to follow the proper procedures would place the copier at risk of an infringement suit.
Because of the difficulty of following every individual work, copyright collectives or collecting societies and performing rights organizations (such as ASCAP, BMI, and SESAC) have been formed to collect royalties for hundreds (thousands and more) works at once.
Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.
Copyright licenses known as open or free licenses seek to grant several rights to licensees, either for a fee or not.
Free in this context is not as much of a reference to price as it is to freedom.
What constitutes free licensing has been characterised in a number of similar definitions, including by order of longevity the Free Software Definition, the Debian Free Software Guidelines, the Open Source Definition and the Definition of Free Cultural Works.
Further refinements to these definitions have resulted in categories such as copyleft and permissive.
Common examples of free licences are the GNU General Public License, BSD licenses and some Creative Commons licenses.
Founded in 2001 by James Boyle, Lawrence Lessig, and Hal Abelson, the Creative Commons (CC) is a non-profit organization[68] which aims to facilitate the legal sharing of creative works.
To this end, the organization provides a number of generic copyright license options to the public, gratis.
These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.
Terms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee.
Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely.
Six general types of CC licenses are available (although some of them are not properly free per the above definitions and per Creative Commons' own advice).
These are based upon copyright-holder stipulations such as whether he or she is willing to allow modifications to the work, whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work.
[69] As of 2009[update] approximately 130 million individuals had received such licenses.
Some sources are critical of particular aspects of the copyright system.
This is known as a debate over copynorms.
Particularly to the background of uploading content to internet platforms and the digital exchange of original work, there is discussion about the copyright aspects of downloading and streaming, the copyright aspects of hyperlinking and framing.
Concerns are often couched in the language of digital rights, digital freedom, database rights, open data or censorship.
[70] Discussions include Free Culture, a 2004 book by Lawrence Lessig.
Lessig coined the term permission culture to describe a worst-case system.
Good Copy Bad Copy (documentary) and RiP!
: A Remix Manifesto, discuss copyright.
Some suggest an alternative compensation system.
In Europe consumers are acting up against the raising costs of music, film and books, a political party has been grown out of it, The Pirates.
Some groups reject copyright altogether, taking an anti-copyright stance.
The perceived inability to enforce copyright online leads some to advocate ignoring legal statutes when on the web.
Copyright, like other intellectual property rights, is subject to a statutorily determined term.
Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be used or exploited by anyone without obtaining permission, and normally without payment.
However, in paying public domain regimes the user may still have to pay royalties to the state or to an authors' association.
Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a common law copyright.
Public domain works should not be confused with works that are publicly available.
Works posted in the internet, for example, are publicly available, but are not generally in the public domain.
Copying such works may therefore violate the author's copyright.
A government is the system or group of people governing an organized community, often a state.
In the case of its broad associative definition, government normally consists of legislature, executive, and judiciary.
Government is a means by which organizational policies are enforced, as well as a mechanism for determining policy.
Each government has a kind of constitution, a statement of its governing principles and philosophy.
Typically the philosophy chosen is some balance between the principle of individual freedom and the idea of absolute state authority (tyranny).
While all types of organizations have governance, the word government is often used more specifically to refer to the approximately 200 independent national governments on Earth, as well as subsidiary organizations.
Historically prevalent forms of government include monarchy, aristocracy, timocracy, oligarchy, democracy, theocracy and tyranny.
The main aspect of any philosophy of government is how political power is obtained, with the two main forms being electoral contest and hereditary succession.
A government is the system to govern a state or community.
The word government derives, ultimately, from the Greek verb κυβερνάω [kubernáo] (meaning to steer with gubernaculum (rudder), the metaphorical sense being attested in Plato's Ship of State).
The Columbia Encyclopedia defines government as "a system of social control under which the right to make laws, and the right to enforce them, is vested in a particular group in society".
While all types of organizations have governance, the word government is often used more specifically to refer to the approximately 200 independent national governments on Earth, as well as their subsidiary organizations.
In the Commonwealth of Nations, the word government is also used more narrowly to refer to the ministry (collective executive), a collective group of people that exercises executive authority in a state[citation needed] or, metonymically, to the governing cabinet as part of the executive.
Finally, government is also sometimes used in English as a synonym for governance.
The moment and place that the phenomenon of human government developed is lost in time; however, history does record the formations of early governments.
About 5,000 years ago, the first small city-states appeared.
[6] By the third to second millenniums BC, some of these had developed into larger governed areas: Sumer, Ancient Egypt, the Indus Valley Civilization, and the Yellow River Civilization.
The development of agriculture and water control projects were a catalyst for the development of governments.
[8] For many thousands of years when people were hunter-gatherers and small scale farmers, humans lived in small, non-hierarchical and self-sufficient communities.
[citation needed] On occasion a chief of a tribe was elected by various rituals or tests of strength to govern his tribe, sometimes with a group of elder tribesmen as a council.
The human ability to precisely communicate abstract, learned information allowed humans to become ever more effective at agriculture,[9] and that allowed for ever increasing population densities.
[6] David Christian explains how this resulted in states with laws and governments:[10]
As farming populations gathered in larger and denser communities, interactions between different groups increased and the social pressure rose until, in a striking parallel with star formation, new structures suddenly appeared, together with a new level of complexity.
Like stars, cities and states reorganize and energize the smaller objects within their gravitational field.
Starting at the end of the 17th century, the prevalence of republican forms of government grew.
The Glorious Revolution in England, the American Revolution, and the French Revolution contributed to the growth of representative forms of government.
The Soviet Union was the first large country to have a Communist government.
[2] Since the fall of the Berlin Wall, liberal democracy has become an even more prevalent form of government.
In the nineteenth and twentieth century, there was a significant increase in the size and scale of government at the national level.
[12] This included the regulation of corporations and the development of the welfare state.
In political science, it has long been a goal to create a typology or taxonomy of polities, as typologies of political systems are not obvious.
[13] It is especially important in the political science fields of comparative politics and international relations.
Like all categories discerned within forms of government, the boundaries of government classifications are either fluid or ill-defined.
Superficially, all governments have an official or ideal form.
The United States is a constitutional republic, while the former Soviet Union was a socialist republic.
However self-identification is not objective, and as Kopstein and Lichbach argue, defining regimes can be tricky.
[14] For example, elections are a defining characteristic of an electoral democracy,[citation needed] but in practice elections in the former Soviet Union were not "free and fair" and took place in a one-party state.
Voltaire argued that "the Holy Roman Empire is neither Holy, nor Roman, nor an Empire".
[15] Many governments that officially call themselves a "democratic republic" are not democratic, nor a republic; they are usually a dictatorship de facto.
Communist dictatorships have been especially prone to use this term.
For example, the official name of North Vietnam was "The Democratic Republic of Vietnam".
China uses a variant, "The People's Republic of China".
Thus in many practical classifications it would not be considered democratic.
Identifying a form of government is also difficult because many political systems originate as socio-economic movements and are then carried into governments by parties naming themselves after those movements; all with competing political-ideologies.
Experience with those movements in power, and the strong ties they may have to particular forms of government, can cause them to be considered as forms of government in themselves.
Other complications include general non-consensus or deliberate "distortion or bias" of reasonable technical definitions to political ideologies and associated forms of governing, due to the nature of politics in the modern era.
For example: The meaning of "conservatism" in the United States has little in common with the way the word's definition is used elsewhere.
As Ribuffo notes, "what Americans now call conservatism much of the world calls liberalism or neoliberalism".
[16] Since the 1950s conservatism in the United States has been chiefly associated with the Republican Party.
However, during the era of segregation many Southern Democrats were conservatives, and they played a key role in the Conservative Coalition that controlled Congress from 1937 to 1963.
Every country in the world is ruled by a system of governance that combines at least three or more political or economic attributes.
[citation needed] Additionally, opinions vary by individuals concerning the types and properties of governments that exist.
"Shades of gray" are commonplace in any government and its corresponding classification.
Even the most liberal democracies limit rival political activity to one extent or another while the most tyrannical dictatorships must organize a broad base of support thereby creating difficulties for "pigeonholing" governments into narrow categories.
Examples include the claims of the United States as being a plutocracy rather than a democracy since some American voters believe elections are being manipulated by wealthy Super PACs.
The Classical Greek philosopher Plato discusses five types of regimes: aristocracy, timocracy, oligarchy, democracy and tyranny.
Plato also assigns a man to each of these regimes to illustrate what they stand for.
The tyrannical man would represent tyranny for example.
These five regimes progressively degenerate starting with aristocracy at the top and tyranny at the bottom.
One method of classifying governments is through which people have the authority to rule.
This can either be one person (an autocracy, such as monarchy), a select group of people (an aristocracy), or the people as a whole (a democracy, such as a republic).
The division of governments as monarchy, aristocracy and democracy has been used since Aristotle's Politics.
[citation needed]  In his book Leviathan, Thomas Hobbes expands on this classification.
The difference of Commonwealths consisteth in the difference of the sovereign, or the person representative of all and every one of the multitude.
And because the sovereignty is either in one man, or in an assembly of more than one; and into that assembly either every man hath right to enter, or not every one, but certain men distinguished from the rest; it is manifest there can be but three kinds of Commonwealth.
For the representative must needs be one man, or more; and if more, then it is the assembly of all, or but of a part.
When the representative is one man, then is the Commonwealth a monarchy; when an assembly of all that will come together, then it is a democracy, or popular Commonwealth; when an assembly of a part only, then it is called an aristocracy.
Other kind of Commonwealth there can be none: for either one, or more, or all, must have the sovereign power (which I have shown to be indivisible) entire.
An autocracy is a system of government in which supreme power is concentrated in the hands of one person, whose decisions are subject to neither external legal restraints nor regularized mechanisms of popular control (except perhaps for the implicit threat of a coup d'état or mass insurrection).
A despotism is a government ruled by a single entity with absolute power, whose decisions are subject to neither external legal restraints nor regular mechanisms of popular control (except perhaps for implicit threat).
That entity may be an individual, as in an autocracy, or it may be a group, as in an oligarchy.
The word despotism means to "rule in the fashion of despots".
[citation needed]
A monarchy is where a family or group of families (rarely another type of group), called the royalty, represents national identity, with power traditionally assigned to one of its individuals, called the monarch, who mostly rule kingdoms.
The actual role of the monarch and other members of royalty varies from purely symbolical (crowned republic) to partial and restricted (constitutional monarchy) to completely despotic (absolute monarchy).
Traditionally and in most cases, the post of the monarch is inherited, but there are also elective monarchies where the monarch is elected.
[citation needed]
Aristocracy (Greek ἀριστοκρατία aristokratía, from  ἄριστος aristos "excellent", and κράτος kratos "power") is a form of government that places power in the hands of a small, privileged ruling class.
Many monarchies were aristocracies, although in modern constitutional monarchies the monarch himself or herself has little real power.
The term "Aristocracy" could also refer to the non-peasant, non-servant, and non-city classes in the Feudal system.
An oligarchy is ruled by a small group of segregated, powerful or influential people who usually share similar interests or family relations.
These people may spread power and elect candidates equally or not equally.
An oligarchy is different from a true democracy because very few people are given the chance to change things.
An oligarchy does not have to be hereditary or monarchic.
An oligarchy does not have one clear ruler but several rulers.
[citation needed]
Some historical examples of oligarchy are the former Union of Soviet Socialist Republics.
Some critics of representative democracy think of the United States as an oligarchy.
The Athenian democracy used sortition to elect candidates, almost always male, Greek, educated citizens holding a minimum of land, wealth and status.
[citation needed]
A theocracy is rule by a religious elite; a system of governance composed of religious institutions in which the state and the church are traditionally or constitutionally the same entity.
The Vatican's (see Pope), Iran's (see Supreme Leader), Tibetan government's (see Dalai Lama), Caliphates and other Islamic states are historically considered theocracies.
[citation needed]
In a general sense, in a democracy, all the people of a state or polity are involved in making decisions about its affairs.
Also refer to the rule by a government chosen by election where most of the populace are enfranchised.
The key distinction between a democracy and other forms of constitutional government is usually taken to be that the right to vote is not limited by a person's wealth or race (the main qualification for enfranchisement is usually having reached a certain age).
A democratic government is, therefore, one supported (at least at the time of the election) by a majority of the populace (provided the election was held fairly).
A "majority" may be defined in different ways.
There are many "power-sharing" (usually in countries where people mainly identify themselves by race or religion) or "electoral-college" or "constituency" systems where the government is not chosen by a simple one-vote-per-person headcount.
[citation needed]
In democracies, large proportions of the population may vote, either to make decisions or to choose representatives to make decisions.
Commonly significant in democracies are political parties, which are groups of people with similar ideas about how a country or region should be governed.
Different political parties have different ideas about how the government should handle different problems.
[citation needed]
Liberal democracy is a variant of democracy.
It is a form of government in which representative democracy operates under the principles of liberalism.
It is characterised by fair, free, and competitive elections between multiple distinct political parties, a separation of powers into different branches of government, the rule of law in everyday life as part of an open society, and the protection of human rights and civil liberties for all persons.
To define the system in practice, liberal democracies often draw upon a constitution, either formally written or uncodified, to delineate the powers of government and enshrine the social contract.
After a period of sustained expansion throughout the 20th century, liberal democracy became the predominant political system in the world.
A liberal democracy may take various constitutional forms: it may be a republic, such as France, Germany, India, Ireland, Italy, Taiwan, or the United States; or a constitutional monarchy, such as Japan, Spain, or the United Kingdom.
It may have a presidential system (Argentina, Brazil, Mexico, or the United States), a semi-presidential system (France, Portugal, or Taiwan), or a parliamentary system (Australia, Canada, Germany, Ireland, India, Italy, New Zealand, or the United Kingdom).
[citation needed]
A republic is a form of government in which the country is considered a "public matter" (Latin: res publica), not the private concern or property of the rulers, and where offices of states are subsequently directly or indirectly elected or appointed rather than inherited.
The people, or some significant portion of them, have supreme control over the government and where offices of state are elected or chosen by elected people.
[22][23] A common simplified definition of a republic is a government where the head of state is not a monarch.
[24][25] Montesquieu included both democracies, where all the people have a share in rule, and aristocracies or oligarchies, where only some of the people rule, as republican forms of government.
Other terms used to describe different republics include Democratic republic, Parliamentary republic, Federal republic, and Islamic Republic.
Rule by authoritarian governments is identified in societies where a specific set of people possess the authority of the state in a republic or union.
It is a political system controlled by unelected rulers who usually permit some degree of individual freedom.
Rule by a totalitarian government is characterised by a highly centralised and coercive authority that regulates nearly every aspect of public and private life.
[citation needed]
In contrast, a constitutional republic is rule by a government whose powers are limited by law or a formal constitution, and chosen by a vote amongst at least some sections of the populace (Ancient Sparta was in its own terms a republic, though most inhabitants were disenfranchised).
Republics that exclude sections of the populace from participation will typically claim to represent all citizens (by defining people without the vote as "non-citizens").
Examples include the United States, South Africa, India, etc.
[citation needed]
Federalism is a political concept in which a group of members are bound together by covenant (Latin: foedus, covenant) with a governing representative head.
The term "federalism" is also used to describe a system of government in which sovereignty is constitutionally divided between a central governing authority and constituent political units (such as states or provinces).
Federalism is a system based upon democratic rules and institutions in which the power to govern is shared between national and provincial/state governments, creating what is often called a federation.
Proponents are often called federalists.
Historically, most political systems originated as socioeconomic ideologies.
Experience with those movements in power and the strong ties they may have to particular forms of government can cause them to be considered as forms of government in themselves.
Certain major characteristics are defining of certain types; others are historically associated with certain types of government.
This list focuses on differing approaches that political systems take to the distribution of sovereignty, and the autonomy of regions within the state.
Law is a system of rules that are created and enforced through social or governmental institutions to regulate behavior.
[2] It has been defined both as  "the Science of Justice" and "the Art of Justice".
[3][4]  Law is a system that regulates and ensures that individuals or a community adhere to the will of the state.
State-enforced laws can be made by a collective legislature or by a single legislator, resulting in statutes, by the executive through decrees and regulations, or established by judges through precedent, normally in common law jurisdictions.
Private individuals can create legally binding contracts, including arbitration agreements that may elect to accept alternative arbitration to the normal court process.
The formation of laws themselves may be influenced by a constitution, written or tacit, and the rights encoded therein.
The law shapes politics, economics, history and society in various ways and serves as a mediator of relations between people.
A general distinction can be made between (a) civil law jurisdictions, in which a legislature or other central body codifies and consolidates their laws, and (b) common law systems, where judge-made precedent is accepted as binding law.
Historically, religious laws played a significant role even in settling of secular matters, and is still used in some religious communities.
Islamic Sharia law is the world's most widely used religious law, and is used as the primary legal system in some countries, such as Iran and Saudi Arabia.
The adjudication of the law is generally divided into two main areas.
Criminal law deals with conduct that is considered harmful to social order and in which the guilty party may be imprisoned or fined.
Civil law (not to be confused with civil law jurisdictions above) deals with the resolution of lawsuits (disputes) between individuals and/or organizations.
Law provides a source of scholarly inquiry into legal history, philosophy, economic analysis and sociology.
Law also raises important and complex issues concerning equality, fairness, and justice.
Numerous definitions of law have been put forward over the centuries.
The Third New International Dictionary from Merriam-Webster[7] defines law as: "Law is a binding custom or practice of a community; a rule or mode of conduct or action that is prescribed or formally recognized as binding by a supreme controlling authority or is made obligatory by a sanction (as an edict, decree, rescript, order, ordinance, statute, resolution, rule, judicial decision, or usage) made, recognized, or enforced by the controlling authority."
The Dictionary of the History of Ideas published by Scribner's in 1973 defined the concept of law accordingly as: "A legal system is the most explicit, institutionalized, and complex mode of regulating human conduct.
At the same time, it plays only one part in the congeries of rules which influence behavior, for social and moral rules of a less institutionalized kind are also of great importance.
There have been several attempts to produce "a universally acceptable definition of law".
In 1972, one source indicated that no such definition could be produced.
[9] McCoubrey and White said that the question "what is law?"
has no simple answer.
[10] Glanville Williams said that the meaning of the word "law" depends on the context in which that word is used.
He said that, for example, "early customary law" and "municipal law" were contexts where the word "law" had two different and irreconcilable meanings.
[11] Thurman Arnold said that it is obvious that it is impossible to define the word "law" and that it is also equally obvious that the struggle to define that word should not ever be abandoned.
[12] It is possible to take the view that there is no need to define the word "law" (e.g.
"let's forget about generalities and get down to cases").
The history of law links closely to the development of civilization.
Ancient Egyptian law, dating as far back as 3000 BC, contained a civil code that was probably broken into twelve books.
It was based on the concept of Ma'at, characterised by tradition, rhetorical speech, social equality and impartiality.
[14][15] By the 22nd century BC, the ancient Sumerian ruler Ur-Nammu had formulated the first law code, which consisted of casuistic statements ("if … then ...").
Around 1760 BC, King Hammurabi further developed Babylonian law, by codifying and inscribing it in stone.
Hammurabi placed several copies of his law code throughout the kingdom of Babylon as stelae, for the entire public to see; this became known as the Codex Hammurabi.
The most intact copy of these stelae was discovered in the 19th century by British Assyriologists, and has since been fully transliterated and translated into various languages, including English, Italian, German, and French.
The Old Testament dates back to 1280 BC and takes the form of moral imperatives as recommendations for a good society.
The small Greek city-state, ancient Athens, from about the 8th century BC was the first society to be based on broad inclusion of its citizenry, excluding women and the slave class.
However, Athens had no legal science or single word for "law",[17] relying instead on the three-way distinction between divine law (thémis), human decree (nomos) and custom (díkē).
[18] Yet Ancient Greek law contained major constitutional innovations in the development of democracy.
Roman law was heavily influenced by Greek philosophy, but its detailed rules were developed by professional jurists and were highly sophisticated.
[20][21] Over the centuries between the rise and decline of the Roman Empire, law was adapted to cope with the changing social situations and underwent major codification under Theodosius II and Justinian I.
[22] Although codes were replaced by custom and case law during the Early Middle Ages, Roman law was rediscovered around the 11th century when medieval legal scholars began to research Roman codes and adapt their concepts to the canon law, giving birth to the jus commune.
Latin legal maxims (called brocards) were compiled for guidance.
In medieval England, royal courts developed a body of precedent which later became the common law.
A Europe-wide Law Merchant was formed so that merchants could trade with common standards of practice rather than with the many splintered facets of local laws.
The Law Merchant, a precursor to modern commercial law, emphasised the freedom to contract and alienability of property.
[23] As nationalism grew in the 18th and 19th centuries, the Law Merchant was incorporated into countries' local law under new civil codes.
The Napoleonic and German Codes became the most influential.
In contrast to English common law, which consists of enormous tomes of case law, codes in small books are easy to export and easy for judges to apply.
However, today there are signs that civil and common law are converging.
[24] EU law is codified in treaties, but develops through the precedent laid down by the European Court of Justice.
Ancient India and China represent distinct traditions of law, and have historically had independent schools of legal theory and practice.
The Arthashastra, probably compiled around 100 AD (although it contains older material), and the Manusmriti (c. 100–300 AD) were foundational treatises in India, and comprise texts considered authoritative legal guidance.
[25] Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia.
[26] This Hindu tradition, along with Islamic law, was supplanted by the common law when India became part of the British Empire.
[27] Malaysia, Brunei, Singapore and Hong Kong also adopted the common law.
The eastern Asia legal tradition reflects a unique blend of secular and religious influences.
[28] Japan was the first country to begin modernising its legal system along western lines, by importing bits of the French, but mostly the German Civil Code.
[29] This partly reflected Germany's status as a rising power in the late 19th century.
Similarly, traditional Chinese law gave way to westernisation towards the final years of the Qing Dynasty in the form of six private law codes based mainly on the Japanese model of German law.
[30] Today Taiwanese law retains the closest affinity to the codifications from that period, because of the split between Chiang Kai-shek's nationalists, who fled there, and Mao Zedong's communists who won control of the mainland in 1949.
The current legal infrastructure in the People's Republic of China was heavily influenced by Soviet Socialist law, which essentially inflates administrative law at the expense of private law rights.
[31] Due to rapid industrialisation, today China is undergoing a process of reform, at least in terms of economic, if not social and political, rights.
A new contract code in 1999 represented a move away from administrative domination.
[32] Furthermore, after negotiations lasting fifteen years, in 2001 China joined the World Trade Organization.
Jean-Jacques Rousseau, The Social Contract, II, 6.
The philosophy of law is commonly known as (general) jurisprudence.
Normative jurisprudence asks "what should law be?
", while analytic jurisprudence asks "what is law?"
John Austin's utilitarian answer was that law is "commands, backed by threat of sanctions, from a sovereign, to whom people have a habit of obedience".
[35] Natural lawyers on the other side, such as Jean-Jacques Rousseau, argue that law reflects essentially moral and unchangeable laws of nature.
The concept of "natural law" emerged in ancient Greek philosophy concurrently and in connection with the notion of justice, and re-entered the mainstream of Western culture through the writings of Thomas Aquinas, notably his Treatise on Law.
Hugo Grotius, the founder of a purely rationalistic system of natural law, argued that law arises from both a social impulse—as Aristotle had indicated—and reason.
[36] Immanuel Kant believed a moral imperative requires laws "be chosen as though they should hold as universal laws of nature".
[37] Jeremy Bentham and his student Austin, following David Hume, believed that this conflated the "is" and what "ought to be" problem.
Bentham and Austin argued for law's positivism; that real law is entirely separate from "morality".
[38] Kant was also criticised by Friedrich Nietzsche, who rejected the principle of equality, and believed that law emanates from the will to power, and cannot be labelled as "moral" or "immoral".
In 1934, the Austrian philosopher Hans Kelsen continued the positivist tradition in his book the Pure Theory of Law.
[42] Kelsen believed that although law is separate from morality, it is endowed with "normativity", meaning we ought to obey it.
While laws are positive "is" statements (e.g.
the fine for reversing on a highway is €500); law tells us what we "should" do.
Thus, each legal system can be hypothesised to have a basic norm (Grundnorm) instructing us to obey.
Kelsen's major opponent, Carl Schmitt, rejected both positivism and the idea of the rule of law because he did not accept the primacy of abstract normative principles over concrete political positions and decisions.
[43] Therefore, Schmitt advocated a jurisprudence of the exception (state of emergency), which denied that legal norms could encompass all of political experience.
Later in the 20th century, H. L. A. Hart attacked Austin for his simplifications and Kelsen for his fictions in The Concept of Law.
[45] Hart argued law is a system of rules, divided into primary (rules of conduct) and secondary ones (rules addressed to officials to administer primary rules).
Secondary rules are further divided into rules of adjudication (to resolve legal disputes), rules of change (allowing laws to be varied) and the rule of recognition (allowing laws to be identified as valid).
Two of Hart's students continued the debate: In his book Law's Empire, Ronald Dworkin attacked Hart and the positivists for their refusal to treat law as a moral issue.
Dworkin argues that law is an "interpretive concept",[46] that requires judges to find the best fitting and most just solution to a legal dispute, given their constitutional traditions.
Joseph Raz, on the other hand, defended the positivist outlook and criticised Hart's "soft social thesis" approach in The Authority of Law.
[47] Raz argues that law is authority, identifiable purely through social sources and without reference to moral reasoning.
In his view, any categorisation of rules beyond their role as authoritative instruments in mediation are best left to sociology, rather than jurisprudence.
One definition is that law is a system of rules and guidelines which are enforced through social institutions to govern behaviour.
[2] In The Concept of Law Hart argued law is a "system of rules";[49] Austin said law was "the command of a sovereign, backed by the threat of a sanction";[35] Dworkin describes law as an "interpretive concept" to achieve justice in his text titled Law's Empire;[50] and Raz argues law is an "authority" to mediate people's interests.
[47] Holmes said "The prophecies of what the courts will do in fact, and nothing more pretentious, are what I mean by the law.
"[51] In his Treatise on Law Aquinas argues that law is a rational ordering of things which concern the common good that is promulgated by whoever is charged with the care of the community.
[52] This definition has both positivist and naturalist elements.
In the 18th century Adam Smith presented a philosophical foundation for explaining the relationship between law and economics.
[54] The discipline arose partly out of a critique of trade unions and U.S. antitrust law.
The most influential proponents, such as Richard Posner and Oliver Williamson and the so-called Chicago School of economists and lawyers including Milton Friedman and Gary Becker, are generally advocates of deregulation and privatisation, and are hostile to state regulation or what they see as restrictions on the operation of free markets.
The most prominent economic analyst of law is 1991 Nobel Prize winner Ronald Coase, whose first major article, The Nature of the Firm (1937), argued that the reason for the existence of firms (companies, partnerships, etc.)
is the existence of transaction costs.
[57] Rational individuals trade through bilateral contracts on open markets until the costs of transactions mean that using corporations to produce things is more cost-effective.
His second major article, The Problem of Social Cost (1960), argued that if we lived in a world without transaction costs, people would bargain with one another to create the same allocation of resources, regardless of the way a court might rule in property disputes.
[58] Coase used the example of a nuisance case named Sturges v Bridgman, where a noisy sweetmaker and a quiet doctor were neighbours and went to court to see who should have to move.
[59] Coase said that regardless of whether the judge ruled that the sweetmaker had to stop using his machinery, or that the doctor had to put up with it, they could strike a mutually beneficial bargain about who moves that reaches the same outcome of resource distribution.
Only the existence of transaction costs may prevent this.
[60] So the law ought to pre-empt what would happen, and be guided by the most efficient solution.
The idea is that law and regulation are not as important or effective at helping people as lawyers and government planners believe.
[61] Coase and others like him wanted a change of approach, to put the burden of proof for positive effects on a government that was intervening in the market, by analysing the costs of action.
Sociology of law is a diverse field of study that examines the interaction of law with society and overlaps with jurisprudence, philosophy of law, social theory and more specialised subjects such as criminology.
[63] The institutions of social construction, social norms, dispute processing and legal culture are key areas for inquiry in this knowledge field.
Sociology of law is sometimes seen as a sub-discipline of sociology, but its ties to the academic discipline of law are equally strong, and it is best seen as a transdisciplinary and multidisciplinary study focused on the theorisation and empirical study of legal practices and experiences as social phenomena.
In the United States the field is usually called law and society studies; in Europe it is more often referred to as socio-legal studies.
At first, jurists and legal philosophers were suspicious of sociology of law.
Kelsen attacked one of its founders, Eugen Ehrlich, who sought to make clear the differences and connections between positive law, which lawyers learn and apply, and other forms of 'law' or social norms that regulate everyday life, generally preventing conflicts from reaching barristers and courts.
[64] Contemporary research in sociology of law is much concerned with the way that law is developing outside discrete state jurisdictions, being produced through social interaction in many different kinds of social arenas, and acquiring a diversity of sources of (often competing or conflicting) authority in communal networks existing sometimes within nation states but increasingly also transnationally.
Around 1900 Max Weber defined his "scientific" approach to law, identifying the "legal rational form" as a type of domination, not attributable to personal authority but to the authority of abstract norms.
[66] Formal legal rationality was his term for the key characteristic of the kind of coherent and calculable law that was a precondition for modern political developments and the modern bureaucratic state.
Weber saw this law as having developed in parallel with the growth of capitalism.
[63] Another leading sociologist, Émile Durkheim, wrote in his classic work The Division of Labour in Society that as society becomes more complex, the body of civil law concerned primarily with restitution and compensation grows at the expense of criminal laws and penal sanctions.
[67] Other notable early legal sociologists included Hugo Sinzheimer, Theodor Geiger, Georges Gurvitch and Leon Petrażycki in Europe, and William Graham Sumner in the U.S.[68][69]
There are distinguished methods of legal reasoning (applying the law) and methods of interpreting (construing) the law.
The former are legal syllogism, which holds sway in civil law legal systems,  analogy, which is present in common law legal systems, especially in the US, and argumentative theories that occur in both systems.
The latter are different rules (directives) of legal interpretation such as directives of linguistic interpretation, teleological interpretation or systemic interpretation as well as more specific rules, for instance, golden rule or mischief rule.
There are also many other arguments and cannons of interpretation which altogether make statutory interpretation possible.
Law professor and former United States Attorney General Edward H. Levi noted that the "basic pattern of legal reasoning is reasoning by example" - that is, reasoning by comparing outcomes in cases resolving similar legal questions.
[70] In a U.S. Supreme Court case regarding procedural efforts taken by a debt collection company to avoid errors, Justice Sotomayor cautioned that "legal reasoning is not a mechanical or strictly linear process".
In general, legal systems can be split between civil law and common law systems.
[72] The term "civil law" referring to a legal system should not be confused with "civil law" as a group of legal subjects distinct from criminal or public law.
A third type of legal system—accepted by some countries without separation of church and state—is religious law, based on scriptures.
The specific system that a country is ruled by is often determined by its history, connections with other countries, or its adherence to international standards.
The sources that jurisdictions adopt as authoritatively binding are the defining features of any legal system.
Yet classification is a matter of form rather than substance, since similar rules often prevail.
Civil law is the legal system used in most countries around the world today.
In civil law the sources recognised as authoritative are, primarily, legislation—especially codifications in constitutions or statutes passed by government—and custom.
[73] Codifications date back millennia, with one early example being the Babylonian Codex Hammurabi.
Modern civil law systems essentially derive from the legal practice of the 6th-century Eastern Roman Empire whose texts were rediscovered by late medieval Western Europe.
Roman law in the days of the Roman Republic and Empire was heavily procedural, and lacked a professional legal class.
[74] Instead a lay magistrate, iudex, was chosen to adjudicate.
Decisions were not published in any systematic way, so any case law that developed was disguised and almost unrecognised.
[75] Each case was to be decided afresh from the laws of the State, which mirrors the (theoretical) unimportance of judges' decisions for future cases in civil law systems today.
From 529–534 AD the Byzantine Emperor Justinian I codified and consolidated Roman law up until that point, so that what remained was one-twentieth of the mass of legal texts from before.
[76] This became known as the Corpus Juris Civilis.
As one legal historian wrote, "Justinian consciously looked back to the golden age of Roman law and aimed to restore it to the peak it had reached three centuries before.
"[77] The Justinian Code remained in force in the East until the fall of the Byzantine Empire.
Western Europe, meanwhile, relied on a mix of the Theodosian Code and Germanic customary law until the Justinian Code was rediscovered in the 11th century, and scholars at the University of Bologna used it to interpret their own laws.
[78] Civil law codifications based closely on Roman law, alongside some influences from religious laws such as canon law, continued to spread throughout Europe until the Enlightenment; then, in the 19th century, both France, with the Code Civil, and Germany, with the Bürgerliches Gesetzbuch, modernised their legal codes.
Both these codes influenced heavily not only the law systems of the countries in continental Europe (e.g.
Greece), but also the Japanese and Korean legal traditions.
[79][80] Today, countries that have civil law systems range from Russia and China to most of Central and Latin America.
[81] With the exception of Louisiana's Civil Code, the United States follows the common law system described below.
In common law legal systems, decisions by courts are explicitly acknowledged as "law" on equal footing with statutes adopted through the legislative process and with regulations issued by the executive branch.
The "doctrine of precedent", or stare decisis (Latin for "to stand by decisions") means that decisions by higher courts bind lower courts, and future decisions of the same court, to assure that similar cases reach similar results.
In contrast, in "civil law" systems, legislative statutes are typically more detailed, and judicial decisions are shorter and less detailed, because the judge or barrister is only writing to decide the single case, rather than to set out reasoning that will guide future courts.
Common law originated from England and has been inherited by almost every country once tied to the British Empire (except Malta, Scotland, the U.S. state of Louisiana, and the Canadian province of Quebec).
In medieval England, the Norman conquest the law varied-shire-to-shire, based on disparate tribal customs.
The concept of a "common law" developed during the reign of Henry II during the late 12th century, when Henry appointed judges that had authority to create an institutionalized and unified system of law "common" to the country.
The next major step in the evolution of the common law came when King John was forced by his barons to sign a document limiting his authority to pass laws.
This "great charter" or Magna Carta of 1215 also required that the King's entourage of judges hold their courts and judgments at "a certain place" rather than dispensing autocratic justice in unpredictable places about the country.
[82] A concentrated and elite group of judges acquired a dominant role in law-making under this system, and compared to its European counterparts the English judiciary became highly centralized.
In 1297, for instance, while the highest court in France had fifty-one judges, the English Court of Common Pleas had five.
[83] This powerful and tight-knit judiciary gave rise to a systematized process of developing common law.
However, the system became overly systematized—overly rigid and inflexible.
As a result, as time went on, increasing numbers of citizens petitioned the King to override the common law, and on the King's behalf the Lord Chancellor gave judgment to do what was equitable in a case.
From the time of Sir Thomas More, the first lawyer to be appointed as Lord Chancellor, a systematic body of equity grew up alongside the rigid common law, and developed its own Court of Chancery.
At first, equity was often criticized as erratic, that it varied according to the length of the Chancellor's foot.
[85] Over time, courts of equity developed solid principles, especially under Lord Eldon.
[86] In the 19th century in England, and in 1937 in the U.S., the two systems were merged.
In developing the common law, academic writings have always played an important part, both to collect overarching principles from dispersed case law, and to argue for change.
William Blackstone, from around 1760, was the first scholar to collect, describe, and teach the common law.
[87] But merely in describing, scholars who sought explanations and underlying structures slowly changed the way the law actually worked.
Religious law is explicitly based on religious precepts.
Examples include the Jewish Halakha and Islamic Sharia—both of which translate as the "path to follow"—while Christian canon law also survives in some church communities.
Often the implication of religion for law is unalterability, because the word of God cannot be amended or legislated against by judges or governments.
[citation needed] However a thorough and detailed legal system generally requires human elaboration.
For instance, the Quran has some law, and it acts as a source of further law through interpretation,[89] Qiyas (reasoning by analogy), Ijma (consensus) and precedent.
This is mainly contained in a body of law and jurisprudence known as Sharia and Fiqh respectively.
Another example is the Torah or Old Testament, in the Pentateuch or Five Books of Moses.
This contains the basic code of Jewish law, which some Israeli communities choose to use.
The Halakha is a code of Jewish law which summarises some of the Talmud's interpretations.
Nevertheless, Israeli law allows litigants to use religious laws only if they choose.
Canon law is only in use by members of the Catholic Church, the Eastern Orthodox Church and the Anglican Communion.
Canon law (from Greek kanon, a 'straight measuring rod, ruler') is a set of ordinances and regulations made by ecclesiastical authority (Church leadership), for the government of a Christian organization or church and its members.
It is the internal ecclesiastical law governing the Catholic Church (both the Latin Church and the Eastern Catholic Churches), the Eastern Orthodox and Oriental Orthodox churches, and the individual national churches within the Anglican Communion.
[90] The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches.
In all three traditions, a canon was originally[91] a rule adopted by a church council; these canons formed the foundation of canon law.
The Catholic Church has the oldest continuously functioning legal system in the western world,[92] predating the evolution of modern European civil law  and common law systems.
The 1983 Code of Canon Law governs the Latin Church sui juris.
The Eastern Catholic Churches, which developed different disciplines and practices,  are governed by the Code of Canons of the Eastern Churches.
[93] The canon law of the Catholic Church influenced the common law during the medieval period[94] through its preservation of Roman law doctrine such as the presumption of innocence.
Until the 18th century, Sharia law was practiced throughout the Muslim world in a non-codified form, with the Ottoman Empire's Mecelle code in the 19th century being a first attempt at codifying elements of Sharia law.
Since the mid-1940s, efforts have been made, in country after country, to bring Sharia law more into line with modern conditions and conceptions.
[96][97] In modern times, the legal systems of many Muslim countries draw upon both civil and common law traditions as well as Islamic law and custom.
The constitutions of certain Muslim states, such as Egypt and Afghanistan, recognise Islam as the religion of the state, obliging legislature to adhere to Sharia.
[98] Saudi Arabia recognises Quran as its constitution, and is governed on the basis of Islamic law.
[99] Iran has also witnessed a reiteration of Islamic law into its legal system after 1979.
[100] During the last few decades, one of the fundamental features of the movement of Islamic resurgence has been the call to restore the Sharia, which has generated a vast amount of literature and affected world politics.
Thomas Hobbes, Leviathan, XVII
The main institutions of law in industrialised countries are independent courts, representative parliaments, an accountable executive, the military and police, bureaucratic organisation, the legal profession and civil society itself.
John Locke, in his Two Treatises of Government, and Baron de Montesquieu in The Spirit of the Laws, advocated for a separation of powers between the political, legislature and executive bodies.
[102] Their principle was that no person should be able to usurp all powers of the state, in contrast to the absolutist theory of Thomas Hobbes' Leviathan.
Max Weber and others reshaped thinking on the extension of state.
Modern military, policing and bureaucratic power over ordinary citizens' daily lives pose special problems for accountability that earlier writers such as Locke or Montesquieu could not have foreseen.
The custom and practice of the legal profession is an important part of people's access to justice, whilst civil society is a term used to refer to the social institutions, communities and partnerships that form law's political basis.
A judiciary is a number of judges mediating disputes to determine outcome.
Most countries have systems of appeal courts, answering up to a supreme legal authority.
In the United States, this authority is the Supreme Court;[104] in Australia, the High Court; in the UK, the Supreme Court;[105] in Germany, the Bundesverfassungsgericht; and in France, the Cour de Cassation.
[106][107] For most European countries the European Court of Justice in Luxembourg can overrule national law, when EU law is relevant.
The European Court of Human Rights in Strasbourg allows citizens of the Council of Europe member states to bring cases relating to human rights issues before it.
Some countries allow their highest judicial authority to overrule legislation they determine to be unconstitutional.
For example, in Brown v. Board of Education, the United States Supreme Court nullified many state statutes that had established racially segregated schools, finding such statutes to be incompatible with the Fourteenth Amendment to the United States Constitution.
A judiciary is theoretically bound by the constitution, just as all other government bodies are.
In most countries judges may only interpret the constitution and all other laws.
But in common law countries, where matters are not constitutional, the judiciary may also create law under the doctrine of precedent.
The UK, Finland and New Zealand assert the ideal of parliamentary sovereignty, whereby the unelected judiciary may not overturn law passed by a democratic legislature.
In communist states, such as China, the courts are often regarded as parts of the executive, or subservient to the legislature; governmental institutions and actors exert thus various forms of influence on the judiciary.
[111] In Muslim countries, courts often examine whether state laws adhere to the Sharia: the Supreme Constitutional Court of Egypt may invalidate such laws,[112] and in Iran the Guardian Council ensures the compatibility of the legislation with the "criteria of Islam".
Prominent examples of legislatures are the Houses of Parliament in London, the Congress in Washington D.C., the Bundestag in Berlin, the Duma in Moscow, the Parlamento Italiano in Rome and the Assemblée nationale in Paris.
By the principle of representative government people vote for politicians to carry out their wishes.
Although countries like Israel, Greece, Sweden and China are unicameral, most countries are bicameral, meaning they have two separately appointed legislative houses.
In the 'lower house' politicians are elected to represent smaller constituencies.
The 'upper house' is usually elected to represent states in a federal system (as in Australia, Germany or the United States) or different voting configuration in a unitary system (as in France).
In the UK the upper house is appointed by the government as a house of review.
One criticism of bicameral systems with two elected chambers is that the upper and lower houses may simply mirror one another.
The traditional justification of bicameralism is that an upper chamber acts as a house of review.
This can minimise arbitrariness and injustice in governmental action.
To pass legislation, a majority of the members of a legislature must vote for a bill (proposed law) in each house.
Normally there will be several readings and amendments proposed by the different political factions.
If a country has an entrenched constitution, a special majority for changes to the constitution may be required, making changes to the law more difficult.
A government usually leads the process, which can be formed from Members of Parliament (e.g.
the UK or Germany).
However, in a presidential system, the government is usually formed by an executive and his or her appointed cabinet officials (e.g.
the United States or Brazil).
The executive in a legal system serves as the centre of political authority of the State.
In a parliamentary system, as with Britain, Italy, Germany, India, and Japan, the executive is known as the cabinet, and composed of members of the legislature.
The executive is led by the head of government, whose office holds power under the confidence of the legislature.
Because popular elections appoint political parties to govern, the leader of a party can change in between elections.
The head of state is apart from the executive, and symbolically enacts laws and acts as representative of the nation.
Examples include the President of Germany (appointed by members of federal and state legislatures), the Queen of the United Kingdom (an hereditary office), and the President of Austria (elected by popular vote).
The other important model is the presidential system, found in the United States and in Brazil.
In presidential systems, the executive acts as both head of state and head of government, and has power to appoint an unelected cabinet.
Under a presidential system, the executive branch is separate from the legislature to which it is not accountable.
Although the role of the executive varies from country to country, usually it will propose the majority of legislation, and propose government agenda.
In presidential systems, the executive often has the power to veto legislation.
Most executives in both systems are responsible for foreign relations, the military and police, and the bureaucracy.
Ministers or other officials head a country's public offices, such as a foreign ministry or defence ministry.
The election of a different executive is therefore capable of revolutionising an entire country's approach to government.
While military organisations have existed as long as government itself, the idea of a standing police force is a relatively modern concept.
For example, Medieval England's system of traveling criminal courts, or assizes, used show trials and public executions to instill communities with fear to maintain control.
[118] The first modern police were probably those in 17th-century Paris, in the court of Louis XIV,[119] although the Paris Prefecture of Police claim they were the world's first uniformed policemen.
Max Weber famously argued that the state is that which controls the monopoly on the legitimate use of force.
[121][122] The military and police carry out enforcement at the request of the government or the courts.
The term failed state refers to states that cannot implement or enforce policies; their police and military no longer control security and order and society moves into anarchy, the absence of government.
The etymology of "bureaucracy" derives from the French word for "office" (bureau) and the Ancient Greek for word "power" (kratos).
[124] Like the military and police, a legal system's government servants and bodies that make up its bureaucracy carry out the directives of the executive.
One of the earliest references to the concept was made by Baron de Grimm, a German author who lived in France.
In 1765 he wrote,
The real spirit of the laws in France is that bureaucracy of which the late Monsieur de Gournay used to complain so greatly; here the offices, clerks, secretaries, inspectors and intendants are not appointed to benefit the public interest, indeed the public interest appears to have been established so that offices might exist.
Cynicism over "officialdom" is still common, and the workings of public servants is typically contrasted to private enterprise motivated by profit.
[126] In fact private companies, especially large ones, also have bureaucracies.
[127] Negative perceptions of "red tape" aside, public services such as schooling, health care, policing or public transport are considered a crucial state function making public bureaucratic action the locus of government power.
Writing in the early 20th century, Max Weber believed that a definitive feature of a developed state had come to be its bureaucratic support.
[128] Weber wrote that the typical characteristics of modern bureaucracy are that officials define its mission, the scope of work is bound by rules, and management is composed of career experts who manage top down, communicating through writing and binding public servants' discretion with rules.
A corollary of the rule of law is the existence of a legal profession sufficiently autonomous to invoke the authority of the independent judiciary; the right to assistance of a barrister in a court proceeding emanates from this corollary—in England the function of barrister or advocate is distinguished from legal counselor.
[131] As the European Court of Human Rights has stated, the law should be adequately accessible to everyone and people should be able to foresee how the law affects them.
In order to maintain professionalism, the practice of law is typically overseen by either a government or independent regulating body such as a bar association, bar council or law society.
Modern lawyers achieve distinct professional identity through specified legal procedures (e.g.
successfully passing a qualifying examination), are required by law to have a special qualification (a legal education earning the student a Bachelor of Laws, a Bachelor of Civil Law, or a Juris Doctor degree.
Higher academic degrees may also be pursued.
Examples include a Master of Laws, a Master of Legal Studies, a Bar Professional Training Course or a Doctor of Laws.
), and are constituted in office by legal forms of appointment (being admitted to the bar).
There are few titles of respect to signify famous lawyers, such as Esquire, to indicate barristers of greater dignity,[133][134] and Doctor of law, to indicate a person who obtained a PhD in Law.
Many Muslim countries have developed similar rules about legal education and the legal profession, but some still allow lawyers with training in traditional Islamic law to practice law before personal status law courts.
[135] In China and other developing countries there are not sufficient professionally trained people to staff the existing judicial systems, and, accordingly, formal standards are more relaxed.
Once accredited, a lawyer will often work in a law firm, in a chambers as a sole practitioner, in a government post or in a private corporation as an internal counsel.
In addition a lawyer may become a legal researcher who provides on-demand legal research through a library, a commercial service or freelance work.
Many people trained in law put their skills to use outside the legal field entirely.
Significant to the practice of law in the common law tradition is the legal research to determine the current state of the law.
This usually entails exploring case-law reports, legal periodicals and legislation.
Law practice also involves drafting documents such as court pleadings, persuasive briefs, contracts, or wills and trusts.
Negotiation and dispute resolution skills (including ADR techniques) are also important to legal practice, depending on the field.
The Classical republican concept of "civil society" dates back to Hobbes and Locke.
[138] Locke saw civil society as people who have "a common established law and judicature to appeal to, with authority to decide controversies between them.
"[139] German philosopher Georg Wilhelm Friedrich Hegel distinguished the "state" from "civil society" (bürgerliche Gesellschaft) in Elements of the Philosophy of Right.
Hegel believed that civil society and the state were polar opposites, within the scheme of his dialectic theory of history.
The modern dipole state–civil society was reproduced in the theories of Alexis de Tocqueville and Karl Marx.
[141][142] Nowadays in post-modern theory civil society is necessarily a source of law, by being the basis from which people form opinions and lobby for what they believe law should be.
As Australian barrister and author Geoffrey Robertson QC wrote of international law,
... one of its primary modern sources is found in the responses of ordinary men and women, and of the non-governmental organizations which many of them support, to the human rights abuses they see on the television screen in their living rooms.
Freedom of speech, freedom of association and many other individual rights allow people to gather, discuss, criticise and hold to account their governments, from which the basis of a deliberative democracy is formed.
The more people are involved with, concerned by and capable of changing how political power is exercised over their lives, the more acceptable and legitimate the law becomes to the people.
The most familiar institutions of civil society include economic markets, profit-oriented firms, families, trade unions, hospitals, universities, schools, charities, debating clubs, non-governmental organisations, neighbourhoods, churches, and religious associations.
All legal systems deal with the same basic issues, but jurisdictions categorise and identify its legal subjects in different ways.
A common distinction is that between "public law" (a term related closely to the state, and including constitutional, administrative and criminal law), and "private law" (which covers contract, tort and property).
[145] In civil law systems, contract and tort fall under a general law of obligations, while trusts law is dealt with under statutory regimes or international conventions.
International, constitutional and administrative law, criminal law, contract, tort, property law and trusts are regarded as the "traditional core subjects",[146] although there are many further disciplines.
International law can refer to three things: public international law, private international law or conflict of laws and the law of supranational organisations.
Constitutional and administrative law govern the affairs of the state.
Constitutional law concerns both the relationships between the executive, legislature and judiciary and the human rights or civil liberties of individuals against the state.
Most jurisdictions, like the United States and France, have a single codified constitution with a bill of rights.
A few, like the United Kingdom, have no such document.
A "constitution" is simply those laws which constitute the body politic, from statute, case law and convention.
A case named Entick v Carrington[154] illustrates a constitutional principle deriving from the common law.
Mr Entick's house was searched and ransacked by Sheriff Carrington.
When Mr Entick complained in court, Sheriff Carrington argued that a warrant from a Government minister, the Earl of Halifax, was valid authority.
However, there was no written statutory provision or court authority.
The leading judge, Lord Camden, stated that,
The great end, for which men entered into society, was to secure their property.
That right is preserved sacred and incommunicable in all instances, where it has not been taken away or abridged by some public law for the good of the whole ...
If no excuse can be found or produced, the silence of the books is an authority against the defendant, and the plaintiff must have judgment.
The fundamental constitutional principle, inspired by John Locke, holds that the individual can do anything except that which is forbidden by law, and the state may do nothing except that which is authorised by law.
[156][157] Administrative law is the chief method for people to hold state bodies to account.
People can sue an agency, local council, public service, or government ministry  for judicial review of actions or decisions, to ensure that they comply with the law, and that the government entity observed required procedure.
The first specialist administrative court was the Conseil d'État set up in 1799, as Napoleon assumed power in France.
Criminal law, also known as penal law, pertains to crimes and punishment.
[159] It thus regulates the definition of and penalties for offences found to have a sufficiently deleterious social impact but, in itself, makes no moral judgment on an offender nor imposes restrictions on society that physically prevent people from committing a crime in the first place.
[160] Investigating, apprehending, charging, and trying suspected offenders is regulated by the law of criminal procedure.
[161] The paradigm case of a crime lies in the proof, beyond reasonable doubt, that a person is guilty of two things.
First, the accused must commit an act which is deemed by society to be criminal, or actus reus (guilty act).
[162] Second, the accused must have the requisite malicious intent to do a criminal act, or mens rea (guilty mind).
However, for so called "strict liability" crimes, an actus reus is enough.
[163] Criminal systems of the civil law tradition distinguish between intention in the broad sense (dolus directus and dolus eventualis), and negligence.
Negligence does not carry criminal responsibility unless a particular crime provides for its punishment.
Examples of crimes include murder, assault, fraud and theft.
In exceptional circumstances defences can apply to specific acts, such as killing in self defence, or pleading insanity.
Another example is in the 19th-century English case of R v Dudley and Stephens, which tested a defence of "necessity".
The Mignonette, sailing from Southampton to Sydney, sank.
Three crew members and Richard Parker, a 17-year-old cabin boy, were stranded on a raft.
They were starving and the cabin boy was close to death.
Driven to extreme hunger, the crew killed and ate the cabin boy.
The crew survived and were rescued, but put on trial for murder.
They argued it was necessary to kill the cabin boy to preserve their own lives.
Lord Coleridge, expressing immense disapproval, ruled, "to preserve one's life is generally speaking a duty, but it may be the plainest and the highest duty to sacrifice it."
The men were sentenced to hang, but public opinion was overwhelmingly supportive of the crew's right to preserve their own lives.
In the end, the Crown commuted their sentences to six months in jail.
Criminal law offences are viewed as offences against not just individual victims, but the community as well.
[160] The state, usually with the help of police, takes the lead in prosecution, which is why in common law countries cases are cited as "The People v ..." or "R (for Rex or Regina) v ...".
Also, lay juries are often used to determine the guilt of defendants on points of fact: juries cannot change legal rules.
Some developed countries still condone capital punishment for criminal activity, but the normal punishment for a crime will be imprisonment, fines, state supervision (such as probation), or community service.
Modern criminal law has been affected considerably by the social sciences, especially with respect to sentencing, legal research, legislation, and rehabilitation.
[167] On the international field, 111 countries are members of the International Criminal Court, which was established to try people for crimes against humanity.
Contract law concerns enforceable promises, and can be summed up in the Latin phrase pacta sunt servanda (agreements must be kept).
[169] In common law jurisdictions, three key elements to the creation of a contract are necessary: offer and acceptance, consideration and the intention to create legal relations.
In Carlill v Carbolic Smoke Ball Company a medical firm advertised that its new wonder drug, the smokeball, would cure people's flu, and if it did not, the buyers would get £100.
Many people sued for their £100 when the drug did not work.
Fearing bankruptcy, Carbolic argued the advert was not to be taken as a serious, legally binding offer.
It was an invitation to treat, mere puffery, a gimmick.
But the Court of Appeal held that to a reasonable man Carbolic had made a serious offer, accentuated by their reassuring statement, "£1000 is deposited".
Equally, people had given good consideration for the offer by going to the "distinct inconvenience" of using a faulty product.
"Read the advertisement how you will, and twist it about as you will", said Lord Justice Lindley, "here is a distinct promise expressed in language which is perfectly unmistakable".
"Consideration" indicates the fact that all parties to a contract have exchanged something of value.
Some common law systems, including Australia, are moving away from the idea of consideration as a requirement.
The idea of estoppel or culpa in contrahendo, can be used to create obligations during pre-contractual negotiations.
[171] In civil law jurisdictions, consideration is not required for a contract to be binding.
[172] In France, an ordinary contract is said to form simply on the basis of a "meeting of the minds" or a "concurrence of wills".
Germany has a special approach to contracts, which ties into property law.
Their 'abstraction principle' (Abstraktionsprinzip) means that the personal obligation of contract forms separately from the title of property being conferred.
When contracts are invalidated for some reason (e.g.
a car buyer is so drunk that he lacks legal capacity to contract)[173] the contractual obligation to pay can be invalidated separately from the proprietary title of the car.
Unjust enrichment law, rather than contract law, is then used to restore title to the rightful owner.
Torts, sometimes called delicts, are civil wrongs.
To have acted tortiously, one must have breached a duty to another person, or infringed some pre-existing legal right.
A simple example might be accidentally hitting someone with a cricket ball.
[175] Under the law of negligence, the most common form of tort, the injured party could potentially claim compensation for their injuries from the party responsible.
The principles of negligence are illustrated by Donoghue v Stevenson.
[176] A friend of Mrs Donoghue ordered an opaque bottle of ginger beer (intended for the consumption of Mrs Donoghue) in a café in Paisley.
Having consumed half of it, Mrs Donoghue poured the remainder into a tumbler.
The decomposing remains of a snail floated out.
She claimed to have suffered from shock, fell ill with gastroenteritis and sued the manufacturer for carelessly allowing the drink to be contaminated.
The House of Lords decided that the manufacturer was liable for Mrs Donoghue's illness.
Lord Atkin took a distinctly moral approach, and said,
The liability for negligence ... is no doubt based upon a general public sentiment of moral wrongdoing for which the offender must pay ...
The rule that you are to love your neighbour becomes in law, you must not injure your neighbour; and the lawyer's question, Who is my neighbour?
receives a restricted reply.
You must take reasonable care to avoid acts or omissions which you can reasonably foresee would be likely to injure your neighbour.
This became the basis for the four principles of negligence: (1) Mr Stevenson owed Mrs Donoghue a duty of care to provide safe drinks (2) he breached his duty of care (3) the harm would not have occurred but for his breach and (4) his act was the proximate cause of her harm.
[176] Another example of tort might be a neighbour making excessively loud noises with machinery on his property.
[59] Under a nuisance claim the noise could be stopped.
Torts can also involve intentional acts, such as assault, battery or trespass.
A better known tort is defamation, which occurs, for example, when a newspaper makes unsupportable allegations that damage a politician's reputation.
[178] More infamous are economic torts, which form the basis of labour law in some countries by making trade unions liable for strikes,[179] when statute does not provide immunity.
Property law governs ownership and possession.
Real property, sometimes called 'real estate', refers to ownership of land and things attached to it.
[182] Personal property, refers to everything else; movable objects, such as computers, cars, jewelry or intangible rights, such as stocks and shares.
A right in rem is a right to a specific piece of property, contrasting to a right in personam which allows compensation for a loss, but not a particular thing back.
Land law forms the basis for most kinds of property law, and is the most complex.
It concerns mortgages, rental agreements, licences, covenants, easements and the statutory systems for land registration.
Regulations on the use of personal property fall under intellectual property, company law, trusts and commercial law.
An example of a basic case of most property law is Armory v Delamirie [1722].
[183] A chimney sweep's boy found a jewel encrusted with precious stones.
He took it to a goldsmith to have it valued.
The goldsmith's apprentice looked at it, sneakily removed the stones, told the boy it was worth three halfpence and that he would buy it.
The boy said he would prefer the jewel back, so the apprentice gave it to him, but without the stones.
The boy sued the goldsmith for his apprentice's attempt to cheat him.
Lord Chief Justice Pratt ruled that even though the boy could not be said to own the jewel, he should be considered the rightful keeper ("finders keepers") until the original owner is found.
In fact the apprentice and the boy both had a right of possession in the jewel (a technical concept, meaning evidence that something could belong to someone), but the boy's possessory interest was considered better, because it could be shown to be first in time.
Possession may be nine tenths of the law, but not all.
This case is used to support the view of property in common law jurisdictions, that the person who can show the best claim to a piece of property, against any contesting party, is the owner.
[184] By contrast, the classic civil law approach to property, propounded by Friedrich Carl von Savigny, is that it is a right good against the world.
Obligations, like contracts and torts, are conceptualised as rights good between individuals.
[185] The idea of property raises many further philosophical and political issues.
Locke argued that our "lives, liberties and estates" are our property because we own our bodies and mix our labour with our surroundings.
Equity is a body of rules that developed in England separately from the "common law".
The common law was administered by judges and barristers.
The Lord Chancellor on the other hand, as the King's keeper of conscience, could overrule the judge-made law if he thought it equitable to do so.
[187] This meant equity came to operate more through principles than rigid rules.
For instance, whereas neither the common law nor civil law systems allow people to split the ownership from the control of one piece of property, equity allows this through an arrangement known as a 'trust'.
'Trustees' control property, whereas the 'beneficial' (or 'equitable') ownership of trust property is held by people known as 'beneficiaries'.
Trustees owe duties to their beneficiaries to take good care of the entrusted property.
[188] In the early case of Keech v Sandford [1722][189] a child had inherited the lease on a market in Romford, London.
Mr Sandford was entrusted to look after this property until the child matured.
But before then, the lease expired.
The landlord had (apparently) told Mr Sandford that he did not want the child to have the renewed lease.
Yet the landlord was happy (apparently) to give Mr Sandford the opportunity of the lease instead.
Mr Sandford took it.
When the child (now Mr Keech) grew up, he sued Mr Sandford for the profit that he had been making by getting the market's lease.
Mr Sandford was meant to be trusted, but he put himself in a position of conflict of interest.
The Lord Chancellor, Lord King, agreed and ordered Mr Sandford should disgorge his profits.
He wrote,
I very well see, if a trustee, on the refusal to renew, might have a lease to himself few trust-estates would be renewed … This may seem very hard, that the trustee is the only person of all mankind who might not have the lease; but it is very proper that the rule should be strictly pursued and not at all relaxed.
Of course, Lord King LC was worried that trustees might exploit opportunities to use trust property for themselves instead of looking after it.
Business speculators using trusts had just recently caused a stock market crash.
Strict duties for trustees made their way into company law and were applied to directors and chief executive officers.
Another example of a trustee's duty might be to invest property wisely or sell it.
[190] This is especially the case for pension funds, the most important form of trust, where investors are trustees for people's savings until retirement.
But trusts can also be set up for charitable purposes, famous examples being the British Museum or the Rockefeller Foundation.
Law spreads far beyond the core subjects into virtually every area of life.
Three categories are presented for convenience, though the subjects intertwine and overlap.
World War I (often abbreviated as WWI or WW1), also known as the First World War or the Great War, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918.
Contemporaneously described as "the war to end all wars",[7] it led to the mobilisation of more than 70 million military personnel, including 60 million Europeans, making it one of the largest wars in history.
[8][9] It is also one of the deadliest conflicts in history,[10] with an estimated nine million combatants and seven million civilian deaths as a direct result of the war, while resulting genocides and the 1918 influenza pandemic caused another 50 to 100 million deaths worldwide.
On 28 June 1914, Gavrilo Princip, a Bosnian Serb Yugoslav nationalist, assassinated the Austro-Hungarian heir Archduke Franz Ferdinand in Sarajevo, leading to the July Crisis.
[12][13] In response, on 23 July Austria-Hungary issued an ultimatum to Serbia.
Serbia's reply failed to satisfy the Austrians, and the two moved to a war footing.
A network of interlocking alliances enlarged the crisis from a bilateral issue in the Balkans to one involving most of Europe.
By July 1914, the great powers of Europe were divided into two coalitions: the Triple Entente—consisting of France, Russia and Britain—and the Triple Alliance of Germany, Austria-Hungary and Italy (the Triple Alliance was primarily defensive in nature, allowing Italy to stay out of the war in 1914).
[14] Russia felt it necessary to back Serbia and, after Austria-Hungary shelled the Serbian capital of Belgrade on the 28th, approved partial mobilisation.
[15] Full Russian mobilisation was announced on the evening of 30 July; on the 31st, Austria-Hungary and Germany did the same, while Germany demanded Russia demobilise within twelve hours.
[16] When Russia failed to comply, Germany declared war on Russia on 1 August in support of Austria-Hungary, with Austria-Hungary following suit on 6th; France ordered full mobilisation in support of Russia on 2 August.
German strategy for a war on two fronts against France and Russia was to rapidly concentrate the bulk of its army in the West to defeat France within four weeks, then shift forces to the East before Russia could fully mobilise; this was later known as the Schlieffen Plan.
[18] On 2 August, Germany demanded free passage through Belgium, an essential element in achieving a quick victory over France.
[19] When this was refused, German forces invaded Belgium on 3 August and declared war on France the same day; the Belgian government invoked the 1839 Treaty of London and in compliance with its obligations under this, Britain declared war on Germany on 4 August.
[20][21] On 12 August, Britain and France also declared war on Austria-Hungary; on the 23rd, Japan sided with the Entente, seizing German possessions in China and the Pacific.
In November 1914, the Ottoman Empire entered the war on the side of the Alliance, opening fronts in the Caucasus, Mesopotamia and the Sinai Peninsula.
The war was fought in and drew upon each power's colonial empire as well, spreading the conflict to Africa and across the globe.
The Entente and its allies would eventually become known as the Allied Powers, while the grouping of Austria-Hungary, Germany and their allies would become known as the Central Powers.
The German advance into France was halted at the Battle of the Marne and by the end of 1914, the Western Front settled into a battle of attrition, marked by a long series of trench lines that changed little until 1917 (the Eastern Front, by contrast, was marked by much greater exchanges of territory).
In 1915, Italy joined the Allied Powers and opened a front in the Alps.
Bulgaria joined the Central Powers in 1915 and Greece joined the Allies in 1917, expanding the war in the Balkans.
The United States initially remained neutral, though even while neutral it became an important supplier of war material to the Allies.
Eventually, after the sinking of American merchant ships by German submarines, and the revelation that the Germans were trying to incite Mexico to make war on the United States, the U.S. declared war on Germany on 6 April 1917.
Trained American forces would not begin arriving at the front in large numbers until mid-1918, but ultimately the American Expeditionary Force would reach some two million troops.
Though Serbia was defeated in 1915, and Romania joined the Allied Powers in 1916 only to be defeated in 1917, none of the great powers were knocked out of the war until 1918.
The 1917 February Revolution in Russia replaced the Tsarist autocracy with the Provisional Government, but continuing discontent at the cost of the war led to the October Revolution, the creation of the Soviet Socialist Republic, and the signing of the Treaty of Brest-Litovsk by the new government in March 1918, ending Russia's involvement in the war.
This allowed the transfer of large numbers of German troops from the East to the Western Front, resulting in the German March 1918 Offensive.
This offensive was initially successful, but failed to score a decisive victory and exhausted the last of the German reserves.
The Allies rallied and drove the Germans back in their Hundred Days Offensive, a continual series of attacks to which the Germans had no reply.
[23] Bulgaria was the first Central Power to sign an armistice—the Armistice of Salonica on 29 September 1918.
On 30 October, the Ottoman Empire capitulated, signing the Armistice of Mudros.
[24] On 4 November, the Austro-Hungarian empire agreed to the Armistice of Villa Giusti.
With its allies defeated, revolution at home, and the military no longer willing to fight, Kaiser Wilhelm abdicated on 9 November and Germany signed an armistice on 11 November 1918, effectively ending the war.
World War I was a significant turning point in the political, cultural, economic, and social climate of the world.
It is considered to mark the end of the Second Industrial Revolution and the Pax Britannica.
The war and its immediate aftermath sparked numerous revolutions and uprisings.
The Big Four (Britain, France, the United States, and Italy) imposed their terms on the defeated powers in a series of treaties agreed at the 1919 Paris Peace Conference, the most well known being the German peace treaty—the Treaty of Versailles.
[25] Ultimately, as a result of the war the Austro-Hungarian, German, Ottoman, and Russian Empires ceased to exist, with numerous new states created from their remains.
However, despite the conclusive Allied victory (and the creation of the League of Nations during the Peace Conference, intended to prevent future wars), a Second World War would follow just over twenty years later.
The term "First World War" was first used in September 1914 by German biologist and philosopher Ernst Haeckel, who claimed that "there is no doubt that the course and character of the feared 'European War' ... will become the first world war in the full sense of the word,"[26] citing a wire service report in The Indianapolis Star on 20 September 1914.
Prior to World War II, the events of 1914–1918 were generally known as the Great War or simply the World War.
[27][28] In October 1914, the Canadian magazine Maclean's wrote, "Some wars name themselves.
This is the Great War.
"[29] Contemporary Europeans also referred to it as "the war to end war" or "the war to end all wars" due to their perception of its then-unparalleled scale and devastation.
[30] After World War II began in 1939, the terms became more standard, with British Empire historians, including Canadians, favouring "The First World War" and Americans "World War I".
For much of the 19th century, the major European powers had tried to maintain a tenuous balance of power among themselves, resulting in a complex network of political and military alliances.
[32] The biggest challenges to this were Britain's withdrawal into so-called splendid isolation, the decline of the Ottoman Empire and the post-1848 rise of Prussia under Otto von Bismarck.
Victory in the 1866 Austro-Prussian War established Prussian hegemony in Germany, while victory over France in the 1870–1871 Franco-Prussian War unified the German states into a German Reich under Prussian leadership.
In 1873, to isolate France and avoid a war on two fronts, Bismarck negotiated the League of the Three Emperors (German: Dreikaiserbund) between Austria-Hungary, Russia and Germany.
Concerned by Russia's victory in the 1877–1878 Russo-Turkish War and their influence in the Balkans, the League was dissolved in 1878, with Germany and Austria-Hungary subsequently forming the 1879 Dual Alliance; this became the Triple Alliance when Italy joined in 1882.
[33] [34]
The practical details of these alliances were limited, since their primary purpose was to ensure cooperation between the three Imperial Powers and isolate France.
Attempts by Britain in 1880 to resolve colonial tensions with Russia and diplomatic moves by France led to Bismarck reforming the League in 1881.
[35] When the League finally lapsed in 1887, it was replaced by the Reinsurance Treaty, a secret agreement between Germany and Russia to remain neutral if either were attacked by France or Austria-Hungary.
In 1890, the new German Emperor, Kaiser Wilhelm II, forced Bismarck to retire and was persuaded not to renew the Reinsurance Treaty by the new Chancellor, Leo von Caprivi.
[36] This allowed France to counteract the Triple Alliance with the Franco-Russian Alliance of 1894 and the 1904 Entente Cordiale with Britain, while in 1907 Britain and Russia signed the Anglo-Russian Convention.
The agreements did not constitute formal alliances, but by settling long-standing colonial disputes, they made British entry into any future conflict involving France or Russia a possibility; these interlocking bilateral agreements became known as the Triple Entente.
Victory in the 1871 Franco-Prussian War and the creation of the German Reich led to a massive increase in Germany's economic and industrial strength.
Admiral Alfred von Tirpitz and Wilhelm II, who became Emperor in 1890, sought to use that to create a Kaiserliche Marine or Imperial German Navy to compete with Britain's Royal Navy for world naval supremacy.
[38] Their rationale was based on the ideas of US naval strategist Alfred Mahan, who argued that whoever ruled the sea also ruled the world; Tirpitz had Mahan's books translated into German, while Wilhelm made them required reading for his officers.
[39] However, Wilhelm annoyed his ministers by publicly declaring one motive to be his childhood admiration of the Royal Navy, which he had visited "with kind aunts and friendly admirals.
The result was the Anglo-German naval arms race.
With the launch of HMS Dreadnought in 1906, the Royal Navy increased its advantage over its German rival and continued to do so.
[38] By 1912, the German economy could no longer support both naval expansion and the largest permanent army in Europe, with Chancellor Theobald von Bethmann-Hollweg acknowledging defeat.
In many ways, it was a strategic disaster for Germany, diverting huge resources to create a navy large enough to antagonise Britain but not defeat it.
Ending the naval arms race reduced tensions between Britain and Germany but did not lead to reductions elsewhere; in 1913, Germany approved an increase in its standing army by 170,000 men, Russia committed to another 500,000 men over the next three years, while France extended compulsory military service from two to three years.
Between 1870 and 1914, total military spending by Austria, Germany, Italy, and Russia increased from £94 million to £394 million (equivalent to £37 billion in 2018).
The largest proportional increases occurred in Germany (+73%) and Russia (+39%).
[41][dubious  – discuss]
In October 1908, Austria-Hungary precipitated the Bosnian crisis of 1908–1909 by officially annexing the former Ottoman territory of Bosnia and Herzegovina, which it had occupied since 1878.
This angered the Kingdom of Serbia and its patron, the Pan-Slavic and Orthodox Russian Empire.
Russian political manoeuvring in the region destabilised peace accords that were already fracturing in the Balkans, which came to be known as the "powder keg of Europe".
In 1912 and 1913, the First Balkan War was fought between the Balkan League and the fracturing Ottoman Empire.
The resulting Treaty of London further shrank the Ottoman Empire, creating an independent Albanian state while enlarging the territorial holdings of Bulgaria, Serbia, Montenegro, and Greece.
When Bulgaria attacked Serbia and Greece on 16 June 1913, it sparked the 33-day Second Balkan War, by the end of which it lost most of Macedonia to Serbia and Greece, and Southern Dobruja to Romania, further destabilising the region.
[43] The Great Powers were able to keep these Balkan conflicts contained, but the next one would spread throughout Europe and beyond.
On 28 June 1914, Archduke Franz Ferdinand, heir presumptive to the Austro-Hungarian Empire, visited the Bosnian capital, Sarajevo.
A group of six assassins (Cvjetko Popović, Gavrilo Princip, Muhamed Mehmedbašić, Nedeljko Čabrinović, Trifko Grabež, and Vaso Čubrilović) from the Yugoslavist group Mlada Bosna, supplied with arms by the Serbian Black Hand, gathered on the street where the Archduke's motorcade was to pass, with the intention of assassinating him.
The political objective of the assassination was to break off Austria-Hungary's South Slav provinces, which Austria-Hungary had annexated from the Ottoman Empire, so they could be combined into a Yugoslavia.
Čabrinović threw a grenade at the car, but missed.
Some nearby were injured by the blast, but Ferdinand's convoy carried on.
The other assassins failed to act as the cars drove past them.
About an hour later, when Ferdinand was returning from a visit at the Sarajevo Hospital with those wounded in the assassination attempt, the convoy took a wrong turn into a street where, by coincidence, Princip stood.
With a pistol, Princip shot and killed Ferdinand and his wife Sophie.
Although they were reportedly not personally close, the Emperor Franz Joseph was profoundly shocked and upset.
The reaction among the people in Austria, however, was mild, almost indifferent.
As historian Zbyněk Zeman later wrote, "the event almost failed to make any impression whatsoever.
On Sunday and Monday (28 and 29 June), the crowds in Vienna listened to music and drank wine, as if nothing had happened.
"[46][47] Nevertheless, the political effect of the murder of the heir to the throne was significant, and was described by historian Christopher Clark on the BBC Radio 4 series Month of Madness as a "9/11 effect, a terrorist event charged with historic meaning, transforming the political chemistry in Vienna.
The Austro-Hungarian authorities encouraged the subsequent anti-Serb riots in Sarajevo, in which Bosnian Croats and Bosniaks killed two Bosnian Serbs and damaged numerous Serb-owned buildings.
[49][50] Violent actions against ethnic Serbs were also organised outside Sarajevo, in other cities in Austro-Hungarian-controlled Bosnia and Herzegovina, Croatia and Slovenia.
Austro-Hungarian authorities in Bosnia and Herzegovina imprisoned and extradited approximately 5,500 prominent Serbs, 700 to 2,200 of whom died in prison.
A further 460 Serbs were sentenced to death.
A predominantly Bosniak special militia known as the Schutzkorps was established and carried out the persecution of Serbs.
The assassination led to a month of diplomatic manoeuvring between Austria-Hungary, Germany, Russia, France and Britain, called the July Crisis.
Austria-Hungary correctly believed that Serbian officials (especially the officers of the Black Hand) were involved in the plot to murder the Archduke, and wanted to finally end Serbian interference in Bosnia.
[55] On 23 July, Austria-Hungary delivered to Serbia the July Ultimatum, a series of ten demands that were made intentionally unacceptable, in an effort to provoke a war with Serbia.
[56] Serbia decreed general mobilisation on the 25th.
Serbia accepted all of the terms of the ultimatum except for article six, which demanded that Austrian delegates be allowed in Serbia for the purpose of participation in the investigation into the assassination.
[57] Following this, Austria broke off diplomatic relations with Serbia and, the next day, ordered a partial mobilisation.
Finally, on 28 July 1914, a month after the assassination, Austria-Hungary declared war on Serbia.
On 29 July, Russia, in support of Serbia, declared partial mobilisation against Austria-Hungary.
[58] On the 30th, Russia ordered general mobilisation.
German Chancellor Bethmann-Hollweg waited until the 31st for an appropriate response, when Germany declared Erklärung des Kriegszustandes, or "Statement on the war status".
[16][59] Kaiser Wilhelm II asked his cousin, Tsar Nicolas II, to suspend the Russian general mobilisation.
When he refused, Germany issued an ultimatum demanding its mobilisation be stopped, and a commitment not to support Serbia.
Another was sent to France, asking her not to support Russia if it were to come to the defence of Serbia.
On 1 August, after the Russian response, Germany mobilised and declared war on Russia.
This also led to the general mobilisation in Austria-Hungary on 4 August.
The German government issued demands to France that it remain neutral as they had to decide which deployment plan to implement, it being extremely difficult to change the deployment whilst it was underway.
The modified German Schlieffen Plan, Aufmarsch II West, would deploy 80% of the army in the west, while Aufmarsch I Ost and Aufmarsch II Ost would deploy 60% in the west and 40% in the east.
The French did not respond, but sent a mixed message by ordering their troops to withdraw 10 km (6 mi) from the border to avoid any incidents, and at the same time ordered the mobilisation of their reserves.
Germany responded by mobilising its own reserves and implementing Aufmarsch II West.
On 1 August, Wilhelm ordered General Helmuth von Moltke the Younger to "march the whole of the ... army to the East" after being wrongly informed that the British would remain neutral if France was not attacked.
Moltke told the Kaiser that attempting to redeploy a million men was unthinkable, and that making it possible for the French to attack the Germans "in the rear" would prove disastrous.
Yet Wilhelm insisted that the German army should not march into Luxembourg until he received a telegram sent by his cousin George V, who made it clear that there had been a misunderstanding.
Eventually the Kaiser told Moltke, "Now you can do what you want.
On 2 August, Germany occupied Luxembourg, and on 3 August declared war on France; on the same day, they sent the Belgian government an ultimatum demanding unimpeded right of way through any part of Belgium, which was refused.
Early on the morning of 4 August, the Germans invaded; King Albert ordered his military to resist and called for assistance under the 1839 Treaty of London.
[62][63][64] Britain demanded Germany comply with the Treaty and respect Belgian neutrality; it declared war on Germany at 19:00 UTC on 4 August 1914 (effective from 23:00), following an "unsatisfactory reply".
The strategy of the Central Powers suffered from miscommunication.
Germany had promised to support Austria-Hungary's invasion of Serbia, but interpretations of what this meant differed.
Previously tested deployment plans had been replaced early in 1914, but those had never been tested in exercises.
Austro-Hungarian leaders believed Germany would cover its northern flank against Russia.
[66] Germany, however, envisioned Austria-Hungary directing most of its troops against Russia, while Germany dealt with France.
This confusion forced the Austro-Hungarian Army to divide its forces between the Russian and Serbian fronts.
Austria invaded and fought the Serbian army at the Battle of Cer and Battle of Kolubara beginning on 12 August.
Over the next two weeks, Austrian attacks were thrown back with heavy losses, which marked the first major Allied victories of the war and dashed Austro-Hungarian hopes of a swift victory.
As a result, Austria had to keep sizeable forces on the Serbian front, weakening its efforts against Russia.
[67] Serbia's defeat of the Austro-Hungarian invasion of 1914 has been called one of the major upset victories of the twentieth century.
When the war began, the German Order of Battle placed 80% of the army in the West, with the remainder acting as a screening force in the East.
The plan was to quickly knock France out of the war, then redeploy to the East and do the same to Russia.
The German offensive in the West was officially titled Aufmarsch II West, but is better known as the Schlieffen Plan, after its original creator.
Schlieffen deliberately kept the German left (i.e.
its positions in Alsace-Lorraine) weak to lure the French into attacking there, while the majority were allocated to the German right, so as to sweep through Belgium, encircle Paris and trap the French armies against the Swiss border (the French charged into Alsace-Lorraine on the outbreak of war as envisaged by their Plan XVII, thus actually aiding this strategy).
[69] However, Schlieffen's successor Moltke grew concerned that the French might push too hard on his left flank.
As such, as the German Army increased in size in the years leading up to the war, he changed the allocation of forces between the German right and left wings from 85:15 to 70:30.
Ultimately, Moltke's changes meant insufficient forces to achieve decisive success and thus unrealistic goals and timings.
[70][dubious  – discuss]
The initial German advance in the West was very successful: by the end of August the Allied left, which included the British Expeditionary Force (BEF), was in full retreat; French casualties in the first month exceeded 260,000, including 27,000 killed on 22 August during the Battle of the Frontiers.
[71] German planning provided broad strategic instructions, while allowing army commanders considerable freedom in carrying them out at the front; this worked well in 1866 and 1870 but in 1914, von Kluck used this freedom to disobey orders, opening a gap between the German armies as they closed on Paris.
[72] The French and British exploited this gap to halt the German advance east of Paris at the First Battle of the Marne from 5 to 12 September and push the German forces back some 50 km (31 mi).
In 1911, the Russian Stavka had agreed with the French to attack Germany within 15 days of mobilisation; this was unrealistic and the two Russian armies that entered East Prussia on 17 August did so without many of their support elements.
[73] The Russian Second Army was effectively destroyed at the Battle of Tannenberg on 26–30 August but the Russian advance caused the Germans to re-route their 8th Field Army from France to East Prussia, a factor in Allied victory on the Marne.
[citation needed]
By the end of 1914, German troops held strong defensive positions inside France, controlled the bulk of France's domestic coalfields and had inflicted 230,000 more casualties than it lost itself.
However, communications problems and questionable command decisions cost Germany the chance of a decisive outcome while it had failed to achieve the primary objective of avoiding a long, two-front war.
[74] This amounted to a strategic defeat; shortly after the Marne, Crown Prince Wilhelm told an American reporter; "We have lost the war.
It will go on for a long time but lost it is already.
New Zealand occupied German Samoa (later Western Samoa) on 30 August 1914.
On 11 September, the Australian Naval and Military Expeditionary Force landed on the island of Neu Pommern (later New Britain), which formed part of German New Guinea.
On 28 October, the German cruiser SMS Emden sank the Russian cruiser Zhemchug in the Battle of Penang.
Japan seized Germany's Micronesian colonies and, after the Siege of Tsingtao, the German coaling port of Qingdao on the Chinese Shandong peninsula.
As Vienna refused to withdraw the Austro-Hungarian cruiser SMS Kaiserin Elisabeth from Tsingtao, Japan declared war not only on Germany, but also on Austria-Hungary; the ship participated in the defence of Tsingtao where it was sunk in November 1914.
[76] Within a few months, the Allied forces had seized all the German territories in the Pacific; only isolated commerce raiders and a few holdouts in New Guinea remained.
Some of the first clashes of the war involved British, French, and German colonial forces in Africa.
On 6–7 August, French and British troops invaded the German protectorate of Togoland and Kamerun.
On 10 August, German forces in South-West Africa attacked South Africa; sporadic and fierce fighting continued for the rest of the war.
The German colonial forces in German East Africa, led by Colonel Paul von Lettow-Vorbeck, fought a guerrilla warfare campaign during World War I and only surrendered two weeks after the armistice took effect in Europe.
Germany attempted to use Indian nationalism and pan-Islamism to its advantage, instigating uprisings in India, and sending a mission that urged Afghanistan to join the war on the side of Central Powers.
However, contrary to British fears of a revolt in India, the outbreak of the war saw an unprecedented outpouring of loyalty and goodwill towards Britain.
[80][81] Indian political leaders from the Indian National Congress and other groups were eager to support the British war effort, since they believed that strong support for the war effort would further the cause of Indian Home Rule.
[citation needed] The Indian Army in fact outnumbered the British Army at the beginning of the war; about 1.3 million Indian soldiers and labourers served in Europe, Africa, and the Middle East, while the central government and the princely states sent large supplies of food, money, and ammunition.
In all, 140,000 men served on the Western Front and nearly 700,000 in the Middle East.
Casualties of Indian soldiers totalled 47,746 killed and 65,126 wounded during World War I.
The suffering engendered by the war, as well as the failure of the British government to grant self-government to India after the end of hostilities, bred disillusionment and fuelled the campaign for full independence that would be led by Mohandas K. Gandhi and others.
Military tactics developed before World War I failed to keep pace with advances in technology and had become obsolete.
These advances had allowed the creation of strong defensive systems, which out-of-date military tactics could not break through for most of the war.
Barbed wire was a significant hindrance to massed infantry advances, while artillery, vastly more lethal than in the 1870s, coupled with machine guns, made crossing open ground extremely difficult.
[84] Commanders on both sides failed to develop tactics for breaching entrenched positions without heavy casualties.
In time, however, technology began to produce new offensive weapons, such as gas warfare and the tank.
After the First Battle of the Marne (5–12 September 1914), Allied and German forces unsuccessfully tried to outflank each other, a series of manoeuvres later known as the "Race to the Sea".
By the end of 1914, the opposing forces were left confronting each other along an uninterrupted line of entrenched positions from Alsace to Belgium's North Sea coast.
[12] Since the Germans were able to choose where to stand, they normally had the advantage of the high ground; in addition, their trenches tended to be better built, since Anglo-French trenches were initially intended as "temporary," preparatory to breaking the German defences.
Both sides tried to break the stalemate using scientific and technological advances.
On 22 April 1915, at the Second Battle of Ypres, the Germans (violating the Hague Convention) used chlorine gas for the first time on the Western Front.
Several types of gas soon became widely used by both sides, and though it never proved a decisive, battle-winning weapon, poison gas became one of the most-feared and best-remembered horrors of the war.
[87][88] Tanks were developed by Britain and France and were first used in combat by the British during the Battle of Flers–Courcelette (part of the Battle of the Somme) on 15 September 1916, with only partial success.
However, their effectiveness would grow as the war progressed; the Allies built tanks in large numbers, whilst the Germans employed only a few of their own design, supplemented by captured Allied tanks.
Neither side proved able to deliver a decisive blow for the next two years.
Throughout 1915–17, the British Empire and France suffered more casualties than Germany, because of both the strategic and tactical stances chosen by the sides.
Strategically, while the Germans only mounted one major offensive, the Allies made several attempts to break through the German lines.
In February 1916 the Germans attacked French defensive positions at the Battle of Verdun, lasting until December 1916.
The Germans made initial gains, before French counter-attacks returned matters to near their starting point.
Casualties were greater for the French, but the Germans bled heavily as well, with anywhere from 700,000[89] to 975,000[90] casualties suffered between the two combatants.
Verdun became a symbol of French determination and self-sacrifice.
The Battle of the Somme was an Anglo-French offensive of July to November 1916.
The opening day of the offensive (1 July 1916) was the bloodiest day in the history of the British Army, suffering 57,470 casualties, including 19,240 dead.
The entire Somme offensive cost the British Army some 420,000 casualties.
The French suffered another estimated 200,000 casualties and the Germans an estimated 500,000.
[92] Gun fire wasn't the only factor taking lives; the diseases that emerged in the trenches were a major killer on both sides.
The living conditions made it so that countless diseases and infections occurred, such as trench foot, shell shock, blindness/burns from mustard gas, lice, trench fever, cooties (body lice) and the ‘Spanish Flu’.
[93][unreliable source?]
To maintain morale, wartime censors minimised early reports of widespread influenza illness and mortality in Germany, the United Kingdom, France, and the United States.
[94][95] Papers were free to report the epidemic's effects in neutral Spain (such as the grave illness of King Alfonso XIII).
[96] This created a false impression of Spain as especially hard hit,[97] thereby giving rise to the pandemic's nickname, "Spanish Flu".
Protracted action at Verdun throughout 1916,[99] combined with the bloodletting at the Somme, brought the exhausted French army to the brink of collapse.
Futile attempts using frontal assault came at a high price for both the British and the French and led to the widespread French Army Mutinies, after the failure of the costly Nivelle Offensive of April–May 1917.
[100] The concurrent British Battle of Arras was more limited in scope, and more successful, although ultimately of little strategic value.
[101][102] A smaller part of the Arras offensive, the capture of Vimy Ridge by the Canadian Corps, became highly significant to that country: the idea that Canada's national identity was born out of the battle is an opinion widely held in military and general histories of Canada.
The last large-scale offensive of this period was a British attack (with French support) at Passchendaele (July–November 1917).
This offensive opened with great promise for the Allies, before bogging down in the October mud.
Casualties, though disputed, were roughly equal, at some 200,000–400,000 per side.
The years of trench warfare on the Western front achieved no major exchanges of territory and, as a result, are often thought of as static and unchanging.
However, throughout this period, British, French, and German tactics constantly evolved to meet new battlefield challenges.
At the start of the war, the German Empire had cruisers scattered across the globe, some of which were subsequently used to attack Allied merchant shipping.
The British Royal Navy systematically hunted them down, though not without some embarrassment from its inability to protect Allied shipping.
Before the beginning of the war, it was widely understood that Britain held the position of strongest, most influential navy in the world.
[105][unreliable source?]
The publishing of the book The Influence of Sea Power upon History by Alfred Thayer Mahan in 1890 was intended to encourage the United States to increase their naval power.
Instead, this book made it to Germany and inspired its readers to try to over-power the British Royal Navy.
[106] For example, the German detached light cruiser SMS Emden, part of the East Asia Squadron stationed at Qingdao, seized or destroyed 15 merchantmen, as well as sinking a Russian cruiser and a French destroyer.
However, most of the German East-Asia squadron—consisting of the armoured cruisers SMS Scharnhorst and Gneisenau, light cruisers Nürnberg and Leipzig and two transport ships—did not have orders to raid shipping and was instead underway to Germany when it met British warships.
The German flotilla and Dresden sank two armoured cruisers at the Battle of Coronel, but was virtually destroyed at the Battle of the Falkland Islands in December 1914, with only Dresden and a few auxiliaries escaping, but after the Battle of Más a Tierra these too had been destroyed or interned.
Soon after the outbreak of hostilities, Britain began a naval blockade of Germany.
The strategy proved effective, cutting off vital military and civilian supplies, although this blockade violated accepted international law codified by several international agreements of the past two centuries.
[108] Britain mined international waters to prevent any ships from entering entire sections of ocean, causing danger to even neutral ships.
[109] Since there was limited response to this tactic of the British, Germany expected a similar response to its unrestricted submarine warfare.
The Battle of Jutland (German: Skagerrakschlacht, or "Battle of the Skagerrak") in May/June 1916 developed into the largest naval battle of the war.
It was the only full-scale clash of battleships during the war, and one of the largest in history.
The Kaiserliche Marine's High Seas Fleet, commanded by Vice Admiral Reinhard Scheer, fought the Royal Navy's Grand Fleet, led by Admiral Sir John Jellicoe.
The engagement was a stand off, as the Germans were outmanoeuvred by the larger British fleet, but managed to escape and inflicted more damage to the British fleet than they received.
Strategically, however, the British asserted their control of the sea, and the bulk of the German surface fleet remained confined to port for the duration of the war.
German U-boats attempted to cut the supply lines between North America and Britain.
[112] The nature of submarine warfare meant that attacks often came without warning, giving the crews of the merchant ships little hope of survival.
[112][113] The United States launched a protest, and Germany changed its rules of engagement.
After the sinking of the passenger ship RMS Lusitania in 1915, Germany promised not to target passenger liners, while Britain armed its merchant ships, placing them beyond the protection of the "cruiser rules", which demanded warning and movement of crews to "a place of safety" (a standard that lifeboats did not meet).
[114] Finally, in early 1917, Germany adopted a policy of unrestricted submarine warfare, realising that the Americans would eventually enter the war.
[112][115] Germany sought to strangle Allied sea lanes before the United States could transport a large army overseas, but after initial successes eventually failed to do so.
The U-boat threat lessened in 1917, when merchant ships began travelling in convoys, escorted by destroyers.
This tactic made it difficult for U-boats to find targets, which significantly lessened losses; after the hydrophone and depth charges were introduced, accompanying destroyers could attack a submerged submarine with some hope of success.
Convoys slowed the flow of supplies, since ships had to wait as convoys were assembled.
The solution to the delays was an extensive program of building new freighters.
Troopships were too fast for the submarines and did not travel the North Atlantic in convoys.
[116] The U-boats had sunk more than 5,000 Allied ships, at a cost of 199 submarines.
World War I also saw the first use of aircraft carriers in combat, with HMS Furious launching Sopwith Camels in a successful raid against the Zeppelin hangars at Tondern in July 1918, as well as blimps for antisubmarine patrol.
Faced with Russia in the east, Austria-Hungary could spare only one-third of its army to attack Serbia.
After suffering heavy losses, the Austrians briefly occupied the Serbian capital, Belgrade.
A Serbian counter-attack in the Battle of Kolubara succeeded in driving them from the country by the end of 1914.
For the first ten months of 1915, Austria-Hungary used most of its military reserves to fight Italy.
German and Austro-Hungarian diplomats, however, scored a coup by persuading Bulgaria to join the attack on Serbia.
[120] The Austro-Hungarian provinces of Slovenia, Croatia and Bosnia provided troops for Austria-Hungary in the fight with Serbia, Russia and Italy.
Montenegro allied itself with Serbia.
Bulgaria declared war on Serbia on 12 October 1915 and joined in the attack by the Austro-Hungarian army under Mackensen's army of 250,000 that was already underway.
Serbia was conquered in a little more than a month, as the Central Powers, now including Bulgaria, sent in 600,000 troops total.
The Serbian army, fighting on two fronts and facing certain defeat, retreated into northern Albania.
The Serbs suffered defeat in the Battle of Kosovo.
Montenegro covered the Serbian retreat towards the Adriatic coast in the Battle of Mojkovac in 6–7 January 1916, but ultimately the Austrians also conquered Montenegro.
The surviving Serbian soldiers were evacuated by ship to Greece.
[122] After conquest, Serbia was divided between Austro-Hungary and Bulgaria.
In late 1915, a Franco-British force landed at Salonica in Greece to offer assistance and to pressure its government to declare war against the Central Powers.
However, the pro-German King Constantine I dismissed the pro-Allied government of Eleftherios Venizelos before the Allied expeditionary force arrived.
[124] The friction between the King of Greece and the Allies continued to accumulate with the National Schism, which effectively divided Greece between regions still loyal to the king and the new provisional government of Venizelos in Salonica.
After intense negotiations and an armed confrontation in Athens between Allied and royalist forces (an incident known as Noemvriana), the King of Greece resigned and his second son Alexander took his place; Greece officially joined the war on the side of the Allies in June 1917.
The Macedonian Front was initially mostly static.
French and Serbian forces retook limited areas of Macedonia by recapturing Bitola on 19 November 1916 following the costly Monastir Offensive, which brought stabilisation of the front.
Serbian and French troops finally made a breakthrough in September 1918 in the Vardar Offensive, after most of the German and Austro-Hungarian troops had been withdrawn.
The Bulgarians were defeated at the Battle of Dobro Pole, and by 25 September British and French troops had crossed the border into Bulgaria proper as the Bulgarian army collapsed.
Bulgaria capitulated four days later, on 29 September 1918.
[126] The German high command responded by despatching troops to hold the line, but these forces were far too weak to reestablish a front.
The disappearance of the Macedonian Front meant that the road to Budapest and Vienna was now opened to Allied forces.
Hindenburg and Ludendorff concluded that the strategic and operational balance had now shifted decidedly against the Central Powers and, a day after the Bulgarian collapse, insisted on an immediate peace settlement.
The Ottomans threatened Russia's Caucasian territories and Britain's communications with India via the Suez Canal.
As the conflict progressed, the Ottoman Empire took advantage of the European powers' preoccupation with the war and conducted large-scale ethnic cleansing of the indigenous Armenian, Greek, and Assyrian Christian populations, known as the Armenian Genocide, Greek Genocide, and Assyrian Genocide.
The British and French opened overseas fronts with the Gallipoli (1915) and Mesopotamian campaigns (1914).
In Gallipoli, the Ottoman Empire successfully repelled the British, French, and Australian and New Zealand Army Corps (ANZACs).
In Mesopotamia, by contrast, after the defeat of the British defenders in the Siege of Kut by the Ottomans (1915–16), British Imperial forces reorganised and captured Baghdad in March 1917.
The British were aided in Mesopotamia by local Arab and Assyrian tribesmen, while the Ottomans employed local Kurdish and Turcoman tribes.
Further to the west, the Suez Canal was defended from Ottoman attacks in 1915 and 1916; in August, a German and Ottoman force was defeated at the Battle of Romani by the ANZAC Mounted Division and the 52nd (Lowland) Infantry Division.
Following this victory, an Egyptian Expeditionary Force advanced across the Sinai Peninsula, pushing Ottoman forces back in the Battle of Magdhaba in December and the Battle of Rafa on the border between the Egyptian Sinai and Ottoman Palestine in January 1917.
Russian armies generally had success in the Caucasus.
Enver Pasha, supreme commander of the Ottoman armed forces, was ambitious and dreamed of re-conquering central Asia and areas that had been lost to Russia previously.
He was, however, a poor commander.
[134] He launched an offensive against the Russians in the Caucasus in December 1914 with 100,000 troops, insisting on a frontal attack against mountainous Russian positions in winter.
He lost 86% of his force at the Battle of Sarikamish.
The Ottoman Empire, with German support, invaded Persia (modern Iran) in December 1914 in an effort to cut off British and Russian access to petroleum reservoirs around Baku near the Caspian Sea.
[136] Persia, ostensibly neutral, had long been under the spheres of British and Russian influence.
The Ottomans and Germans were aided by Kurdish and Azeri forces, together with a large number of major Iranian tribes, such as the Qashqai, Tangistanis, Luristanis, and Khamseh, while the Russians and British had the support of Armenian and Assyrian forces.
The Persian Campaign was to last until 1918 and end in failure for the Ottomans and their allies.
However, the Russian withdrawal from the war in 1917 led to Armenian and Assyrian forces, who had hitherto inflicted a series of defeats upon the forces of the Ottomans and their allies, being cut off from supply lines, outnumbered, outgunned and isolated, forcing them to fight and flee towards British lines in northern Mesopotamia.
General Yudenich, the Russian commander from 1915 to 1916, drove the Turks out of most of the southern Caucasus with a string of victories.
[135] In 1917, Russian Grand Duke Nicholas assumed command of the Caucasus front.
Nicholas planned a railway from Russian Georgia to the conquered territories, so that fresh supplies could be brought up for a new offensive in 1917.
However, in March 1917 (February in the pre-revolutionary Russian calendar), the Czar abdicated in the course of the February Revolution, and the Russian Caucasus Army began to fall apart.
The Arab Revolt, instigated by the Arab bureau of the British Foreign Office, started June 1916 with the Battle of Mecca, led by Sherif Hussein of Mecca, and ended with the Ottoman surrender of Damascus.
Fakhri Pasha, the Ottoman commander of Medina, resisted for more than two and half years during the Siege of Medina before surrendering in January 1919.
The Senussi tribe, along the border of Italian Libya and British Egypt, incited and armed by the Turks, waged a small-scale guerrilla war against Allied troops.
The British were forced to dispatch 12,000 troops to oppose them in the Senussi Campaign.
Their rebellion was finally crushed in mid-1916.
Total Allied casualties on the Ottoman fronts amounted 650,000 men.
Total Ottoman casualties were 725,000 (325,000 dead and 400,000 wounded).
Italy had been allied with the German and Austro-Hungarian Empires since 1882 as part of the Triple Alliance.
However, the nation had its own designs on Austrian territory in Trentino, the Austrian Littoral, Fiume (Rijeka) and Dalmatia.
Rome had a secret 1902 pact with France, effectively nullifying its part in the Triple Alliance;[141] Italy secretly agreed with France to remain neutral if the latter was attacked by Germany.
[14] At the start of hostilities, Italy refused to commit troops, arguing that the Triple Alliance was defensive and that Austria-Hungary was an aggressor.
The Austro-Hungarian government began negotiations to secure Italian neutrality, offering the French colony of Tunisia in return.
The Allies made a counter-offer in which Italy would receive the Southern Tyrol, Austrian Littoral and territory on the Dalmatian coast after the defeat of Austria-Hungary.
This was formalised by the Treaty of London.
Further encouraged by the Allied invasion of Turkey in April 1915, Italy joined the Triple Entente and declared war on Austria-Hungary on 23 May.
Fifteen months later, Italy declared war on Germany.
The Italians had numerical superiority, but this advantage was lost, not only because of the difficult terrain in which the fighting took place, but also because of the strategies and tactics employed.
[143] Field Marshal Luigi Cadorna, a staunch proponent of the frontal assault, had dreams of breaking into the Slovenian plateau, taking Ljubljana and threatening Vienna.
On the Trentino front, the Austro-Hungarians took advantage of the mountainous terrain, which favoured the defender.
After an initial strategic retreat, the front remained largely unchanged, while Austrian Kaiserschützen and Standschützen engaged Italian Alpini in bitter hand-to-hand combat throughout the summer.
The Austro-Hungarians counterattacked in the Altopiano of Asiago, towards Verona and Padua, in the spring of 1916 (Strafexpedition), but made little progress and were defeated by the Italians.
Beginning in 1915, the Italians under Cadorna mounted eleven offensives on the Isonzo front along the Isonzo (Soča) River, northeast of Trieste.
Of this eleven offensives, five were won by Italy, three remained inconclusive, and other three were repelled by the Austro-Hungarians, who held the higher ground.
In the summer of 1916, after the Battle of Doberdò, the Italians captured the town of Gorizia.
After this victory, the front remained static for over a year, despite several Italian offensives, centred on the Banjšice and Karst Plateau east of Gorizia.
The Central Powers launched a crushing offensive on 26 October 1917, spearheaded by the Germans, and achieved a victory at Caporetto (Kobarid).
The Italian Army was routed and retreated more than 100 kilometres (62 mi) to reorganise.
The new Italian chief of staff, Armando Diaz, ordered the Army to stop their retreat and defend the Monte Grappa summit, where fortified defenses were constructed; the Italians repelled the Austro-Hungarian and German Army, and stabilised the front at the Piave River.
Since the Italian Army had suffered heavy losses in the Battle of Caporetto, the Italian Government ordered conscription of the so-called '99 Boys (Ragazzi del '99): all males born in 1899 and prior, who were 18 years old or older.
In 1918, the Austro-Hungarians failed to break through in a series of battles on the Piave and were finally decisively defeated in the Battle of Vittorio Veneto in October.
On 1 November, the Italian Navy destroyed much of the Austro-Hungarian fleet stationed in Pula, preventing it from being handed over to the new State of Slovenes, Croats and Serbs.
On 3 November, the Italians invaded Trieste from the sea.
On the same day, the Armistice of Villa Giusti was signed.
By mid-November 1918, the Italian military occupied the entire former Austrian Littoral and had seized control of the portion of Dalmatia that had been guaranteed to Italy by the London Pact.
[145] By the end of hostilities in November 1918,[146] Admiral Enrico Millo declared himself Italy's Governor of Dalmatia.
[146] Austria-Hungary surrendered on 11 November 1918.
Romania had been allied with the Central Powers since 1882.
When the war began, however, it declared its neutrality, arguing that because Austria-Hungary had itself declared war on Serbia, Romania was under no obligation to join the war.
On 4 August 1916, Romania and the Entente signed the Political Treaty and Military Convention, that established the coordinates of Romania's participation in the war.
In return, it received the Allies' formal sanction for Transylvania, Banat and other territories of Austria-Hungary to be annexed to Romania.
The action had large popular support.
[149] On 27 August 1916, the Romanian Army launched an attack against Austria-Hungary, with limited Russian support.
The Romanian offensive was initially successful in Transylvania, but a Central Powers counterattack by the drove them back.
[150] As a result of the Battle of Bucharest, the Central Powers occupied Bucharest on 6 December 1916.
Fighting in Moldova continued in 1917, but Russian withdrawal from the war in late 1917 as a result of the October Revolution meant that Romania was forced to sign an armistice with the Central Powers on 9 December 1917.
In January 1918, Romanian forces established control over Bessarabia as the Russian Army abandoned the province.
Although a treaty was signed by the Romanian and Bolshevik Russian governments following talks between 5 and 9 March 1918 on the withdrawal of Romanian forces from Bessarabia within two months, on 27 March 1918 Romania formally attached Bessarabia, inhabited by a Romanian majority, to its territory, based on a resolution passed by the local assembly of that territory on its unification with Romania.
Romania officially made peace with the Central Powers by signing the Treaty of Bucharest on 7 May 1918.
Under the treaty, Romania was obliged to end the war with the Central Powers and make small territorial concessions to Austria-Hungary, ceding control of some passes in the Carpathian Mountains, and to grant oil concessions to Germany.
In exchange, the Central Powers recognised the sovereignty of Romania over Bessarabia.
The treaty was renounced in October 1918 by the Alexandru Marghiloman government, and Romania nominally re-entered the war on 10 November 1918 against the Central Powers.
The next day, the Treaty of Bucharest was nullified by the terms of the Armistice of Compiègne.
[153][154] Total Romanian deaths from 1914 to 1918, military and civilian, within contemporary borders, were estimated at 748,000.
Russian plans for the start of the war called for simultaneous invasions of Austrian Galicia and East Prussia.
Although Russia's initial advance into Galicia was largely successful, it was driven back from East Prussia by Hindenburg and Ludendorff at the battles of Tannenberg and the Masurian Lakes in August and September 1914.
[156][157] Russia's less developed industrial base and ineffective military leadership were instrumental in the events that unfolded.
By the spring of 1915, the Russians had retreated to Galicia, and, in May, the Central Powers achieved a remarkable breakthrough on Poland's southern frontiers with their Gorlice–Tarnów Offensive.
[158] On 5 August, they captured Warsaw and forced the Russians to withdraw from Poland.
Despite Russia's success in the June 1916 Brusilov Offensive against the Austrians in eastern Galicia,[159] the offensive was undermined by the reluctance of other Russian generals to commit their forces to support the victory.
Allied and Russian forces were revived only briefly by Romania's entry into the war on 27 August, as Romania was rapidly defeated by a Central Powers offensive.
Meanwhile, unrest grew in Russia as the Tsar remained at the front.
The increasingly incompetent rule of Empress Alexandra drew protests and resulted in the murder of her favourite, Rasputin, at the end of 1916.
In March 1917, demonstrations in Petrograd culminated in the abdication of Tsar Nicholas II and the appointment of a weak Provisional Government, which shared power with the Petrograd Soviet socialists.
This arrangement led to confusion and chaos both at the front and at home.
The army became increasingly ineffective.
Following the Tsar's abdication, Vladimir Lenin—with the help of the German government—was ushered by train from Switzerland into Russia 16 April 1917.
[161] Discontent and the weaknesses of the Provisional Government led to a rise in the popularity of the Bolshevik Party, led by Lenin, which demanded an immediate end to the war.
The Revolution of November was followed in December by an armistice and negotiations with Germany.
At first, the Bolsheviks refused the German terms, but when German troops began marching across Ukraine unopposed, the new government acceded to the Treaty of Brest-Litovsk on 3 March 1918.
The treaty ceded vast territories, including Finland, the Baltic provinces, parts of Poland and Ukraine to the Central Powers.
[162][citation not found] Despite this enormous German success, the manpower required by the Germans to occupy the captured territory may have contributed to the failure of their Spring Offensive, and secured relatively little food or other materiel for the Central Powers war effort.
With the adoption of the Treaty of Brest-Litovsk, the Entente no longer existed.
The Allied powers led a small-scale invasion of Russia, partly to stop Germany from exploiting Russian resources, and to a lesser extent, to support the "Whites" (as opposed to the "Reds") in the Russian Civil War.
[163] Allied troops landed in Arkhangelsk and in Vladivostok as part of the North Russia Intervention.
The Czechoslovak Legion fought on the side of the Entente.
Its goal was to win support for the independence of Czechoslovakia.
The Legion in Russia was established in September 1914, in December 1917 in France (including volunteers from America) and in April 1918 in Italy.
Czechoslovak Legion troops defeated the Austro-Hungarian army at the Ukrainian village of Zborov, in July 1917.
After this success, the number of Czechoslovak legionaries increased, as well as Czechoslovak military power.
In the Battle of Bakhmach, the Legion defeated the Germans and forced them to make a truce.
In Russia, they were heavily involved in the Russian Civil War, siding with the Whites against the Bolsheviks, at times controlling most of the Trans-Siberian railway and conquering all the major cities of Siberia.
The presence of the Czechoslovak Legion near Yekaterinburg appears to have been one of the motivations for the Bolshevik execution of the Tsar and his family in July 1918.
Legionaries arrived less than a week afterwards and captured the city.
Because Russia's European ports were not safe, the corps was evacuated by a long detour via the port of Vladivostok.
The last transport was the American ship Heffron in September 1920.
On 12 December 1916, after ten brutal months of the Battle of Verdun and a successful offensive against Romania, Germany attempted to negotiate a peace with the Allies.
[164] However, this attempt was rejected out of hand as a "duplicitous war ruse".
Soon after, the US President, Woodrow Wilson, attempted to intervene as a peacemaker, asking in a note for both sides to state their demands.
Lloyd George's War Cabinet considered the German offer to be a ploy to create divisions amongst the Allies.
After initial outrage and much deliberation, they took Wilson's note as a separate effort, signalling that the United States was on the verge of entering the war against Germany following the "submarine outrages".
While the Allies debated a response to Wilson's offer, the Germans chose to rebuff it in favour of "a direct exchange of views".
Learning of the German response, the Allied governments were free to make clear demands in their response of 14 January.
They sought restoration of damages, the evacuation of occupied territories, reparations for France, Russia and Romania, and a recognition of the principle of nationalities.
[165] This included the liberation of Italians, Slavs, Romanians, Czecho-Slovaks, and the creation of a "free and united Poland".
[165] On the question of security, the Allies sought guarantees that would prevent or limit future wars, complete with sanctions, as a condition of any peace settlement.
[166] The negotiations failed and the Entente powers rejected the German offer on the grounds that Germany had not put forward any specific proposals.
Events of 1917 proved decisive in ending the war, although their effects were not fully felt until 1918.
The British naval blockade began to have a serious impact on Germany.
In response, in February 1917, the German General Staff convinced Chancellor Theobald von Bethmann-Hollweg to declare unrestricted submarine warfare, with the goal of starving Britain out of the war.
German planners estimated that unrestricted submarine warfare would cost Britain a monthly shipping loss of 600,000 tons.
The General Staff acknowledged that the policy would almost certainly bring the United States into the conflict, but calculated that British shipping losses would be so high that they would be forced to sue for peace after five to six months, before American intervention could have an effect.
Tonnage sunk rose above 500,000 tons per month from February to July.
It peaked at 860,000 tons in April.
After July, the newly re-introduced convoy system became effective in reducing the U-boat threat.
Britain was safe from starvation, while German industrial output fell, and the United States joined the war far earlier than Germany had anticipated.
On 3 May 1917, during the Nivelle Offensive, the French 2nd Colonial Division, veterans of the Battle of Verdun, refused orders, arriving drunk and without their weapons.
Their officers lacked the means to punish an entire division, and harsh measures were not immediately implemented.
The French Army Mutinies eventually spread to a further 54 French divisions, and 20,000 men deserted.
However, appeals to patriotism and duty, as well as mass arrests and trials, encouraged the soldiers to return to defend their trenches, although the French soldiers refused to participate in further offensive action.
[167] Robert Nivelle was removed from command by 15 May, replaced by General Philippe Pétain, who suspended bloody large-scale attacks.
The victory of the Central Powers at the Battle of Caporetto led the Allies to convene the Rapallo Conference at which they formed the Supreme War Council to co-ordinate planning.
Previously, British and French armies had operated under separate commands.
In December, the Central Powers signed an armistice with Russia, thus freeing large numbers of German troops for use in the west.
With German reinforcements and new American troops pouring in, the outcome was to be decided on the Western Front.
The Central Powers knew that they could not win a protracted war, but they held high hopes for success based on a final quick offensive.
Furthermore, both sides became increasingly fearful of social unrest and revolution in Europe.
Thus, both sides urgently sought a decisive victory.
In 1917, Emperor Charles I of Austria secretly attempted separate peace negotiations with Clemenceau, through his wife's brother Sixtus in Belgium as an intermediary, without the knowledge of Germany.
Italy opposed the proposals.
When the negotiations failed, his attempt was revealed to Germany, resulting in a diplomatic catastrophe.
In March and April 1917, at the First and Second Battles of Gaza, German and Ottoman forces stopped the advance of the Egyptian Expeditionary Force, which had begun in August 1916 at the Battle of Romani.
At the end of October, the Sinai and Palestine Campaign resumed, when General Edmund Allenby's XXth Corps, XXI Corps and Desert Mounted Corps won the Battle of Beersheba.
[173] Two Ottoman armies were defeated a few weeks later at the Battle of Mughar Ridge and, early in December, Jerusalem was captured following another Ottoman defeat at the Battle of Jerusalem.
[174][175][176] About this time, Friedrich Freiherr Kress von Kressenstein was relieved of his duties as the Eighth Army's commander, replaced by Djevad Pasha, and a few months later the commander of the Ottoman Army in Palestine, Erich von Falkenhayn, was replaced by Otto Liman von Sanders.
In early 1918, the front line was extended and the Jordan Valley was occupied, following the First Transjordan and the Second Transjordan attacks by British Empire forces in March and April 1918.
[179] In March, most of the Egyptian Expeditionary Force's British infantry and Yeomanry cavalry were sent to the Western Front as a consequence of the Spring Offensive.
They were replaced by Indian Army units.
During several months of reorganisation and training of the summer, a number of attacks were carried out on sections of the Ottoman front line.
These pushed the front line north to more advantageous positions for the Entente in preparation for an attack and to acclimatise the newly arrived Indian Army infantry.
It was not until the middle of September that the integrated force was ready for large-scale operations.
The reorganised Egyptian Expeditionary Force, with an additional mounted division, broke Ottoman forces at the Battle of Megiddo in September 1918.
In two days the British and Indian infantry, supported by a creeping barrage, broke the Ottoman front line and captured the headquarters of the Eighth Army (Ottoman Empire) at Tulkarm, the continuous trench lines at Tabsor, Arara, and the Seventh Army (Ottoman Empire) headquarters at Nablus.
The Desert Mounted Corps rode through the break in the front line created by the infantry.
During virtually continuous operations by Australian Light Horse, British mounted Yeomanry, Indian Lancers, and New Zealand Mounted Rifle brigades in the Jezreel Valley, they captured Nazareth, Afulah and Beisan, Jenin, along with Haifa on the Mediterranean coast and Daraa east of the Jordan River on the Hejaz railway.
Samakh and Tiberias on the Sea of Galilee were captured on the way northwards to Damascus.
Meanwhile, Chaytor's Force of Australian light horse, New Zealand mounted rifles, Indian, British West Indies and Jewish infantry captured the crossings of the Jordan River, Es Salt, Amman and at Ziza most of the Fourth Army (Ottoman Empire).
The Armistice of Mudros, signed at the end of October, ended hostilities with the Ottoman Empire when fighting was continuing north of Aleppo.
On or shortly before 15 August 1917 Pope Benedict XV made a peace proposal[180] suggesting:
At the outbreak of the war, the United States pursued a policy of non-intervention, avoiding conflict while trying to broker a peace.
When the German U-boat U-20 sank the British liner RMS Lusitania on 7 May 1915 with 128 Americans among the dead, President Woodrow Wilson insisted that America is "too proud to fight" but demanded an end to attacks on passenger ships.
Germany complied.
Wilson unsuccessfully tried to mediate a settlement.
However, he also repeatedly warned that the United States would not tolerate unrestricted submarine warfare, in violation of international law.
Former president Theodore Roosevelt denounced German acts as "piracy".
[181] Wilson was narrowly re-elected in 1916 after campaigning with the slogan "he kept us out of war".
In January 1917, Germany decided to resume unrestricted submarine warfare, realising it would mean American entry.
The German Foreign Minister, in the Zimmermann Telegram, invited Mexico to join the war as Germany's ally against the United States.
In return, the Germans would finance Mexico's war and help it recover the territories of Texas, New Mexico, and Arizona.
[185] The United Kingdom intercepted the message and presented it to the US embassy in the UK.
From there it made its way to President Wilson who released the Zimmermann note to the public, and Americans saw it as casus belli.
Wilson called on anti-war elements to end all wars, by winning this one and eliminating militarism from the globe.
He argued that the war was so important that the US had to have a voice in the peace conference.
[186] After the sinking of seven US merchant ships by submarines and the publication of the Zimmermann telegram, Wilson called for war on Germany on 2 April 1917,[187] which the US Congress declared 4 days later.
The United States was never formally a member of the Allies but became a self-styled "Associated Power".
The United States had a small army, but, after the passage of the Selective Service Act, it drafted 2.8 million men,[188] and, by summer 1918, was sending 10,000 fresh soldiers to France every day.
In 1917, the US Congress granted US citizenship to Puerto Ricans to allow them to be drafted to participate in World War I, as part of the Jones–Shafroth Act.
German General Staff assumptions that it would be able to defeat the British and French forces before American troops reinforced them were proven incorrect.
The United States Navy sent a battleship group to Scapa Flow to join with the British Grand Fleet, destroyers to Queenstown, Ireland, and submarines to help guard convoys.
Several regiments of US Marines were also dispatched to France.
The British and French wanted American units used to reinforce their troops already on the battle lines and not waste scarce shipping on bringing over supplies.
General John J. Pershing, American Expeditionary Forces (AEF) commander, refused to break up American units to be used as filler material.
As an exception, he did allow African-American combat regiments to be used in French divisions.
The Harlem Hellfighters fought as part of the French 16th Division, and earned a unit Croix de Guerre for their actions at Château-Thierry, Belleau Wood, and Sechault.
[190] AEF doctrine called for the use of frontal assaults, which had long since been discarded by British Empire and French commanders due to the large loss of life that resulted.
Ludendorff drew up plans (codenamed Operation Michael) for the 1918 offensive on the Western Front.
The Spring Offensive sought to divide the British and French forces with a series of feints and advances.
The German leadership hoped to end the war before significant US forces arrived.
The operation commenced on 21 March 1918 with an attack on British forces near Saint-Quentin.
German forces achieved an unprecedented advance of 60 kilometres (37 mi).
British and French trenches were penetrated using novel infiltration tactics, also named Hutier tactics after General Oskar von Hutier, by specially trained units called stormtroopers.
Previously, attacks had been characterised by long artillery bombardments and massed assaults.
In the Spring Offensive of 1918, however, Ludendorff used artillery only briefly and infiltrated small groups of infantry at weak points.
They attacked command and logistics areas and bypassed points of serious resistance.
More heavily armed infantry then destroyed these isolated positions.
This German success relied greatly on the element of surprise.
[193][citation not found]
The front moved to within 120 kilometres (75 mi) of Paris.
Three heavy Krupp railway guns fired 183 shells on the capital, causing many Parisians to flee.
The initial offensive was so successful that Kaiser Wilhelm II declared 24 March a national holiday.
Many Germans thought victory was near.
After heavy fighting, however, the offensive was halted.
Lacking tanks or motorised artillery, the Germans were unable to consolidate their gains.
The problems of re-supply were also exacerbated by increasing distances that now stretched over terrain that was shell-torn and often impassable to traffic.
General Foch pressed to use the arriving American troops as individual replacements, whereas Pershing sought to field American units as an independent force.
These units were assigned to the depleted French and British Empire commands on 28 March.
A Supreme War Council of Allied forces was created at the Doullens Conference on 5 November 1917.
General Foch was appointed as supreme commander of the Allied forces.
Haig, Petain, and Pershing retained tactical control of their respective armies; Foch assumed a co-ordinating rather than a directing role, and the British, French, and US commands operated largely independently.
Following Operation Michael, Germany launched Operation Georgette against the northern English Channel ports.
The Allies halted the drive after limited territorial gains by Germany.
The German Army to the south then conducted Operations Blücher and Yorck, pushing broadly towards Paris.
Germany launched Operation Marne (Second Battle of the Marne) on 15 July, in an attempt to encircle Reims.
The resulting counter-attack, which started the Hundred Days Offensive, marked the first successful Allied offensive of the war.
By 20 July, the Germans had retreated across the Marne to their starting lines,[196] having achieved little, and the German Army never regained the initiative.
German casualties between March and April 1918 were 270,000, including many highly trained stormtroopers.
Meanwhile, Germany was falling apart at home.
Anti-war marches became frequent and morale in the army fell.
Industrial output was half the 1913 levels.
In the late spring of 1918, three new states were formed in the South Caucasus: the First Republic of Armenia, the Azerbaijan Democratic Republic, and the Democratic Republic of Georgia, which declared their independence from the Russian Empire.
Two other minor entities were established, the Centrocaspian Dictatorship and South West Caucasian Republic (the former was liquidated by Azerbaijan in the autumn of 1918 and the latter by a joint Armenian-British task force in early 1919).
With the withdrawal of the Russian armies from the Caucasus front in the winter of 1917–18, the three major republics braced for an imminent Ottoman advance, which commenced in the early months of 1918.
Solidarity was briefly maintained when the Transcaucasian Federative Republic was created in the spring of 1918, but this collapsed in May, when the Georgians asked for and received protection from Germany and the Azerbaijanis concluded a treaty with the Ottoman Empire that was more akin to a military alliance.
Armenia was left to fend for itself and struggled for five months against the threat of a full-fledged occupation by the Ottoman Turks before defeating them at the Battle of Sardarabad.
The Allied counteroffensive, known as the Hundred Days Offensive, began on 8 August 1918, with the Battle of Amiens.
The battle involved over 400 tanks and 120,000 British, Dominion, and French troops, and by the end of its first day a gap 24 kilometres (15 mi) long had been created in the German lines.
The defenders displayed a marked collapse in morale, causing Ludendorff to refer to this day as the "Black Day of the German army".
[199][200][201] After an advance as far as 23 kilometres (14 mi), German resistance stiffened, and the battle was concluded on 12 August.
Rather than continuing the Amiens battle past the point of initial success, as had been done so many times in the past, the Allies shifted attention elsewhere.
Allied leaders had now realised that to continue an attack after resistance had hardened was a waste of lives, and it was better to turn a line than to try to roll over it.
They began to undertake attacks in quick order to take advantage of successful advances on the flanks, then broke them off when each attack lost its initial impetus.
The day after the Offensive began, Ludendorff said: "We cannot win the war any more, but we must not lose it either."
On 11 August he offered his resignation to the Kaiser, who refused it, replying, "I see that we must strike a balance.
We have nearly reached the limit of our powers of resistance.
The war must be ended."
On 13 August, at Spa, Hindenburg, Ludendorff, the Chancellor, and Foreign Minister Hintz agreed that the war could not be ended militarily and, on the following day, the German Crown Council decided that victory in the field was now most improbable.
Austria and Hungary warned that they could only continue the war until December, and Ludendorff recommended immediate peace negotiations.
Prince Rupprecht warned Prince Max of Baden: "Our military situation has deteriorated so rapidly that I no longer believe we can hold out over the winter; it is even possible that a catastrophe will come earlier.
British and Dominion forces launched the next phase of the campaign with the Battle of Albert on 21 August.
[204] The assault was widened by French[203] and then further British forces in the following days.
During the last week of August the Allied pressure along a 110-kilometre (68 mi) front against the enemy was heavy and unrelenting.
From German accounts, "Each day was spent in bloody fighting against an ever and again on-storming enemy, and nights passed without sleep in retirements to new lines.
Faced with these advances, on 2 September the German Supreme Army Command issued orders to withdraw in the south to the Hindenburg Line.
This ceded without a fight the salient seized the previous April.
[205] According to Ludendorff, "We had to admit the necessity ... to withdraw the entire front from the Scarpe to the Vesle.
[206] In nearly four weeks of fighting beginning on 8 August, over 100,000 German prisoners were taken.
The German High Command realised that the war was lost and made attempts to reach a satisfactory end.
On 10 September Hindenburg urged peace moves to Emperor Charles of Austria, and Germany appealed to the Netherlands for mediation.
On 14 September Austria sent a note to all belligerents and neutrals suggesting a meeting for peace talks on neutral soil, and on 15 September Germany made a peace offer to Belgium.
Both peace offers were rejected.
In September the Allies advanced to the Hindenburg Line in the north and centre.
The Germans continued to fight strong rear-guard actions and launched numerous counterattacks, but positions and outposts of the Line continued to fall, with the BEF alone taking 30,441 prisoners in the last week of September.
On 24 September an assault by both the British and French came within 3 kilometres (2 mi) of St. Quentin.
The Germans had now retreated to positions along or behind the Hindenburg Line.
That same day, Supreme Army Command informed the leaders in Berlin that armistice talks were inevitable.
The final assault on the Hindenburg Line began with the Meuse-Argonne Offensive, launched by French and American troops on 26 September.
The following week, co-operating French and American units broke through in Champagne at the Battle of Blanc Mont Ridge, forcing the Germans off the commanding heights, and closing towards the Belgian frontier.
[207] On 8 October the line was pierced again by British and Dominion troops at the Battle of Cambrai.
[208] The German army had to shorten its front and use the Dutch frontier as an anchor to fight rear-guard actions as it fell back towards Germany.
When Bulgaria signed a separate armistice on 29 September, Ludendorff, having been under great stress for months, suffered something similar to a breakdown.
It was evident that Germany could no longer mount a successful defence.
The collapse of the Balkans meant that Germany was about to lose its main supplies of oil and food.
Its reserves had been used up, even as US troops kept arriving at the rate of 10,000 per day.
[209][210][211] The Americans supplied more than 80% of Allied oil during the war, and there was no shortage.
News of Germany's impending military defeat spread throughout the German armed forces.
The threat of mutiny was rife.
Admiral Reinhard Scheer and Ludendorff decided to launch a last attempt to restore the "valour" of the German Navy.
In northern Germany, the German Revolution of 1918–1919 began at the end of October 1918.
Units of the German Navy refused to set sail for a last, large-scale operation in a war they believed to be as good as lost, initiating the uprising.
The sailors' revolt, which then ensued in the naval ports of Wilhelmshaven and Kiel, spread across the whole country within days and led to the proclamation of a republic on 9 November 1918, shortly thereafter to the abdication of Kaiser Wilhelm II, and to German surrender.
With the military faltering and with widespread loss of confidence in the Kaiser leading to his abdication and fleeing of the country, Germany moved towards surrender.
Prince Maximilian of Baden took charge of a new government on 3 October as Chancellor of Germany to negotiate with the Allies.
Negotiations with President Wilson began immediately, in the hope that he would offer better terms than the British and French.
Wilson demanded a constitutional monarchy and parliamentary control over the German military.
[216] There was no resistance when the Social Democrat Philipp Scheidemann on 9 November declared Germany to be a republic.
The Kaiser, kings and other hereditary rulers all were removed from power and Wilhelm fled to exile in the Netherlands.
Imperial Germany was dead; a new Germany had been born as the Weimar Republic.
The collapse of the Central Powers came swiftly.
Bulgaria was the first to sign an armistice, the Armistice of Salonica on 29 September 1918.
[24] On 30 October, the Ottoman Empire capitulated, signing the Armistice of Mudros.
On 24 October, the Italians began a push that rapidly recovered territory lost after the Battle of Caporetto.
This culminated in the Battle of Vittorio Veneto, which marked the end of the Austro-Hungarian Army as an effective fighting force.
The offensive also triggered the disintegration of the Austro-Hungarian Empire.
During the last week of October, declarations of independence were made in Budapest, Prague, and Zagreb.
On 29 October, the imperial authorities asked Italy for an armistice, but the Italians continued advancing, reaching Trento, Udine, and Trieste.
On 3 November, Austria-Hungary sent a flag of truce to ask for an armistice (Armistice of Villa Giusti).
The terms, arranged by telegraph with the Allied Authorities in Paris, were communicated to the Austrian commander and accepted.
The Armistice with Austria was signed in the Villa Giusti, near Padua, on 3 November.
Austria and Hungary signed separate armistices following the overthrow of the Habsburg Monarchy.
In the following days the Italian Army occupied Innsbruck and all Tyrol with 20 to 22,000 soldiers.
On 11 November, at 5:00 am, an armistice with Germany was signed in a railroad carriage at Compiègne.
At 11 am on 11 November 1918—"the eleventh hour of the eleventh day of the eleventh month"—a ceasefire came into effect.
During the six hours between the signing of the armistice and its taking effect, opposing armies on the Western Front began to withdraw from their positions, but fighting continued along many areas of the front, as commanders wanted to capture territory before the war ended.
The occupation of the Rhineland took place following the Armistice.
The occupying armies consisted of American, Belgian, British and French forces.
In November 1918, the Allies had ample supplies of men and materiel to invade Germany.
Yet at the time of the armistice, no Allied force had crossed the German frontier, the Western Front was still some 720 kilometres (450 mi) from Berlin, and the Kaiser's armies had retreated from the battlefield in good order.
These factors enabled Hindenburg and other senior German leaders to spread the story that their armies had not really been defeated.
This resulted in the stab-in-the-back legend,[220][221] which attributed Germany's defeat not to its inability to continue fighting (even though up to a million soldiers were suffering from the 1918 flu pandemic and unfit to fight), but to the public's failure to respond to its "patriotic calling" and the supposed intentional sabotage of the war effort, particularly by Jews, Socialists, and Bolsheviks.
The Allies had much more potential wealth they could spend on the war.
One estimate (using 1913 US dollars) is that the Allies spent $58 billion on the war and the Central Powers only $25 billion.
Among the Allies, the UK spent $21 billion and the US$17 billion; among the Central Powers Germany spent $20 billion.
In the aftermath of the war, four empires disappeared: the German, Austro-Hungarian, Ottoman, and Russian.
Numerous nations regained their former independence, and new ones were created.
Four dynasties, together with their ancillary aristocracies, fell as a result of the war: the Romanovs, the Hohenzollerns, the Habsburgs, and the Ottomans.
Belgium and Serbia were badly damaged, as was France, with 1.4 million soldiers dead,[223] not counting other casualties.
Germany and Russia were similarly affected.
A formal state of war between the two sides persisted for another seven months, until the signing of the Treaty of Versailles with Germany on 28 June 1919.
The United States Senate did not ratify the treaty despite public support for it,[225][226] and did not formally end its involvement in the war until the Knox–Porter Resolution was signed on 2 July 1921 by President Warren G.
[227] For the United Kingdom and the British Empire, the state of war ceased under the provisions of the Termination of the Present War (Definition) Act 1918 with respect to:
After the Treaty of Versailles, treaties with Austria, Hungary, Bulgaria, and the Ottoman Empire were signed.
However, the negotiation of the treaty with the Ottoman Empire was followed by strife, and a final peace treaty between the Allied Powers and the country that would shortly become the Republic of Turkey was not signed until 24 July 1923, at Lausanne.
Some war memorials date the end of the war as being when the Versailles Treaty was signed in 1919, which was when many of the troops serving abroad finally returned home; by contrast, most commemorations of the war's end concentrate on the armistice of 11 November 1918.
Legally, the formal peace treaties were not complete until the last, the Treaty of Lausanne, was signed.
Under its terms, the Allied forces left Constantinople on 23 August 1923.
After the war, the Paris Peace Conference imposed a series of peace treaties on the Central Powers officially ending the war.
The 1919 Treaty of Versailles dealt with Germany and, building on Wilson's 14th point, brought into being the League of Nations on 28 June 1919.
The Central Powers had to acknowledge responsibility for "all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by" their aggression.
In the Treaty of Versailles, this statement was Article 231.
This article became known as the War Guilt clause as the majority of Germans felt humiliated and resentful.
[235] Overall the Germans felt they had been unjustly dealt with by what they called the "diktat of Versailles".
German historian Hagen Schulze said the Treaty placed Germany "under legal sanctions, deprived of military power, economically ruined, and politically humiliated.
"[236] Belgian historian Laurence Van Ypersele emphasises the central role played by memory of the war and the Versailles Treaty in German politics in the 1920s and 1930s:
Active denial of war guilt in Germany and German resentment at both reparations and continued Allied occupation of the Rhineland made widespread revision of the meaning and memory of the war problematic.
The legend of the "stab in the back" and the wish to revise the "Versailles diktat", and the belief in an international threat aimed at the elimination of the German nation persisted at the heart of German politics.
Even a man of peace such as [Gustav] Stresemann publicly rejected German guilt.
As for the Nazis, they waved the banners of domestic treason and international conspiracy in an attempt to galvanise the German nation into a spirit of revenge.
Like a Fascist Italy, Nazi Germany sought to redirect the memory of the war to the benefit of its own policies.
Meanwhile, new nations liberated from German rule viewed the treaty as recognition of wrongs committed against small nations by much larger aggressive neighbours.
[238] The Peace Conference required all the defeated powers to pay reparations for all the damage done to civilians.
However, owing to economic difficulties and Germany being the only defeated power with an intact economy, the burden fell largely on Germany.
Austria-Hungary was partitioned into several successor states, including Austria, Hungary, Czechoslovakia, and Yugoslavia, largely but not entirely along ethnic lines.
Transylvania was shifted from Hungary to Greater Romania.
The details were contained in the Treaty of Saint-Germain and the Treaty of Trianon.
As a result of the Treaty of Trianon, 3.3 million Hungarians came under foreign rule.
Although the Hungarians made up approximately 54% of the population of the pre-war Kingdom of Hungary (according to the 1910 census), only 32% of its territory was left to Hungary.
Between 1920 and 1924, 354,000 Hungarians fled former Hungarian territories attached to Romania, Czechoslovakia, and Yugoslavia.
The Russian Empire, which had withdrawn from the war in 1917 after the October Revolution, lost much of its western frontier as the newly independent nations of Estonia, Finland, Latvia, Lithuania, and Poland were carved from it.
Romania took control of Bessarabia in April 1918.
The Ottoman Empire disintegrated, with much of its Levant territory awarded to various Allied powers as protectorates.
The Turkish core in Anatolia was reorganised as the Republic of Turkey.
The Ottoman Empire was to be partitioned by the Treaty of Sèvres of 1920.
This treaty was never ratified by the Sultan and was rejected by the Turkish National Movement, leading to the victorious Turkish War of Independence and the much less stringent 1923 Treaty of Lausanne.
Even though a lot of countries had already made a peace treaty, there was one exception, Andorra.
Andorra declared war on Germany in August 1914, but, because it had a very small population, Andorra had never sent any soldiers to the battlefield.
Because of that, Andorra wasn't allowed to go to the Treaty of Versailles, so the country hadn't made a peace treaty with Germany until 1958.
When Andorra made the declaration of war, it had an army of 600 part-time militarymen, commanded by two officials.
After 123 years, Poland re-emerged as an independent country.
The Kingdom of Serbia and its dynasty, as a "minor Entente nation" and the country with the most casualties per capita,[245][246][247] became the backbone of a new multinational state, the Kingdom of Serbs, Croats and Slovenes, later renamed Yugoslavia.
Czechoslovakia, combining the Kingdom of Bohemia with parts of the Kingdom of Hungary, became a new nation.
Russia became the Soviet Union and lost Finland, Estonia, Lithuania, and Latvia, which became independent countries.
The Ottoman Empire was soon replaced by Turkey and several other countries in the Middle East.
In the British Empire, the war unleashed new forms of nationalism.
In Australia and New Zealand the Battle of Gallipoli became known as those nations' "Baptism of Fire".
It was the first major war in which the newly established countries fought, and it was one of the first times that Australian troops fought as Australians, not just subjects of the British Crown.
Anzac Day, commemorating the Australian and New Zealand Army Corps, celebrates this defining moment.
After the Battle of Vimy Ridge, where the Canadian divisions fought together for the first time as a single corps, Canadians began to refer to their country as a nation "forged from fire".
[250] Having succeeded on the same battleground where the "mother countries" had previously faltered, they were for the first time respected internationally for their own accomplishments.
Canada entered the war as a Dominion of the British Empire and remained so, although it emerged with a greater measure of independence.
[251][252] When Britain declared war in 1914, the dominions were automatically at war; at the conclusion, Canada, Australia, New Zealand, and South Africa were individual signatories of the Treaty of Versailles.
Lobbying by Chaim Weizmann and fear that American Jews would encourage the United States to support Germany culminated in the British government's Balfour Declaration of 1917, endorsing creation of a Jewish homeland in Palestine.
[254] A total of more than 1,172,000 Jewish soldiers served in the Allied and Central Power forces in World War I, including 275,000 in Austria-Hungary and 450,000 in Tsarist Russia.
The establishment of the modern state of Israel and the roots of the continuing Israeli–Palestinian conflict are partially found in the unstable power dynamics of the Middle East that resulted from World War I.
[256] Before the end of the war, the Ottoman Empire had maintained a modest level of peace and stability throughout the Middle East.
[257] With the fall of the Ottoman government, power vacuums developed and conflicting claims to land and nationhood began to emerge.
[258] The political boundaries drawn by the victors of World War I were quickly imposed, sometimes after only cursory consultation with the local population.
These continue to be problematic in the 21st-century struggles for national identity.
[259][260] While the dissolution of the Ottoman Empire at the end of World War I was pivotal in contributing to the modern political situation of the Middle East, including the Arab-Israeli conflict,[261][262][263] the end of Ottoman rule also spawned lesser known disputes over water and other natural resources.
The prestige of Germany and German things in Latin America remained high after the war but did not recovered to its pre-war levels.
[265][266] Indeed, in Chile the war bought an end to a period of intense scientific and cultural influence writer Eduardo de la Barra scorningly called "the German bewichment" (Spanish: el embrujamiento alemán).
Of the 60 million European military personnel who were mobilised from 1914 to 1918, 8 million were killed, 7 million were permanently disabled, and 15 million were seriously injured.
Germany lost 15.1% of its active male population, Austria-Hungary lost 17.1%, and France lost 10.5%.
[267] In Germany, civilian deaths were 474,000 higher than in peacetime, due in large part to food shortages and malnutrition that weakened resistance to disease.
[268] By the end of the war, starvation caused by famine had killed approximately 100,000 people in Lebanon.
[269] Between 5 and 10 million people died in the Russian famine of 1921.
[270] By 1922, there were between 4.5 million and 7 million homeless children in Russia as a result of nearly a decade of devastation from World War I, the Russian Civil War, and the subsequent famine of 1920–1922.
[271] Numerous anti-Soviet Russians fled the country after the Revolution; by the 1930s, the northern Chinese city of Harbin had 100,000 Russians.
[272] Thousands more emigrated to France, England, and the United States.
The Australian prime minister, Billy Hughes, wrote to the British prime minister, Lloyd George, "You have assured us that you cannot get better terms.
I much regret it, and hope even now that some way may be found of securing agreement for demanding reparation commensurate with the tremendous sacrifices made by the British Empire and her Allies."
Australia received £5,571,720 war reparations, but the direct cost of the war to Australia had been £376,993,052, and, by the mid-1930s, repatriation pensions, war gratuities, interest and sinking fund charges were £831,280,947.
[273] Of about 416,000 Australians who served, about 60,000 were killed and another 152,000 were wounded.
Diseases flourished in the chaotic wartime conditions.
In 1914 alone, louse-borne epidemic typhus killed 200,000 in Serbia.
[275] From 1918 to 1922, Russia had about 25 million infections and 3 million deaths from epidemic typhus.
[276] In 1923, 13 million Russians contracted malaria, a sharp increase from the pre-war years.
[277] In addition, a major influenza epidemic spread around the world.
Overall, the 1918 flu pandemic killed at least 50 million people.
[278][279] Moreover, between 1915 and 1926, an epidemic of encephalitis lethargica spread around the world affecting nearly five million people.
The social disruption and widespread violence of the Russian Revolution of 1917 and the ensuing Russian Civil War sparked more than 2,000 pogroms in the former Russian Empire, mostly in Ukraine.
[282] An estimated 60,000–200,000 civilian Jews were killed in the atrocities.
In the aftermath of World War I, Greece fought against Turkish nationalists led by Mustafa Kemal, a war that eventually resulted in a massive population exchange between the two countries under the Treaty of Lausanne.
[284] According to various sources,[285] several hundred thousand Greeks died during this period, which was tied in with the Greek Genocide.
World War I began as a clash of 20th-century technology and 19th-century tactics, with the inevitably large ensuing casualties.
By the end of 1917, however, the major armies, now numbering millions of men, had modernised and were making use of telephone, wireless communication,[287] armoured cars, tanks,[288] and aircraft.
Infantry formations were reorganised, so that 100-man companies were no longer the main unit of manoeuvre; instead, squads of 10 or so men, under the command of a junior NCO, were favoured.
Artillery also underwent a revolution.
In 1914, cannons were positioned in the front line and fired directly at their targets.
By 1917, indirect fire with guns (as well as mortars and even machine guns) was commonplace, using new techniques for spotting and ranging, notably aircraft and the often overlooked field telephone.
[289] Counter-battery missions became commonplace, also, and sound detection was used to locate enemy batteries.
Germany was far ahead of the Allies in using heavy indirect fire.
The German Army employed 150 mm (6 in) and 210 mm (8 in) howitzers in 1914, when typical French and British guns were only 75 mm (3 in) and 105 mm (4 in).
The British had a 6-inch (152 mm) howitzer, but it was so heavy it had to be hauled to the field in pieces and assembled.
The Germans also fielded Austrian 305 mm (12 in) and 420 mm (17 in) guns and, even at the beginning of the war, had inventories of various calibres of Minenwerfer, which were ideally suited for trench warfare.
On 27 June 1917 the Germans used the biggest gun in the world, Batterie Pommern, nicknamed "Lange Max".
This gun from Krupp was able to shoot 750 kg shells from Koekelare to Dunkirk, a distance of about 50 km (31 mi).
Much of the combat involved trench warfare, in which hundreds often died for each metre gained.
Many of the deadliest battles in history occurred during World War I.
Such battles include Ypres, the Marne, Cambrai, the Somme, Verdun, and Gallipoli.
The Germans employed the Haber process of nitrogen fixation to provide their forces with a constant supply of gunpowder despite the British naval blockade.
[292] Artillery was responsible for the largest number of casualties[293] and consumed vast quantities of explosives.
The large number of head wounds caused by exploding shells and fragmentation forced the combatant nations to develop the modern steel helmet, led by the French, who introduced the Adrian helmet in 1915.
It was quickly followed by the Brodie helmet, worn by British Imperial and US troops, and in 1916 by the distinctive German Stahlhelm, a design, with improvements, still in use today.
Quick, boys!
– An ecstasy of fumbling,
Fitting the clumsy helmets just in time;
But someone still was yelling out and stumbling,
And flound'ring like a man in fire or lime ...
Dim, through the misty panes and thick green light,
As under a green sea, I saw him drowning.
The widespread use of chemical warfare was a distinguishing feature of the conflict.
Gases used included chlorine, mustard gas and phosgene.
Relatively few war casualties were caused by gas,[295] as effective countermeasures to gas attacks were quickly created, such as gas masks.
The use of chemical warfare and small-scale strategic bombing (as opposed to tactical bombing) were both outlawed by the Hague Conventions of 1899 and 1907, and both proved to be of limited effectiveness,[296] though they captured the public imagination.
The most powerful land-based weapons were railway guns, weighing dozens of tons apiece.
[298] The German version were nicknamed Big Berthas, even though the namesake was not a railway gun.
Germany developed the Paris Gun, able to bombard Paris from over 100 kilometres (62 mi), though shells were relatively light at 94 kilograms (210 lb).
Trenches, machine guns, air reconnaissance, barbed wire, and modern artillery with fragmentation shells helped bring the battle lines of World War I to a stalemate.
The British and the French sought a solution with the creation of the tank and mechanised warfare.
The British first tanks were used during the Battle of the Somme on 15 September 1916.
Mechanical reliability was an issue, but the experiment proved its worth.
Within a year, the British were fielding tanks by the hundreds, and they showed their potential during the Battle of Cambrai in November 1917, by breaking the Hindenburg Line, while combined arms teams captured 8,000 enemy soldiers and 100 guns.
Meanwhile, the French introduced the first tanks with a rotating turret, the Renault FT, which became a decisive tool of the victory.
The conflict also saw the introduction of light automatic weapons and submachine guns, such as the Lewis Gun, the Browning Automatic Rifle, and the Bergmann MP18.
Another new weapon, the flamethrower, was first used by the German army and later adopted by other forces.
Although not of high tactical value, the flamethrower was a powerful, demoralising weapon that caused terror on the battlefield.
Trench railways evolved to supply the enormous quantities of food, water, and ammunition required to support large numbers of soldiers in areas where conventional transportation systems had been destroyed.
Internal combustion engines and improved traction systems for automobiles and trucks/lorries eventually rendered trench railways obsolete.
On the Western Front neither side made impressive gains in the first three years of the war with attacks at Verdun, the Somme, Passchendaele, and Cambrai — the exception was Nivelle's Offensive in which the German defence gave ground while mauling the attackers so badly that there were mutinies in the French Army.
In 1918 the Germans smashed through the defence lines in three great attacks: Michael, on the Lys, and on the Aisne, which displayed the power of their new tactics.
The Allies struck back at Soissons, which showed the Germans that they must return to the defensive, and at Amiens; tanks played a prominent role in both of these assaults, as they had the year before at Cambrai.
The areas in the East were larger.
The Germans did well at the First Masurian Lakes driving the invaders from East Prussia, and at Riga, which led the Russians to sue for peace.
The Austro-Hungarians and Germans joined for a great success at Gorlice–Tarnów, which drove the Russians out of Poland.
In a series of attacks along with the Bulgarians they occupied Serbia, Albania, Montenegro and most of Romania.
The Allies successes came later in Palestine, the beginning of the end for the Ottomans, in Macedonia, which drove the Bulgarians out of the war, and at Vittorio Veneto, the final blow for the Austro-Hungarians.
The area occupied in East by the Central powers on 11 November 1918 was 1,042,600 km2 (402,600 sq mi).
Germany deployed U-boats (submarines) after the war began.
Alternating between restricted and unrestricted submarine warfare in the Atlantic, the Kaiserliche Marine employed them to deprive the British Isles of vital supplies.
The deaths of British merchant sailors and the seeming invulnerability of U-boats led to the development of depth charges (1916), hydrophones (passive sonar, 1917), blimps, hunter-killer submarines (HMS R-1, 1917), forward-throwing anti-submarine weapons, and dipping hydrophones (the latter two both abandoned in 1918).
[118] To extend their operations, the Germans proposed supply submarines (1916).
Most of these would be forgotten in the interwar period until World War II revived the need.
Fixed-wing aircraft were first used militarily by the Italians in Libya on 23 October 1911 during the Italo-Turkish War for reconnaissance, soon followed by the dropping of grenades and aerial photography the next year.
By 1914, their military utility was obvious.
They were initially used for reconnaissance and ground attack.
To shoot down enemy planes, anti-aircraft guns and fighter aircraft were developed.
Strategic bombers were created, principally by the Germans and British, though the former used Zeppelins as well.
[301] Towards the end of the conflict, aircraft carriers were used for the first time, with HMS Furious launching Sopwith Camels in a raid to destroy the Zeppelin hangars at Tondern in 1918.
Manned observation balloons, floating high above the trenches, were used as stationary reconnaissance platforms, reporting enemy movements and directing artillery.
Balloons commonly had a crew of two, equipped with parachutes,[304] so that if there was an enemy air attack the crew could parachute to safety.
At the time, parachutes were too heavy to be used by pilots of aircraft (with their marginal power output), and smaller versions were not developed until the end of the war; they were also opposed by the British leadership, who feared they might promote cowardice.
Recognised for their value as observation platforms, balloons were important targets for enemy aircraft.
To defend them against air attack, they were heavily protected by antiaircraft guns and patrolled by friendly aircraft; to attack them, unusual weapons such as air-to-air rockets were tried.
Thus, the reconnaissance value of blimps and balloons contributed to the development of air-to-air combat between all types of aircraft, and to the trench stalemate, because it was impossible to move large numbers of troops undetected.
The Germans conducted air raids on England during 1915 and 1916 with airships, hoping to damage British morale and cause aircraft to be diverted from the front lines, and indeed the resulting panic led to the diversion of several squadrons of fighters from France.
On 19 August 1915, the German submarine U-27 was sunk by the British Q-ship HMS Baralong.
All German survivors were summarily executed by Baralong's crew on the orders of Lieutenant Godfrey Herbert, the captain of the ship.
The shooting was reported to the media by American citizens who were on board the Nicosia, a British freighter loaded with war supplies, which was stopped by U-27 just minutes before the incident.
On 24 September, Baralong destroyed U-41, which was in the process of sinking the cargo ship Urbino.
According to Karl Goetz, the submarine's commander, Baralong continued to fly the US flag after firing on U-41 and then rammed the lifeboat—carrying the German survivors—sinking it.
The Canadian hospital ship HMHS Llandovery Castle was torpedoed by the German submarine SM U-86 on 27 June 1918 in violation of international law.
Only 24 of the 258 medical personnel, patients, and crew survived.
Survivors reported that the U-boat surfaced and ran down the lifeboats, machine-gunning survivors in the water.
The U-boat captain, Helmut Patzig, was charged with war crimes in Germany following the war, but escaped prosecution by going to the Free City of Danzig, beyond the jurisdiction of German courts.
After the war, the German government claimed that approximately 763,000 German civilians died from starvation and disease during the war because of the Allied blockade.
[309][310] Germany protested that the Allies had used starvation as a weapon of war.
[311] According to the British judge and legal philosopher Patrick Devlin, "The War Orders given by the Admiralty on 26 August [1914] were clear enough.
All food consigned to Germany through neutral ports was to be captured and all food consigned to Rotterdam was to be presumed consigned to Germany.
The British were determined on the starvation policy, whether or not it was lawful.
The first successful use of poison gas as a weapon of warfare occurred during the Second Battle of Ypres (22 April – 25 May 1915).
[313] Gas was soon used by all major belligerents throughout the war.
It is estimated that the use of chemical weapons employed by both sides throughout the war had inflicted 1.3 million casualties.
For example, the British had over 180,000 chemical weapons casualties during the war, and up to one-third of American casualties were caused by them.
The Russian Army reportedly suffered roughly 500,000 chemical weapon casualties in World War I.
[314] The use of chemical weapons in warfare was in direct violation of the 1899 Hague Declaration Concerning Asphyxiating Gases and the 1907 Hague Convention on Land Warfare, which prohibited their use.
The effect of poison gas was not limited to combatants.
Civilians were at risk from the gases as winds blew the poison gases through their towns, and they rarely received warnings or alerts of potential danger.
In addition to absent warning systems, civilians often did not have access to effective gas masks.
An estimated 100,000–260,000 civilian casualties were caused by chemical weapons during the conflict and tens of thousands more (along with military personnel) died from scarring of the lungs, skin damage, and cerebral damage in the years after the conflict ended.
Many commanders on both sides knew such weapons would cause major harm to civilians but nonetheless continued to use them.
British Field Marshal Sir Douglas Haig wrote in his diary, "My officers and I were aware that such weapons would cause harm to women and children living in nearby towns, as strong winds were common in the battlefront.
However, because the weapon was to be directed against the enemy, none of us were overly concerned at all.
The ethnic cleansing of the Ottoman Empire's Armenian population, including mass deportations and executions, during the final years of the Ottoman Empire is considered genocide.
[323] The Ottomans carried out organised and systematic massacres of the Armenian population at the beginning of the war and portrayed deliberately provoked acts of Armenian resistance as rebellions to justify further extermination.
[324] In early 1915, a number of Armenians volunteered to join the Russian forces and the Ottoman government used this as a pretext to issue the Tehcir Law (Law on Deportation), which authorised the deportation of Armenians from the Empire's eastern provinces to Syria between 1915 and 1918.
The Armenians were intentionally marched to death and a number were attacked by Ottoman brigands.
[325] While an exact number of deaths is unknown, the International Association of Genocide Scholars estimates 1.5 million.
[323][326] The government of Turkey has consistently denied the genocide, arguing that those who died were victims of inter-ethnic fighting, famine, or disease during World War I; these claims are rejected by most historians.
Other ethnic groups were similarly attacked by the Ottoman Empire during this period, including Assyrians and Greeks, and some scholars consider those events to be part of the same policy of extermination.
[328][329][330] At least 250,000 Assyrian Christians, about half of the population, and 350,000–750,000 Anatolian and Pontic Greeks were killed between 1915 and 1922.
Many pogroms accompanied the Russian Revolution of 1917 and the ensuing Russian Civil War.
60,000–200,000 civilian Jews were killed in the atrocities throughout the former Russian Empire (mostly within the Pale of Settlement in present-day Ukraine).
The German invaders treated any resistance—such as sabotaging rail lines—as illegal and immoral, and shot the offenders and burned buildings in retaliation.
In addition, they tended to suspect that most civilians were potential francs-tireurs (guerrillas) and, accordingly, took and sometimes killed hostages from among the civilian population.
The German army executed over 6,500 French and Belgian civilians between August and November 1914, usually in near-random large-scale shootings of civilians ordered by junior German officers.
The German Army destroyed 15,000–20,000 buildings—most famously the university library at Louvain—and generated a wave of refugees of over a million people.
Over half the German regiments in Belgium were involved in major incidents.
[333] Thousands of workers were shipped to Germany to work in factories.
British propaganda dramatising the Rape of Belgium attracted much attention in the United States, while Berlin said it was both lawful and necessary because of the threat of franc-tireurs like those in France in 1870.
[334] The British and French magnified the reports and disseminated them at home and in the United States, where they played a major role in dissolving support for Germany.
The British soldiers of the war were initially volunteers but increasingly were conscripted into service.
Surviving veterans, returning home, often found they could discuss their experiences only amongst themselves.
Grouping together, they formed "veterans' associations" or "Legions".
A small number of personal accounts of American veterans have been collected by the Library of Congress Veterans History Project.
About eight million men surrendered and were held in POW camps during the war.
All nations pledged to follow the Hague Conventions on fair treatment of prisoners of war, and the survival rate for POWs was generally much higher than that of combatants at the front.
[338] Individual surrenders were uncommon; large units usually surrendered en masse.
At the siege of Maubeuge about 40,000 French soldiers surrendered, at the battle of Galicia Russians took about 100,000 to 120,000 Austrian captives, at the Brusilov Offensive about 325,000 to 417,000 Germans and Austrians surrendered to Russians, and at the Battle of Tannenberg 92,000 Russians surrendered.
When the besieged garrison of Kaunas surrendered in 1915, some 20,000 Russians became prisoners, at the battle near Przasnysz (February–March 1915) 14,000 Germans surrendered to Russians, and at the First Battle of the Marne about 12,000 Germans surrendered to the Allies.
25–31% of Russian losses (as a proportion of those captured, wounded, or killed) were to prisoner status; for Austria-Hungary 32%, for Italy 26%, for France 12%, for Germany 9%; for Britain 7%.
Prisoners from the Allied armies totalled about 1.4 million (not including Russia, which lost 2.5–3.5 million men as prisoners).
From the Central Powers about 3.3 million men became prisoners; most of them surrendered to Russians.
[339] Germany held 2.5 million prisoners; Russia held 2.2–2.9 million; while Britain and France held about 720,000.
Most were captured just before the Armistice.
The United States held 48,000.
The most dangerous moment was the act of surrender, when helpless soldiers were sometimes gunned down.
[340][341] Once prisoners reached a camp, conditions were, in general, satisfactory (and much better than in World War II), thanks in part to the efforts of the International Red Cross and inspections by neutral nations.
However, conditions were terrible in Russia: starvation was common for prisoners and civilians alike; about 15–20% of the prisoners in Russia died, and in Central Powers imprisonment 8% of Russians.
[342] In Germany, food was scarce, but only 5% died.
The Ottoman Empire often treated POWs poorly.
[346] Some 11,800 British Empire soldiers, most of them Indians, became prisoners after the Siege of Kut in Mesopotamia in April 1916; 4,250 died in captivity.
[347] Although many were in a poor condition when captured, Ottoman officers forced them to march 1,100 kilometres (684 mi) to Anatolia.
A survivor said: "We were driven along like beasts; to drop out was to die.
"[348] The survivors were then forced to build a railway through the Taurus Mountains.
In Russia, when the prisoners from the Czech Legion of the Austro-Hungarian army were released in 1917, they re-armed themselves and briefly became a military and diplomatic force during the Russian Civil War.
While the Allied prisoners of the Central Powers were quickly sent home at the end of active hostilities, the same treatment was not granted to Central Power prisoners of the Allies and Russia, many of whom served as forced labour, e.g., in France until 1920.
They were released only after many approaches by the Red Cross to the Allied Supreme Council.
[349] German prisoners were still being held in Russia as late as 1924.
Military and civilian observers from every major power closely followed the course of the war.
Many were able to report on events from a perspective somewhat akin to modern "embedded" positions within the opposing land and naval forces.
In the Balkans, Yugoslav nationalists such as the leader, Ante Trumbić, strongly supported the war, desiring the freedom of Yugoslavs from Austria-Hungary and other foreign powers and the creation of an independent Yugoslavia.
The Yugoslav Committee, led by Trumbić, was formed in Paris on 30 April 1915 but shortly moved its office to London.
[351] In April 1918, the Rome Congress of Oppressed Nationalities met, including Czechoslovak, Italian, Polish, Transylvanian, and Yugoslav representatives who urged the Allies to support national self-determination for the peoples residing within Austria-Hungary.
In the Middle East, Arab nationalism soared in Ottoman territories in response to the rise of Turkish nationalism during the war, with Arab nationalist leaders advocating the creation of a pan-Arab state.
In 1916, the Arab Revolt began in Ottoman-controlled territories of the Middle East in an effort to achieve independence.
In East Africa, Iyasu V of Ethiopia was supporting the Dervish state who were at war with the British in the Somaliland Campaign.
[354] Von Syburg, the German envoy in Addis Ababa, said, "now the time has come for Ethiopia to regain the coast of the Red Sea driving the Italians home, to restore the Empire to its ancient size."
The Ethiopian Empire was on the verge of entering World War I on the side of the Central Powers before Iyasu's overthrow due to Allied pressure on the Ethiopian aristocracy.
[355] Iyasu was accused of converting to Islam.
[356] According to Ethiopian historian Bahru Zewde, the evidence used to prove Iyasu's conversion was a doctored photo of Iyasu wearing a turban provided by the Allies.
[357] Some historians claim the British spy T. E. Lawrence forged the Iyasu photo.
A number of socialist parties initially supported the war when it began in August 1914.
[352] But European socialists split on national lines, with the concept of class conflict held by radical socialists such as Marxists and syndicalists being overborne by their patriotic support for the war.
[359] Once the war began, Austrian, British, French, German, and Russian socialists followed the rising nationalist current by supporting their countries' intervention in the war.
Italian nationalism was stirred by the outbreak of the war and was initially strongly supported by a variety of political factions.
One of the most prominent and popular Italian nationalist supporters of the war was Gabriele d'Annunzio, who promoted Italian irredentism and helped sway the Italian public to support intervention in the war.
[361] The Italian Liberal Party, under the leadership of Paolo Boselli, promoted intervention in the war on the side of the Allies and used the Dante Alighieri Society to promote Italian nationalism.
[362] Italian socialists were divided on whether to support the war or oppose it; some were militant supporters of the war, including Benito Mussolini and Leonida Bissolati.
[363] However, the Italian Socialist Party decided to oppose the war after anti-militarist protestors were killed, resulting in a general strike called Red Week.
[364] The Italian Socialist Party purged itself of pro-war nationalist members, including Mussolini.
[364] Mussolini, a syndicalist who supported the war on grounds of irredentist claims on Italian-populated regions of Austria-Hungary, formed the pro-interventionist Il Popolo d'Italia and the Fasci Rivoluzionario d'Azione Internazionalista ("Revolutionary Fasci for International Action") in October 1914 that later developed into the Fasci di Combattimento in 1919, the origin of fascism.
[365] Mussolini's nationalism enabled him to raise funds from Ansaldo (an armaments firm) and other companies to create Il Popolo d'Italia to convince socialists and revolutionaries to support the war.
Once war was declared, many socialists and trade unions backed their governments.
Among the exceptions were the Bolsheviks, the Socialist Party of America, the Italian Socialist Party, and people like Karl Liebknecht, Rosa Luxemburg, and their followers in Germany.
Benedict XV, elected to the papacy less than three months into World War I, made the war and its consequences the main focus of his early pontificate.
In stark contrast to his predecessor,[367] five days after his election he spoke of his determination to do what he could to bring peace.
His first encyclical, Ad beatissimi Apostolorum, given 1 November 1914, was concerned with this subject.
Benedict XV found his abilities and unique position as a religious emissary of peace ignored by the belligerent powers.
The 1915 Treaty of London between Italy and the Triple Entente included secret provisions whereby the Allies agreed with Italy to ignore papal peace moves towards the Central Powers.
Consequently, the publication of Benedict's proposed seven-point Peace Note of August 1917 was roundly ignored by all parties except Austria-Hungary.
In Britain in 1914, the Public Schools Officers' Training Corps annual camp was held at Tidworth Pennings, near Salisbury Plain.
Head of the British Army, Lord Kitchener, was to review the cadets, but the imminence of the war prevented him.
General Horace Smith-Dorrien was sent instead.
He surprised the two-or-three thousand cadets by declaring (in the words of Donald Christopher Smith, a Bermudian cadet who was present),
that war should be avoided at almost any cost, that war would solve nothing, that the whole of Europe and more besides would be reduced to ruin, and that the loss of life would be so large that whole populations would be decimated.
In our ignorance I, and many of us, felt almost ashamed of a British General who uttered such depressing and unpatriotic sentiments, but during the next four years, those of us who survived the holocaust—probably not more than one-quarter of us—learned how right the General's prognosis was and how courageous he had been to utter it.
Voicing these sentiments did not hinder Smith-Dorrien's career, or prevent him from doing his duty in World War I to the best of his abilities.
Many countries jailed those who spoke out against the conflict.
These included Eugene Debs in the United States and Bertrand Russell in Britain.
In the US, the Espionage Act of 1917 and Sedition Act of 1918 made it a federal crime to oppose military recruitment or make any statements deemed "disloyal".
Publications at all critical of the government were removed from circulation by postal censors,[186] and many served long prison sentences for statements of fact deemed unpatriotic.
A number of nationalists opposed intervention, particularly within states that the nationalists were hostile to.
Although the vast majority of Irish people consented to participate in the war in 1914 and 1915, a minority of advanced Irish nationalists staunchly opposed taking part.
[370] The war began amid the Home Rule crisis in Ireland that had resurfaced in 1912, and by July 1914 there was a serious possibility of an outbreak of civil war in Ireland.
Irish nationalists and Marxists attempted to pursue Irish independence, culminating in the Easter Rising of 1916, with Germany sending 20,000 rifles to Ireland to stir unrest in Britain.
[371] The UK government placed Ireland under martial law in response to the Easter Rising, though once the immediate threat of revolution had dissipated, the authorities did try to make concessions to nationalist feeling.
[372] However, opposition to involvement in the war increased in Ireland, resulting in the Conscription Crisis of 1918.
Other opposition came from conscientious objectors—some socialist, some religious—who refused to fight.
In Britain, 16,000 people asked for conscientious objector status.
[373] Some of them, most notably prominent peace activist Stephen Henry Hobhouse, refused both military and alternative service.
[374] Many suffered years of prison, including solitary confinement and bread and water diets.
Even after the war, in Britain many job advertisements were marked "No conscientious objectors need apply".
[This quote needs a citation]
The Central Asian Revolt started in the summer of 1916, when the Russian Empire government ended its exemption of Muslims from military service.
In 1917, a series of French Army Mutinies led to dozens of soldiers being executed and many more imprisoned.
In Milan, in May 1917, Bolshevik revolutionaries organised and engaged in rioting calling for an end to the war, and managed to close down factories and stop public transportation.
[376] The Italian army was forced to enter Milan with tanks and machine guns to face Bolsheviks and anarchists, who fought violently until 23 May when the army gained control of the city.
Almost 50 people (including three Italian soldiers) were killed and over 800 people arrested.
In September 1917, Russian soldiers in France began questioning why they were fighting for the French at all and mutinied.
[377] In Russia, opposition to the war led to soldiers also establishing their own revolutionary committees, which helped foment the October Revolution of 1917, with the call going up for "bread, land, and peace".
[This quote needs a citation] The Bolsheviks agreed to a peace treaty with Germany, the peace of Brest-Litovsk, despite its harsh conditions.
The German Revolution of 1918-1919 led to the abdication of the Kaiser and German surrender.
Conscription was common in most European countries.
However, it was controversial in English-speaking countries.
It was especially unpopular among minority ethnic groups—especially the Irish Catholics in Ireland[378] and Australia, and the French Catholics in Canada.
In Canada the issue produced a major political crisis that permanently alienated the Francophones.
It opened a political gap between French Canadians, who believed their true loyalty was to Canada and not to the British Empire, and members of the Anglophone majority, who saw the war as a duty to their British heritage.
Australia had a form of conscription at the outbreak of the war, as compulsory military training had been introduced in 1911.
However, the Defence Act 1903 provided that unexempted males could only be called upon for home defence during times of war, not overseas service.
Prime Minister Billy Hughes wished to amend the legislation to require conscripts to serve overseas, and held two non-binding referendums – one in 1916 and one in 1917 – in order to secure public support.
[380] Both were defeated by narrow margins, with farmers, the labour movement, the Catholic Church, and Irish-Australians combining to campaign for the "No" vote.
[381] The issue of conscription caused the 1916 Australian Labor Party split.
Hughes and his supporters were expelled from the party, forming the National Labor Party and then the Nationalist Party.
Despite the referendum results, the Nationalists won a landslide victory at the 1917 federal election.
In Britain, conscription resulted in the calling up of nearly every physically fit man in Britain—six of ten million eligible.
Of these, about 750,000 lost their lives.
Most deaths were those of young unmarried men; however, 160,000 wives lost husbands and 300,000 children lost fathers.
[382] Conscription during the First World War began when the British government passed the Military Service Act in 1916.
The act specified that single men aged 18 to 40 years old were liable to be called up for military service unless they were widowed with children or ministers of a religion.
There was a system of Military Service Tribunals to adjudicate upon claims for exemption upon the grounds of performing civilian work of national importance, domestic hardship, health, and conscientious objection.
The law went through several changes before the war ended.
Married men were exempt in the original Act, although this was changed in June 1916.
The age limit was also eventually raised to 51 years old.
Recognition of work of national importance also diminished, and in the last year of the war there was some support for the conscription of clergy.
[383] Conscription lasted until mid-1919.
Due to the political situation in Ireland, conscription was never applied there; only in England, Scotland and Wales.
In the United States, conscription began in 1917 and was generally well received, with a few pockets of opposition in isolated rural areas.
[384] The administration decided to rely primarily on conscription, rather than voluntary enlistment, to raise military manpower for when only 73,000 volunteers enlisted out of the initial 1 million target in the first six weeks of the war.
[385] In 1917 10 million men were registered.
This was deemed to be inadequate, so age ranges were increased and exemptions reduced, and so by the end of 1918 this increased to 24 million men that were registered with nearly 3 million inducted into the military services.
The draft was universal and included blacks on the same terms as whites, although they served in different units.
In all 367,710 black Americans were drafted (13% of the total), compared to 2,442,586 white (87%).
Forms of resistance ranged from peaceful protest to violent demonstrations and from humble letter-writing campaigns asking for mercy to radical newspapers demanding reform.
The most common tactics were dodging and desertion, and many communities sheltered and defended their draft dodgers as political heroes.
Many socialists were jailed for "obstructing the recruitment or enlistment service".
The most famous was Eugene Debs, head of the Socialist Party of America, who ran for president in 1920 from his prison cell.
In 1917 a number of radicals and anarchists challenged the new draft law in federal court, arguing that it was a direct violation of the Thirteenth Amendment's prohibition against slavery and involuntary servitude.
The Supreme Court unanimously upheld the constitutionality of the draft act in the Selective Draft Law Cases on 7 January 1918.
Like all of the armies of mainland Europe, Austria-Hungary relied on conscription to fill its ranks.
Officer recruitment, however, was voluntary.
The effect of this at the start of the war was that well over a quarter of the rank and file were Slavs, while more than 75% of the officers were ethnic Germans.
This was much resented.
The army has been described as being "run on colonial lines" and the Slav soldiers as "disaffected".
Thus conscription contributed greatly to Austria's disastrous performance on the battlefield.
The non-military diplomatic and propaganda interactions among the nations were designed to build support for the cause, or to undermine support for the enemy.
For the most part, wartime diplomacy focused on five issues: propaganda campaigns; defining and redefining the war goals, which became harsher as the war went on; luring neutral nations (Italy, Ottoman Empire, Bulgaria, Romania) into the coalition by offering slices of enemy territory; and encouragement by the Allies of nationalistic minority movements inside the Central Powers, especially among Czechs, Poles, and Arabs.
In addition, there were multiple peace proposals coming from neutrals, or one side or the other; none of them progressed very far.
... "Strange, friend," I said, "Here is no cause to mourn."
"None," said the other, "Save the undone years"...
The War was an unprecedented triumph for natural science.
[Francis] Bacon had promised that knowledge would be power, and power it was: power to destroy the bodies and souls of men more rapidly than had ever been done by human agency before.
This triumph paved the way to other triumphs: improvements in transport, in sanitation, in surgery, medicine, and psychiatry, in commerce and industry, and, above all, in preparations for the next war.
The first tentative efforts to comprehend the meaning and consequences of modern warfare began during the initial phases of the war, and this process continued throughout and after the end of hostilities, and is still underway, more than a century later.
Historian Heather Jones argues that the historiography has been reinvigorated by the cultural turn in recent years.
Scholars have raised entirely new questions regarding military occupation, radicalisation of politics, race, and the male body.
Furthermore, new research has revised our understanding of five major topics that historians have long debated: Why the war began, why the Allies won, whether generals were responsible for high casualty rates, how the soldiers endured the horrors of trench warfare, and to what extent the civilian homefront accepted and endorsed the war effort.
Memorials were erected in thousands of villages and towns.
Close to battlefields, those buried in improvised burial grounds were gradually moved to formal graveyards under the care of organisations such as the Commonwealth War Graves Commission, the American Battle Monuments Commission, the German War Graves Commission, and Le Souvenir français.
Many of these graveyards also have central monuments to the missing or unidentified dead, such as the Menin Gate memorial and the Thiepval Memorial to the Missing of the Somme.
In 1915 John McCrae, a Canadian army doctor, wrote the poem In Flanders Fields as a salute to those who perished in the Great War.
Published in Punch on 8 December 1915, it is still recited today, especially on Remembrance Day and Memorial Day.
National World War I Museum and Memorial in Kansas City, Missouri, is a memorial dedicated to all Americans who served in World War I.
The Liberty Memorial was dedicated on 1 November 1921, when the supreme Allied commanders spoke to a crowd of more than 100,000 people.
The UK Government has budgeted substantial resources to the commemoration of the war during the period 2014 to 2018.
The lead body is the Imperial War Museum.
[395] On 3 August 2014, French President Francois Hollande and German President Joachim Gauck together marked the centenary of Germany's declaration of war on France by laying the first stone of a memorial in Vieil Armand, known in German as Hartmannswillerkopf, for French and German soldiers killed in the war.
World War I had a lasting impact on social memory.
It was seen by many in Britain as signalling the end of an era of stability stretching back to the Victorian period, and across Europe many regarded it as a watershed.
[397] Historian Samuel Hynes explained:
A generation of innocent young men, their heads full of high abstractions like Honour, Glory and England, went off to war to make the world safe for democracy.
They were slaughtered in stupid battles planned by stupid generals.
Those who survived were shocked, disillusioned and embittered by their war experiences, and saw that their real enemies were not the Germans, but the old men at home who had lied to them.
They rejected the values of the society that had sent them to war, and in doing so separated their own generation from the past and from their cultural inheritance.
This has become the most common perception of World War I, perpetuated by the art, cinema, poems, and stories published subsequently.
Films such as All Quiet on the Western Front, Paths of Glory and King & Country have perpetuated the idea, while war-time films including Camrades, Poppies of Flanders, and Shoulder Arms indicate that the most contemporary views of the war were overall far more positive.
[399] Likewise, the art of Paul Nash, John Nash, Christopher Nevinson, and Henry Tonks in Britain painted a negative view of the conflict in keeping with the growing perception, while popular war-time artists such as Muirhead Bone painted more serene and pleasant interpretations subsequently rejected as inaccurate.
[398] Several historians like John Terraine, Niall Ferguson and Gary Sheffield have challenged these interpretations as partial and polemical views:
These beliefs did not become widely shared because they offered the only accurate interpretation of wartime events.
In every respect, the war was much more complicated than they suggest.
In recent years, historians have argued persuasively against almost every popular cliché of World War I.
It has been pointed out that, although the losses were devastating, their greatest impact was socially and geographically limited.
The many emotions other than horror experienced by soldiers in and out of the front line, including comradeship, boredom, and even enjoyment, have been recognised.
The war is not now seen as a 'fight about nothing', but as a war of ideals, a struggle between aggressive militarism and more or less liberal democracy.
It has been acknowledged that British generals were often capable men facing difficult challenges, and that it was under their command that the British army played a major part in the defeat of the Germans in 1918: a great forgotten victory.
Though these views have been discounted as "myths",[398][400] they are common.
They have dynamically changed according to contemporary influences, reflecting in the 1950s perceptions of the war as "aimless" following the contrasting Second World War and emphasising conflict within the ranks during times of class conflict in the 1960s.
The majority of additions to the contrary are often rejected.
The social trauma caused by unprecedented rates of casualties manifested itself in different ways, which have been the subject of subsequent historical debate.
The optimism of la belle époque was destroyed, and those who had fought in the war were referred to as the Lost Generation.
[402] For years afterwards, people mourned the dead, the missing, and the many disabled.
[403] Many soldiers returned with severe trauma, suffering from shell shock (also called neurasthenia, a condition related to posttraumatic stress disorder).
[404] Many more returned home with few after-effects; however, their silence about the war contributed to the conflict's growing mythological status.
Though many participants did not share in the experiences of combat or spend any significant time at the front, or had positive memories of their service, the images of suffering and trauma became the widely shared perception.
Such historians as Dan Todman, Paul Fussell, and Samuel Heyns have all published works since the 1990s arguing that these common perceptions of the war are factually incorrect.
The rise of Nazism and Fascism included a revival of the nationalist spirit and a rejection of many post-war changes.
Similarly, the popularity of the stab-in-the-back legend (German: Dolchstoßlegende) was a testament to the psychological state of defeated Germany and was a rejection of responsibility for the conflict.
This conspiracy theory of betrayal became common, and the German populace came to see themselves as victims.
The widespread acceptance of the "stab-in-the-back" theory delegitimised the Weimar government and destabilised the system, opening it to extremes of right and left.
Communist and fascist movements around Europe drew strength from this theory and enjoyed a new level of popularity.
These feelings were most pronounced in areas directly or harshly affected by the war.
Adolf Hitler was able to gain popularity by using German discontent with the still controversial Treaty of Versailles.
[405] World War II was in part a continuation of the power struggle never fully resolved by World War I.
Furthermore, it was common for Germans in the 1930s to justify acts of aggression due to perceived injustices imposed by the victors of World War I.
[406][407][408] American historian William Rubinstein wrote that:
The 'Age of Totalitarianism' included nearly all of the infamous examples of genocide in modern history, headed by the Jewish Holocaust, but also comprising the mass murders and purges of the Communist world, other mass killings carried out by Nazi Germany and its allies, and also the Armenian Genocide of 1915.
All these slaughters, it is argued here, had a common origin, the collapse of the elite structure and normal modes of government of much of central, eastern and southern Europe as a result of World War I, without which surely neither Communism nor Fascism would have existed except in the minds of unknown agitators and crackpots.
One of the most dramatic effects of the war was the expansion of governmental powers and responsibilities in Britain, France, the United States, and the Dominions of the British Empire.
To harness all the power of their societies, governments created new ministries and powers.
New taxes were levied and laws enacted, all designed to bolster the war effort; many have lasted to this day.
Similarly, the war strained the abilities of some formerly large and bureaucratised governments, such as in Austria-Hungary and Germany.
Gross domestic product (GDP) increased for three Allies (Britain, Italy, and the United States), but decreased in France and Russia, in neutral Netherlands, and in the three main Central Powers.
The shrinkage in GDP in Austria, Russia, France, and the Ottoman Empire ranged between 30% and 40%.
In Austria, for example, most pigs were slaughtered, so at war's end there was no meat.
In all nations, the government's share of GDP increased, surpassing 50% in both Germany and France and nearly reaching that level in Britain.
To pay for purchases in the United States, Britain cashed in its extensive investments in American railroads and then began borrowing heavily from Wall Street.
President Wilson was on the verge of cutting off the loans in late 1916, but allowed a great increase in US government lending to the Allies.
After 1919, the US demanded repayment of these loans.
The repayments were, in part, funded by German reparations that, in turn, were supported by American loans to Germany.
This circular system collapsed in 1931 and some loans were never repaid.
Britain still owed the United States $4.4 billion[410] of World War I debt in 1934, the last instalment was finally paid in 2015.
Macro- and micro-economic consequences devolved from the war.
Families were altered by the departure of many men.
With the death or absence of the primary wage earner, women were forced into the workforce in unprecedented numbers.
At the same time, industry needed to replace the lost labourers sent to war.
This aided the struggle for voting rights for women.
World War I further compounded the gender imbalance, adding to the phenomenon of surplus women.
The deaths of nearly one million men during the war in Britain increased the gender gap by almost a million: from 670,000 to 1,700,000.
The number of unmarried women seeking economic means grew dramatically.
In addition, demobilisation and economic decline following the war caused high unemployment.
The war increased female employment; however, the return of demobilised men displaced many from the workforce, as did the closure of many of the wartime factories.
In Britain, rationing was finally imposed in early 1918, limited to meat, sugar, and fats (butter and margarine), but not bread.
The new system worked smoothly.
From 1914 to 1918, trade union membership doubled, from a little over four million to a little over eight million.
Britain turned to her colonies for help in obtaining essential war materials whose supply from traditional sources had become difficult.
Geologists such as Albert Ernest Kitson were called on to find new resources of precious minerals in the African colonies.
Kitson discovered important new deposits of manganese, used in munitions production, in the Gold Coast.
Article 231 of the Treaty of Versailles (the so-called "war guilt" clause) stated Germany accepted responsibility for "all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by the aggression of Germany and her allies.
"[414] It was worded as such to lay a legal basis for reparations, and a similar clause was inserted in the treaties with Austria and Hungary.
However neither of them interpreted it as an admission of war guilt.
"[415] In 1921, the total reparation sum was placed at 132 billion gold marks.
However, "Allied experts knew that Germany could not pay" this sum.
The total sum was divided into three categories, with the third being "deliberately designed to be chimerical" and its "primary function was to mislead public opinion ...  into believing the "total sum was being maintained.
"[416] Thus, 50 billion gold marks (12.5 billion dollars) "represented the actual Allied assessment of German capacity to pay" and "therefore ... represented the total German reparations" figure that had to be paid.
This figure could be paid in cash or in kind (coal, timber, chemical dyes, etc.).
In addition, some of the territory lost—via the treaty of Versailles—was credited towards the reparation figure as were other acts such as helping to restore the Library of Louvain.
[417] By 1929, the Great Depression arrived, causing political chaos throughout the world.
[418] In 1932 the payment of reparations was suspended by the international community, by which point Germany had only paid the equivalent of 20.598 billion gold marks in reparations.
[419] With the rise of Adolf Hitler, all bonds and loans that had been issued and taken out during the 1920s and early 1930s were cancelled.
David Andelman notes "refusing to pay doesn't make an agreement null and void.
The bonds, the agreement, still exist."
Thus, following the Second World War, at the London Conference in 1953, Germany agreed to resume payment on the money borrowed.
On 3 October 2010, Germany made the final payment on these bonds.
The war contributed to the evolution of the wristwatch from women's jewellery to a practical everyday item, replacing the pocketwatch, which requires a free hand to operate.
[424] Military funding of advancements in radio contributed to the postwar popularity of the medium.
Pacific War
Mediterranean and Middle East
Other campaigns
Contemporaneous wars
World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945.
The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis.
A state of total war emerged, directly involving more than 100 million people from over 30 countries.
The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources.
World War II was the deadliest conflict in human history, marked by 50 to 85 million fatalities, most of whom were civilians in the Soviet Union and China.
It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.
Japan, which aimed to dominate Asia and the Pacific, was at war with China by 1937,[5][b] though neither side had declared war on the other.
World War II is generally said to have begun on 1 September 1939,[6] with the invasion of Poland by Germany and subsequent declarations of war on Germany by France and the United Kingdom.
From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan.
Under the Molotov–Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states.
Following the onset of campaigns in North Africa and East Africa, and the fall of France in mid 1940, the war continued primarily between the European Axis powers and the British Empire.
War in the Balkans, the aerial Battle of Britain, the Blitz, and the long Battle of the Atlantic followed.
On 22 June 1941, the European Axis powers launched an invasion of the Soviet Union, opening the largest land theatre of war in history.
This Eastern Front trapped the Axis, most crucially the German Wehrmacht, into a war of attrition.
In December 1941, Japan launched a surprise attack on the United States as well as European colonies in the Pacific.
Following an immediate U.S. declaration of war against Japan, supported by one from Great Britain, the European Axis powers quickly declared war on the U.S. in solidarity with their Japanese ally.
Rapid Japanese conquests over much of the Western Pacific ensued, perceived by many in Asia as liberation from Western dominance and resulting in the support of several armies from defeated territories.
The Axis advance in the Pacific halted in 1942 when Japan lost the critical Battle of Midway; later, Germany and Italy were defeated in North Africa and then, decisively, at Stalingrad in the Soviet Union.
Key setbacks in 1943, which included a series of German defeats on the Eastern Front, the Allied invasions of Sicily and Italy, and Allied victories in the Pacific, cost the Axis its initiative and forced it into strategic retreat on all fronts.
In 1944, the Western Allies invaded German-occupied France, while the Soviet Union regained its territorial losses and turned toward Germany and its allies.
During 1944 and 1945 the Japanese suffered major reverses in mainland Asia in Central China, South China and Burma, while the Allies crippled the Japanese Navy and captured key Western Pacific islands.
The war in Europe concluded with an invasion of Germany by the Western Allies and the Soviet Union, culminating in the capture of Berlin by Soviet troops, the suicide of Adolf Hitler and the German unconditional surrender on 8 May 1945.
Following the Potsdam Declaration by the Allies on 26 July 1945 and the refusal of Japan to surrender under its terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki on 6 and 9 August respectively.
With an invasion of the Japanese archipelago imminent, the possibility of additional atomic bombings, the Soviet entry into the war against Japan and its invasion of Manchuria, Japan announced its intention to surrender on 15 August 1945, cementing total victory in Asia for the Allies.
Tribunals were set up by fiat by the Allies and war crimes trials were conducted in the wake of the war both against the Germans and the Japanese.
World War II changed the political alignment and social structure of the globe.
The United Nations (UN) was established to foster international co-operation and prevent future conflicts; the victorious great powers—China, France, the Soviet Union, the United Kingdom, and the United States—became the permanent members of its Security Council.
[7] The Soviet Union and United States emerged as rival superpowers, setting the stage for the nearly half-century long Cold War.
In the wake of European devastation, the influence of its great powers waned, triggering the decolonisation of Africa and Asia.
Most countries whose industries had been damaged moved towards economic recovery and expansion.
Political integration, especially in Europe, emerged as an effort to end pre-war enmities and create a common identity.
The start of the war in Europe is generally held to be 1 September 1939,[9][10] beginning with the German invasion of Poland; the United Kingdom and France declared war on Germany two days later.
The dates for the beginning of war in the Pacific include the start of the Second Sino-Japanese War on 7 July 1937,[11][12] or even the Japanese invasion of Manchuria on 19 September 1931.
Others follow the British historian A.J.P.
Taylor, who held that the Sino-Japanese War and war in Europe and its colonies occurred simultaneously, and the two wars merged in 1941.
This article uses the conventional dating.
Other starting dates sometimes used for World War II include the Italian invasion of Abyssinia on 3 October 1935.
[15] The British historian Antony Beevor views the beginning of World War II as the Battles of Khalkhin Gol fought between Japan and the forces of Mongolia and the Soviet Union from May to September 1939.
The exact date of the war's end is also not universally agreed upon.
It was generally accepted at the time that the war ended with the armistice of 14 August 1945 (V-J Day), rather than the formal surrender of Japan, which was on 2 September 1945 that officially ended the war in Asia.
A peace treaty with Japan was signed in 1951.
[17] A treaty regarding Germany's future allowed the reunification of East and West Germany to take place in 1990 and resolved most post-World War II issues.
[18] No formal peace treaty between Japan and the Soviet Union was ever signed.
World War I had radically altered the political European map, with the defeat of the Central Powers—including Austria-Hungary, Germany, Bulgaria and the Ottoman Empire—and the 1917 Bolshevik seizure of power in Russia, which eventually led to the founding of the Soviet Union.
Meanwhile, the victorious Allies of World War I, such as France, Belgium, Italy, Romania and Greece, gained territory, and new nation-states were created out of the collapse of Austria-Hungary and the Ottoman and Russian Empires.
To prevent a future world war, the League of Nations was created during the 1919 Paris Peace Conference.
The organisation's primary goals were to prevent armed conflict through collective security, military and naval disarmament, and settling international disputes through peaceful negotiations and arbitration.
Despite strong pacifist sentiment after World War I,[20] its aftermath still caused irredentist and revanchist nationalism in several European states.
These sentiments were especially marked in Germany because of the significant territorial, colonial, and financial losses incurred by the Treaty of Versailles.
Under the treaty, Germany lost around 13 percent of its home territory and all of its overseas possessions, while German annexation of other states was prohibited, reparations were imposed, and limits were placed on the size and capability of the country's armed forces.
The German Empire was dissolved in the German Revolution of 1918–1919, and a democratic government, later known as the Weimar Republic, was created.
The interwar period saw strife between supporters of the new republic and hardline opponents on both the right and left.
Italy, as an Entente ally, had made some post-war territorial gains; however, Italian nationalists were angered that the promises made by the United Kingdom and France to secure Italian entrance into the war were not fulfilled in the peace settlement.
From 1922 to 1925, the Fascist movement led by Benito Mussolini seized power in Italy with a nationalist, totalitarian, and class collaborationist agenda that abolished representative democracy, repressed socialist, left-wing and liberal forces, and pursued an aggressive expansionist foreign policy aimed at making Italy a world power, promising the creation of a "New Roman Empire".
Adolf Hitler, after an unsuccessful attempt to overthrow the German government in 1923, eventually became the Chancellor of Germany in 1933.
He abolished democracy, espousing a radical, racially motivated revision of the world order, and soon began a massive rearmament campaign.
[23] Meanwhile, France, to secure its alliance, allowed Italy a free hand in Ethiopia, which Italy desired as a colonial possession.
The situation was aggravated in early 1935 when the Territory of the Saar Basin was legally reunited with Germany and Hitler repudiated the Treaty of Versailles, accelerated his rearmament programme, and introduced conscription.
The United Kingdom, France and Italy formed the Stresa Front in April 1935 in order to contain Germany, a key step towards military globalization; however, that June, the United Kingdom made an independent naval agreement with Germany, easing prior restrictions.
The Soviet Union, concerned by Germany's goals of capturing vast areas of Eastern Europe, drafted a treaty of mutual assistance with France.
Before taking effect though, the Franco-Soviet pact was required to go through the bureaucracy of the League of Nations, which rendered it essentially toothless.
[25] The United States, concerned with events in Europe and Asia, passed the Neutrality Act in August of the same year.
Hitler defied the Versailles and Locarno treaties by remilitarising the Rhineland in March 1936, encountering little opposition due to appeasement.
[27] In October 1936, Germany and Italy formed the Rome–Berlin Axis.
A month later, Germany and Japan signed the Anti-Comintern Pact, which Italy would join in the following year.
The Kuomintang (KMT) party in China launched a unification campaign against regional warlords and nominally unified China in the mid-1920s, but was soon embroiled in a civil war against its former Chinese Communist Party allies[29] and new regional warlords.
In 1931, an increasingly militaristic Empire of Japan, which had long sought influence in China[30] as the first step of what its government saw as the country's right to rule Asia, used the Mukden Incident as a pretext to launch an invasion of Manchuria and establish the puppet state of Manchukuo.
China appealed to the League of Nations to stop the Japanese invasion of Manchuria.
Japan withdrew from the League of Nations after being condemned for its incursion into Manchuria.
The two nations then fought several battles, in Shanghai, Rehe and Hebei, until the Tanggu Truce was signed in 1933.
Thereafter, Chinese volunteer forces continued the resistance to Japanese aggression in Manchuria, and Chahar and Suiyuan.
[32] After the 1936 Xi'an Incident, the Kuomintang and communist forces agreed on a ceasefire to present a united front to oppose Japan.
The Second Italo–Ethiopian War was a brief colonial war that began in October 1935 and ended in May 1936.
The war began with the invasion of the Ethiopian Empire (also known as Abyssinia) by the armed forces of the Kingdom of Italy (Regno d'Italia), which was launched from Italian Somaliland and Eritrea.
[34] The war resulted in the military occupation of Ethiopia and its annexation into the newly created colony of Italian East Africa (Africa Orientale Italiana, or AOI); in addition it exposed the weakness of the League of Nations as a force to preserve peace.
Both Italy and Ethiopia were member nations, but the League did little when the former clearly violated Article X of the League's Covenant.
[35] The United Kingdom and France supported imposing sanctions on Italy for the invasion, but they were not fully enforced and failed to end the Italian invasion.
[36] Italy subsequently dropped its objections to Germany's goal of absorbing Austria.
When civil war broke out in Spain, Hitler and Mussolini lent military support to the Nationalist rebels, led by General Francisco Franco.
Italy supported the Nationalist to a greater extent than  National-Socialist did: altogether Mussolini sent to Spain more than 70,000 ground troops and 6,000 aviation personnel, as well as about 720 aircraft.
[38] The Soviet Union supported the existing government, the Spanish Republic.
Over 30,000 foreign volunteers, known as the International Brigades, also fought against the Nationalists.
Both Germany and the Soviet Union used this proxy war as an opportunity to test in combat their most advanced weapons and tactics.
The Nationalists won the civil war in April 1939; Franco, now dictator, remained officially neutral during World War II but generally favoured the Axis.
[39] His greatest collaboration with Germany was the sending of volunteers to fight on the Eastern Front.
In July 1937, Japan captured the former Chinese imperial capital of Peking after instigating the Marco Polo Bridge Incident, which culminated in the Japanese campaign to invade all of China.
[41] The Soviets quickly signed a non-aggression pact with China to lend materiel support, effectively ending China's prior co-operation with Germany.
From September to November, the Japanese attacked Taiyuan, engaged the Kuomintang Army around Xinkou,[42] and fought Communist forces in Pingxingguan.
[43][44] Generalissimo Chiang Kai-shek deployed his best army to defend Shanghai, but, after three months of fighting, Shanghai fell.
The Japanese continued to push the Chinese forces back, capturing the capital Nanking in December 1937.
After the fall of Nanking, tens of thousands if not hundreds of thousands of Chinese civilians and disarmed combatants were murdered by the Japanese.
In March 1938, Nationalist Chinese forces won their first major victory at Taierzhuang but then the city of Xuzhou was taken by Japanese in May.
[47] In June 1938, Chinese forces stalled the Japanese advance by flooding the Yellow River; this manoeuvre bought time for the Chinese to prepare their defences at Wuhan, but the city was taken by October.
[48] Japanese military victories did not bring about the collapse of Chinese resistance that Japan had hoped to achieve; instead the Chinese government relocated inland to Chongqing and continued the war.
In the mid-to-late 1930s, Japanese forces in Manchukuo had sporadic border clashes with the Soviet Union and Mongolia.
The Japanese doctrine of Hokushin-ron, which emphasised Japan's expansion northward, was favoured by the Imperial Army during this time.
With the Japanese defeat at Khalkin Gol in 1939, the ongoing Second Sino-Japanese War[51] and ally Nazi Germany pursuing neutrality with the Soviets, this policy would prove difficult to maintain.
Japan and the Soviet Union eventually signed a Neutrality Pact in April 1941, and Japan adopted the doctrine of Nanshin-ron, promoted by the Navy, which took its focus southward, eventually leading to its war with the United States and the Western Allies.
In Europe, Germany and Italy were becoming more aggressive.
In March 1938, Germany annexed Austria, again provoking little response from other European powers.
[54] Encouraged, Hitler began pressing German claims on the Sudetenland, an area of Czechoslovakia with a predominantly ethnic German population.
Soon the United Kingdom and France followed the counsel of British Prime Minister Neville Chamberlain and conceded this territory to Germany in the Munich Agreement, which was made against the wishes of the Czechoslovak government, in exchange for a promise of no further territorial demands.
[55] Soon afterwards, Germany and Italy forced Czechoslovakia to cede additional territory to Hungary, and Poland annexed Czechoslovakia's Zaolzie region.
Although all of Germany's stated demands had been satisfied by the agreement, privately Hitler was furious that British interference had prevented him from seizing all of Czechoslovakia in one operation.
In subsequent speeches Hitler attacked British and Jewish "war-mongers" and in January 1939 secretly ordered a major build-up of the German navy to challenge British naval supremacy.
In March 1939, Germany invaded the remainder of Czechoslovakia and subsequently split it into the German Protectorate of Bohemia and Moravia and a pro-German client state, the Slovak Republic.
[57] Hitler also delivered the 20 March 1939 ultimatum to Lithuania, forcing the concession of the Klaipėda Region.
Greatly alarmed and with Hitler making further demands on the Free City of Danzig, the United Kingdom and France guaranteed their support for Polish independence; when Italy conquered Albania in April 1939, the same guarantee was extended to Romania and Greece.
[59] Shortly after the Franco-British pledge to Poland, Germany and Italy formalised their own alliance with the Pact of Steel.
[60] Hitler accused the United Kingdom and Poland of trying to "encircle" Germany and renounced the Anglo-German Naval Agreement and the German–Polish Non-Aggression Pact.
The situation reached a general crisis in late August as German troops continued to mobilise against the Polish border.
In August 23, when tripartite negotiations about a military alliance between France, the United Kingdom and Soviet Union stalled,[62] the Soviet Union signed a non-aggression pact with Germany.
[63] This pact had a secret protocol that defined German and Soviet "spheres of influence" (western Poland and Lithuania for Germany; eastern Poland, Finland, Estonia, Latvia and Bessarabia for the Soviet Union), and raised the question of continuing Polish independence.
[64] The pact neutralized the possibility of Soviet opposition to a campaign against Poland and assured that Germany would not have to face the prospect of a two-front war, as it had in World War I.
Immediately after that, Hitler ordered the attack to proceed on 26 August, but upon hearing that the United Kingdom had concluded a formal mutual assistance pact with Poland, and that Italy would maintain neutrality, he decided to delay it.
In response to British requests for direct negotiations to avoid war, Germany made demands on Poland, which only served as a pretext to worsen relations.
[66] On 29 August, Hitler demanded that a Polish plenipotentiary immediately travel to Berlin to negotiate the handover of Danzig, and to allow a plebiscite in the Polish Corridor in which the German minority would vote on secession.
[66] The Poles refused to comply with the German demands, and on the night of 30–31 August in a stormy meeting with the British ambassador Neville Henderson, Ribbentrop declared that Germany considered its claims rejected.
On 1 September 1939, Germany invaded Poland after having staged several false flag border incidents as a pretext to initiate the attack.
[68] The Battle of Westerplatte is often described as the first battle of the war.
The United Kingdom responded with an ultimatum to Germany to cease military operations, and on 3 September, after the ultimatum was ignored, France, the United Kingdom, Australia, and New Zealand declared war on Germany.
This alliance was joined by South Africa (6 September) and Canada (10 September).
The alliance provided no direct military support to Poland, outside of a cautious French probe into the Saarland.
[69] The Western Allies also began a naval blockade of Germany, which aimed to damage the country's economy and war effort.
[70] Germany responded by ordering U-boat warfare against Allied merchant and warships, which would later escalate into the Battle of the Atlantic.
On 8 September, German troops reached the suburbs of Warsaw.
The Polish counter offensive to the west halted the German advance for several days, but it was outflanked and encircled by the Wehrmacht.
Remnants of the Polish army broke through to besieged Warsaw.
On 17 September 1939, after signing a cease-fire with Japan, the Soviets invaded Eastern Poland[72] under a pretext that the Polish state had ostensibly ceased to exist.
[73] On 27 September, the Warsaw garrison surrendered to the Germans, and the last large operational unit of the Polish Army surrendered on 6 October.
Despite the military defeat, the Polish government never surrendered.
[74] A significant part of Polish military personnel evacuated to Romania and the Baltic countries; many of them would fight against the Axis in other theatres of the war.
[75] The Polish government in exile also established an Underground State and a resistance movement; in particular the Polish partisan Home Army would grow to become one of the war's largest resistance movements.
Germany annexed the western and occupied the central part of Poland, and the Soviet Union annexed its eastern part; small shares of Polish territory were transferred to Lithuania and Slovakia.
On 6 October, Hitler made a public peace overture to the United Kingdom and France, but said that the future of Poland was to be determined exclusively by Germany and the Soviet Union.
The proposal was rejected,[67] and Hitler ordered an immediate offensive against France,[76] which would be postponed until the spring of 1940 due to bad weather.
The Soviet Union forced the Baltic countries—Estonia, Latvia and Lithuania, the states that were in the Soviet "sphere of influence" under the Molotov-Ribbentrop pact—to sign "mutual assistance pacts" that stipulated stationing Soviet troops in these countries.
Soon after, significant Soviet military contingents were moved there.
[80][81][82] Finland refused to sign a similar pact and rejected ceding part of its territory to the Soviet Union.
The Soviet Union invaded Finland in November 1939,[83] and the Soviet Union was expelled from the League of Nations.
[84] Despite overwhelming numerical superiority, Soviet military success was modest, and the Finno-Soviet war ended in March 1940 with minimal Finnish concessions.
In June 1940, the Soviet Union forcibly annexed Estonia, Latvia and Lithuania,[81] and the disputed Romanian regions of Bessarabia, Northern Bukovina and Hertza.
Meanwhile, Nazi-Soviet political rapprochement and economic co-operation[86][87] gradually stalled,[88][89] and both states began preparations for war.
In April 1940, Germany invaded Denmark and Norway to protect shipments of iron ore from Sweden, which the Allies were attempting to cut off.
[91] Denmark capitulated after a few hours, and Norway was conquered within two months[92] despite Allied support.
British discontent over the Norwegian campaign led to the appointment of Winston Churchill as Prime Minister on 10 May 1940.
On the same day, Germany launched an offensive against France.
To circumvent the strong Maginot Line fortifications on the Franco-German border, Germany directed its attack at the neutral nations of Belgium, the Netherlands, and Luxembourg.
[94] The Germans carried out a flanking manoeuvre through the Ardennes region,[95] which was mistakenly perceived by Allies as an impenetrable natural barrier against armoured vehicles.
[96][97] By successfully implementing new blitzkrieg tactics, the Wehrmacht rapidly advanced to the Channel and cut off the Allied forces in Belgium, trapping the bulk of the Allied armies in a cauldron on the Franco-Belgian border near Lille.
The United Kingdom was able to evacuate a significant number of Allied troops from the continent by early June, although abandoning almost all of their equipment.
On 10 June, Italy invaded France, declaring war on both France and the United Kingdom.
[99] The Germans turned south against the weakened French army, and Paris fell to them on 14 June.
Eight days later France signed an armistice with Germany; it was divided into German and Italian occupation zones,[100] and an unoccupied rump state under the Vichy Regime, which, though officially neutral, was generally aligned with Germany.
France kept its fleet, which the United Kingdom attacked on 3 July in an attempt to prevent its seizure by Germany.
The Battle of Britain[102] began in early July with Luftwaffe attacks on shipping and harbours.
[103] The United Kingdom rejected Hitler's ultimatum,[104] and the German air superiority campaign started in August but failed to defeat RAF Fighter Command.
Due to this the proposed German invasion of Britain was postponed indefinitely on 17 September.
The German strategic bombing offensive intensified with night attacks on London and other cities in the Blitz, but failed to significantly disrupt the British war effort[103] and largely ended in May 1941.
Using newly captured French ports, the German Navy enjoyed success against an over-extended Royal Navy, using U-boats against British shipping in the Atlantic.
[106] The British Home Fleet scored a significant victory on 27 May 1941 by sinking the German battleship Bismarck.
In November 1939, the United States was taking measures to assist China and the Western Allies, and amended the Neutrality Act to allow "cash and carry" purchases by the Allies.
[108] In 1940, following the German capture of Paris, the size of the United States Navy was significantly increased.
In September the United States further agreed to a trade of American destroyers for British bases.
[109] Still, a large majority of the American public continued to oppose any direct military intervention in the conflict well into 1941.
[110] In December 1940 Roosevelt accused Hitler of planning world conquest and ruled out any negotiations as useless, calling for the United States to become an "arsenal of democracy" and promoting Lend-Lease programmes of aid to support the British war effort.
[104] The United States started strategic planning to prepare for a full-scale offensive against Germany.
At the end of September 1940, the Tripartite Pact formally united Japan, Italy and Germany as the Axis Powers.
The Tripartite Pact stipulated that any country, with the exception of the Soviet Union, which attacked any Axis Power would be forced to go to war against all three.
[112] The Axis expanded in November 1940 when Hungary, Slovakia and Romania joined.
[113] Romania and Hungary would make major contributions to the Axis war against the Soviet Union, in Romania's case partially to recapture territory ceded to the Soviet Union.
In early June 1940 the Italian Regia aeronautica attacked and besieged Malta, a British possession.
In late summer through early autumn Italy conquered British Somaliland and made an incursion into British-held Egypt.
In October Italy attacked Greece, but the attack was repulsed with heavy Italian casualties; the campaign ended within days with minor territorial changes.
[115] Germany started preparation for an invasion of the Balkans to assist Italy, to prevent the British from gaining a foothold there, which would be a potential threat for Romanian oil fields, and to strike against the British dominance of the Mediterranean.
In December 1940, British Empire forces began counter-offensives against Italian forces in Egypt and Italian East Africa.
[117] The offensives were highly successful; by early February 1941 Italy had lost control of eastern Libya, and large numbers of Italian troops had been taken prisoner.
The Italian Navy also suffered significant defeats, with the Royal Navy putting three Italian battleships out of commission by a carrier attack at Taranto and neutralising several more warships at the Battle of Cape Matapan.
Italian defeats prompted Germany to deploy an expeditionary force to North Africa, and at the end of March 1941 Rommel's Afrika Korps launched an offensive which drove back the Commonwealth forces.
[119] In under a month, Axis forces advanced to western Egypt and besieged the port of Tobruk.
By late March 1941 Bulgaria and Yugoslavia signed the Tripartite Pact.
However, the Yugoslav government was overthrown two days later by pro-British nationalists.
Germany responded with simultaneous invasions of both Yugoslavia and Greece, commencing on 6 April 1941; both nations were forced to surrender within the month.
[121] The airborne invasion of the Greek island of Crete at the end of May completed the German conquest of the Balkans.
[122] Although the Axis victory was swift, bitter and large-scale partisan warfare subsequently broke out against the Axis occupation of Yugoslavia, which continued until the end of the war.
In the Middle East, in May Commonwealth forces quashed an uprising in Iraq which had been supported by German aircraft from bases within Vichy-controlled Syria.
[124] Between June and July they invaded and occupied the French possessions Syria and Lebanon, with the assistance of the Free French.
[citation needed]
With the situation in Europe and Asia relatively stable, Germany, Japan, and the Soviet Union made preparations.
With the Soviets wary of mounting tensions with Germany and the Japanese planning to take advantage of the European War by seizing resource-rich European possessions in Southeast Asia, the two powers signed the Soviet–Japanese Neutrality Pact in April 1941.
[125] By contrast, the Germans were steadily making preparations for an attack on the Soviet Union, massing forces on the Soviet border.
Hitler believed that the United Kingdom's refusal to end the war was based on the hope that the United States and the Soviet Union would enter the war against Germany sooner or later.
[127] He therefore decided to try to strengthen Germany's relations with the Soviets, or failing that to attack and eliminate them as a factor.
In November 1940, negotiations took place to determine if the Soviet Union would join the Tripartite Pact.
The Soviets showed some interest, but asked for concessions from Finland, Bulgaria, Turkey, and Japan that Germany considered unacceptable.
On 18 December 1940, Hitler issued the directive to prepare for an invasion of the Soviet Union.
On 22 June 1941, Germany, supported by Italy and Romania, invaded the Soviet Union in Operation Barbarossa, with Germany accusing the Soviets of plotting against them.
They were joined shortly by Finland and Hungary.
[129] The primary targets of this surprise offensive[130] were the Baltic region, Moscow and Ukraine, with the ultimate goal of ending the 1941 campaign near the Arkhangelsk-Astrakhan line, from the Caspian to the White Seas.
Hitler's objectives were to eliminate the Soviet Union as a military power, exterminate Communism, generate Lebensraum ("living space")[131] by dispossessing the native population[132] and guarantee access to the strategic resources needed to defeat Germany's remaining rivals.
Although the Red Army was preparing for strategic counter-offensives before the war,[134] Barbarossa forced the Soviet supreme command to adopt a strategic defence.
During the summer, the Axis made significant gains into Soviet territory, inflicting immense losses in both personnel and materiel.
By the middle of August, however, the German Army High Command decided to suspend the offensive of a considerably depleted Army Group Centre, and to divert the 2nd Panzer Group to reinforce troops advancing towards central Ukraine and Leningrad.
[135] The Kiev offensive was overwhelmingly successful, resulting in encirclement and elimination of four Soviet armies, and made possible further advance into Crimea and industrially developed Eastern Ukraine (the First Battle of Kharkov).
The diversion of three quarters of the Axis troops and the majority of their air forces from France and the central Mediterranean to the Eastern Front[137] prompted the United Kingdom to reconsider its grand strategy.
[138] In July, the UK and the Soviet Union formed a military alliance against Germany[139] The British and Soviets invaded neutral Iran to secure the Persian Corridor and Iran's oil fields.
[140] In August, the United Kingdom and the United States jointly issued the Atlantic Charter.
By October Axis operational objectives in Ukraine and the Baltic region were achieved, with only the sieges of Leningrad[142] and Sevastopol continuing.
[143] A major offensive against Moscow was renewed; after two months of fierce battles in increasingly harsh weather the German army almost reached the outer suburbs of Moscow, where the exhausted troops[144] were forced to suspend their offensive.
[145] Large territorial gains were made by Axis forces, but their campaign had failed to achieve its main objectives: two key cities remained in Soviet hands, the Soviet capability to resist was not broken, and the Soviet Union retained a considerable part of its military potential.
The blitzkrieg phase of the war in Europe had ended.
By early December, freshly mobilised reserves[147] allowed the Soviets to achieve numerical parity with Axis troops.
[148] This, as well as intelligence data which established that a minimal number of Soviet troops in the East would be sufficient to deter any attack by the Japanese Kwantung Army,[149] allowed the Soviets to begin a massive counter-offensive that started on 5 December all along the front and pushed German troops 100–250 kilometres (62–155 mi) west.
In 1939, the United States had renounced its trade treaty with Japan, and beginning with an aviation gasoline ban in July 1940, Japan became subject to increasing economic pressure.
[104] During this time, Japan launched its first attack against Changsha, a strategically important Chinese city, but was repulsed by late September.
[151] Despite several offensives by both sides, the war between China and Japan was stalemated by 1940.
To increase pressure on China by blocking supply routes, and to better position Japanese forces in the event of a war with the Western powers, Japan invaded and occupied northern Indochina.
[152] Afterwards, the United States embargoed iron, steel and mechanical parts against Japan.
Chinese nationalist forces launched a large-scale counter-offensive in early 1940.
In August, Chinese communists launched an offensive in Central China; in retaliation, Japan instituted harsh measures in occupied areas to reduce human and material resources for the communists.
[154] Continued antipathy between Chinese communist and nationalist forces culminated in armed clashes in January 1941, effectively ending their co-operation.
[155] In March, the Japanese 11th army attacked the headquarters of the Chinese 19th army but was repulsed during Battle of Shanggao.
[156] In September, Japan attempted to take the city of Changsha again and clashed with Chinese nationalist forces.
German successes in Europe encouraged Japan to increase pressure on European governments in Southeast Asia.
The Dutch government agreed to provide Japan some oil supplies from the Dutch East Indies, but negotiations for additional access to their resources ended in failure in June 1941.
[158] In July 1941 Japan sent troops to southern Indochina, thus threatening British and Dutch possessions in the Far East.
The United States, United Kingdom and other Western governments reacted to this move with a freeze on Japanese assets and a total oil embargo.
[159][160] At the same time, Japan was planning an invasion of the Soviet Far East, intending to capitalise off the German invasion in the west, but abandoned the operation after the sanctions.
Since early 1941 the United States and Japan had been engaged in negotiations in an attempt to improve their strained relations and end the war in China.
During these negotiations Japan advanced a number of proposals which were dismissed by the Americans as inadequate.
[162] At the same time the United States, the United Kingdom, and the Netherlands engaged in secret discussions for the joint defence of their territories, in the event of a Japanese attack against any of them.
[163] Roosevelt reinforced the Philippines (an American protectorate scheduled for independence in 1946) and warned Japan that the United States would react to Japanese attacks against any "neighboring countries".
Frustrated at the lack of progress and feeling the pinch of the American-British-Dutch sanctions, Japan prepared for war.
On 20 November a new government under Hideki Tojo presented an interim proposal as its final offer.
It called for the end of American aid to China and for lifting the embargo on the supply of oil and other resources to Japan.
In exchange Japan promised not to launch any attacks in Southeast Asia and to withdraw its forces from southern Indochina.
[162] The American counter-proposal of 26 November required that Japan evacuate all of China without conditions and conclude non-aggression pacts with all Pacific powers.
[164] That meant Japan was essentially forced to choose between abandoning its ambitions in China, or seizing the natural resources it needed in the Dutch East Indies by force;[165][166] the Japanese military did not consider the former an option, and many officers considered the oil embargo an unspoken declaration of war.
Japan planned to rapidly seize European colonies in Asia to create a large defensive perimeter stretching into the Central Pacific.
The Japanese would then be free to exploit the resources of Southeast Asia while exhausting the over-stretched Allies by fighting a defensive war.
[168][169] To prevent American intervention while securing the perimeter, it was further planned to neutralise the United States Pacific Fleet and the American military presence in the Philippines from the outset.
[170] On 7 December 1941 (8 December in Asian time zones), Japan attacked British and American holdings with near-simultaneous offensives against Southeast Asia and the Central Pacific.
[171] These included an attack on the American fleets at Pearl Harbor and the Philippines, landings in Thailand and Malaya,[171] and the Battle of Hong Kong.
[citation needed]
These attacks led the United States, United Kingdom, China, Australia and several other states to formally declare war on Japan, whereas the Soviet Union, being heavily involved in large-scale hostilities with European Axis countries, maintained its neutrality agreement with Japan.
[172] Germany, followed by the other Axis states, declared war on the United States[173] in solidarity with Japan, citing as justification the American attacks on German war vessels that had been ordered by Roosevelt.
On 1 January 1942, the Allied Big Four[175]—the Soviet Union, China, the United Kingdom and the United States—and 22 smaller or exiled governments issued the Declaration by United Nations, thereby affirming the Atlantic Charter,[176] and agreeing to not to sign a separate peace with the Axis powers.
During 1942, Allied officials debated on the appropriate grand strategy to pursue.
All agreed that defeating Germany was the primary objective.
The Americans favoured a straightforward, large-scale attack on Germany through France.
The Soviets were also demanding a second front.
The British, on the other hand, argued that military operations should target peripheral areas to wear out German strength, leading to increasing demoralisation, and bolster resistance forces.
Germany itself would be subject to a heavy bombing campaign.
An offensive against Germany would then be launched primarily by Allied armour without using large-scale armies.
[178] Eventually, the British persuaded the Americans that a landing in France was infeasible in 1942 and they should instead focus on driving the Axis out of North Africa.
At the Casablanca Conference in early 1943, the Allies reiterated the statements issued in the 1942 Declaration, and demanded the unconditional surrender of their enemies.
The British and Americans agreed to continue to press the initiative in the Mediterranean by invading Sicily to fully secure the Mediterranean supply routes.
[180] Although the British argued for further operations in the Balkans to bring Turkey into the war, in May 1943, the Americans extracted a British commitment to limit Allied operations in the Mediterranean to an invasion of the Italian mainland and to invade France in 1944.
By the end of April 1942, Japan and its ally Thailand had almost fully conquered Burma, Malaya, the Dutch East Indies, Singapore, and Rabaul, inflicting severe losses on Allied troops and taking a large number of prisoners.
[182] Despite stubborn resistance by Filipino and US forces, the Philippine Commonwealth was eventually captured in May 1942, forcing its government into exile.
[183] On 16 April, in Burma, 7,000 British soldiers were encircled by the Japanese 33rd Division during the Battle of Yenangyaung and rescued by the Chinese 38th Division.
[184] Japanese forces also achieved naval victories in the South China Sea, Java Sea and Indian Ocean,[185] and bombed the Allied naval base at Darwin, Australia.
In January 1942, the only Allied success against Japan was a Chinese victory at Changsha.
[186] These easy victories over unprepared US and European opponents left Japan overconfident, as well as overextended.
In early May 1942, Japan initiated operations to capture Port Moresby by amphibious assault and thus sever communications and supply lines between the United States and Australia.
The planned invasion was thwarted when an Allied task force, centred on two American fleet carriers, fought Japanese naval forces to a draw in the Battle of the Coral Sea.
[188] Japan's next plan, motivated by the earlier Doolittle Raid, was to seize Midway Atoll and lure American carriers into battle to be eliminated; as a diversion, Japan would also send forces to occupy the Aleutian Islands in Alaska.
[189] In mid-May, Japan started the Zhejiang-Jiangxi Campaign in China, with the goal of inflicting retribution on the Chinese who aided the surviving American airmen in the Doolittle Raid by destroying air bases and fighting against the Chinese 23rd and 32nd Army Groups.
[190][191] In early June, Japan put its operations into action, but the Americans, having broken Japanese naval codes in late May, were fully aware of the plans and order of battle, and used this knowledge to achieve a decisive victory at Midway over the Imperial Japanese Navy.
With its capacity for aggressive action greatly diminished as a result of the Midway battle, Japan chose to focus on a belated attempt to capture Port Moresby by an overland campaign in the Territory of Papua.
[193] The Americans planned a counter-attack against Japanese positions in the southern Solomon Islands, primarily Guadalcanal, as a first step towards capturing Rabaul, the main Japanese base in Southeast Asia.
Both plans started in July, but by mid-September, the Battle for Guadalcanal took priority for the Japanese, and troops in New Guinea were ordered to withdraw from the Port Moresby area to the northern part of the island, where they faced Australian and United States troops in the Battle of Buna-Gona.
[195] Guadalcanal soon became a focal point for both sides with heavy commitments of troops and ships in the battle for Guadalcanal.
By the start of 1943, the Japanese were defeated on the island and withdrew their troops.
[196] In Burma, Commonwealth forces mounted two operations.
The first, an offensive into the Arakan region in late 1942, went disastrously, forcing a retreat back to India by May 1943.
[197] The second was the insertion of irregular forces behind Japanese front-lines in February which, by the end of April, had achieved mixed results.
Despite considerable losses, in early 1942 Germany and its allies stopped a major Soviet offensive in central and southern Russia, keeping most territorial gains they had achieved during the previous year.
[199] In May the Germans defeated Soviet offensives in the Kerch Peninsula and at Kharkov,[200] and then launched their main summer offensive against southern Russia in June 1942, to seize the oil fields of the Caucasus and occupy Kuban steppe, while maintaining positions on the northern and central areas of the front.
The Germans split Army Group South into two groups: Army Group A advanced to the lower Don River and struck south-east to the Caucasus, while Army Group B headed towards the Volga River.
The Soviets decided to make their stand at Stalingrad on the Volga.
By mid-November, the Germans had nearly taken Stalingrad in bitter street fighting.
The Soviets began their second winter counter-offensive, starting with an encirclement of German forces at Stalingrad,[202] and an assault on the Rzhev salient near Moscow, though the latter failed disastrously.
[203] By early February 1943, the German Army had taken tremendous losses; German troops at Stalingrad had been forced to surrender,[204] and the front-line had been pushed back beyond its position before the summer offensive.
In mid-February, after the Soviet push had tapered off, the Germans launched another attack on Kharkov, creating a salient in their front line around the Soviet city of Kursk.
Exploiting poor American naval command decisions, the German navy ravaged Allied shipping off the American Atlantic coast.
[206] By November 1941, Commonwealth forces had launched a counter-offensive, Operation Crusader, in North Africa, and reclaimed all the gains the Germans and Italians had made.
[207] In North Africa, the Germans launched an offensive in January, pushing the British back to positions at the Gazala Line by early February,[208] followed by a temporary lull in combat which Germany used to prepare for their upcoming offensives.
[209] Concerns the Japanese might use bases in Vichy-held Madagascar caused the British to invade the island in early May 1942.
[210] An Axis offensive in Libya forced an Allied retreat deep inside Egypt until Axis forces were stopped at El Alamein.
[211] On the Continent, raids of Allied commandos on strategic targets, culminating in the disastrous Dieppe Raid,[212] demonstrated the Western Allies' inability to launch an invasion of continental Europe without much better preparation, equipment, and operational security.
[213][page needed]
In August 1942, the Allies succeeded in repelling a second attack against El Alamein[214] and, at a high cost, managed to deliver desperately needed supplies to the besieged Malta.
[215] A few months later, the Allies commenced an attack of their own in Egypt, dislodging the Axis forces and beginning a drive west across Libya.
[216] This attack was followed up shortly after by Anglo-American landings in French North Africa, which resulted in the region joining the Allies.
[217] Hitler responded to the French colony's defection by ordering the occupation of Vichy France;[217] although Vichy forces did not resist this violation of the armistice, they managed to scuttle their fleet to prevent its capture by German forces.
[217][218] The Axis forces in Africa withdrew into Tunisia, which was conquered by the Allies in May 1943.
In June 1943 the British and Americans began a strategic bombing campaign against Germany with a goal to disrupt the war economy, reduce morale, and "de-house" the civilian population.
[220] The firebombing of Hamburg was among the first attacks in this campaign, inflicting significant casualties and considerable losses on infrastructure of this important industrial centre.
After the Guadalcanal Campaign, the Allies initiated several operations against Japan in the Pacific.
In May 1943, Canadian and US forces were sent to eliminate Japanese forces from the Aleutians.
[222] Soon after, the United States, with support from Australian and New Zealand forces, began major operations to isolate Rabaul by capturing surrounding islands, and breach the Japanese Central Pacific perimeter at the Gilbert and Marshall Islands.
[223] By the end of March 1944, the Allies had completed both of these objectives, and had also neutralised the major Japanese base at Truk in the Caroline Islands.
In April, the Allies launched an operation to retake Western New Guinea.
In the Soviet Union, both the Germans and the Soviets spent the spring and early summer of 1943 preparing for large offensives in central Russia.
On 4 July 1943, Germany attacked Soviet forces around the Kursk Bulge.
Within a week, German forces had exhausted themselves against the Soviets' deeply echeloned and well-constructed defences,[225] and for the first time in the war Hitler cancelled the operation before it had achieved tactical or operational success.
[226] This decision was partially affected by the Western Allies' invasion of Sicily launched on 9 July, which, combined with previous Italian failures, resulted in the ousting and arrest of Mussolini later that month.
On 12 July 1943, the Soviets launched their own counter-offensives, thereby dispelling any chance of German victory or even stalemate in the east.
The Soviet victory at Kursk marked the end of German superiority,[228] giving the Soviet Union the initiative on the Eastern Front.
[229][230] The Germans tried to stabilise their eastern front along the hastily fortified Panther–Wotan line, but the Soviets broke through it at Smolensk and by the Lower Dnieper Offensives.
On 3 September 1943, the Western Allies invaded the Italian mainland, following Italy's armistice with the Allies.
[232] Germany with the help of fascists responded by disarming Italian forces that were in many places without superior orders, seizing military control of Italian areas,[233] and creating a series of defensive lines.
[234] German special forces then rescued Mussolini, who then soon established a new client state in German-occupied Italy named the Italian Social Republic,[235] causing an Italian civil war.
The Western Allies fought through several lines until reaching the main German defensive line in mid-November.
German operations in the Atlantic also suffered.
By May 1943, as Allied counter-measures became increasingly effective, the resulting sizeable German submarine losses forced a temporary halt of the German Atlantic naval campaign.
[237] In November 1943, Franklin D. Roosevelt and Winston Churchill met with Chiang Kai-shek in Cairo and then with Joseph Stalin in Tehran.
[238] The former conference determined the post-war return of Japanese territory[239] and the military planning for the Burma Campaign,[240] while the latter included agreement that the Western Allies would invade Europe in 1944 and that the Soviet Union would declare war on Japan within three months of Germany's defeat.
From November 1943, during the seven-week Battle of Changde, the Chinese forced Japan to fight a costly war of attrition, while awaiting Allied relief.
[242][243][244] In January 1944, the Allies launched a series of attacks in Italy against the line at Monte Cassino and tried to outflank it with landings at Anzio.
On 27 January 1944, Soviet troops launched a major offensive that expelled German forces from the Leningrad region, thereby ending the longest and most lethal siege in history.
[246] The following Soviet offensive was halted on the pre-war Estonian border by the German Army Group North aided by Estonians hoping to re-establish national independence.
This delay slowed subsequent Soviet operations in the Baltic Sea region.
[247] By late May 1944, the Soviets had liberated Crimea, largely expelled Axis forces from Ukraine, and made incursions into Romania, which were repulsed by the Axis troops.
[248] The Allied offensives in Italy had succeeded and, at the expense of allowing several German divisions to retreat, on 4 June, Rome was captured.
The Allies had mixed success in mainland Asia.
In March 1944, the Japanese launched the first of two invasions, an operation against British positions in Assam, India,[250] and soon besieged Commonwealth positions at Imphal and Kohima.
[251] In May 1944, British forces mounted a counter-offensive that drove Japanese troops back to Burma by July,[251] and Chinese forces that had invaded northern Burma in late 1943 besieged Japanese troops in Myitkyina.
[252] The second Japanese invasion of China aimed to destroy China's main fighting forces, secure railways between Japanese-held territory and capture Allied airfields.
[253] By June, the Japanese had conquered the province of Henan and begun a new attack on Changsha in Hunan province.
On 6 June 1944 (known as D-Day), after three years of Soviet pressure,[255] the Western Allies invaded northern France.
After reassigning several Allied divisions from Italy, they also attacked southern France.
[256] These landings were successful, and led to the defeat of the German Army units in France.
Paris was liberated on 25 August by the local resistance assisted by the Free French Forces, both led by General Charles de Gaulle,[257] and the Western Allies continued to push back German forces in western Europe during the latter part of the year.
An attempt to advance into northern Germany spearheaded by a major airborne operation in the Netherlands failed.
[258] After that, the Western Allies slowly pushed into Germany, but failed to cross the Ruhr river in a large offensive.
In Italy, Allied advance also slowed due to the last major German defensive line.
On 22 June, the Soviets launched a strategic offensive in Belarus ("Operation Bagration") that destroyed the German Army Group Centre almost completely.
[260] Soon after that, another Soviet strategic offensive forced German troops from Western Ukraine and Eastern Poland.
The Soviet advance prompted resistance forces in Poland to initiate several uprisings against the German occupation.
However, the largest of these, a citywide uprising in Warsaw and a national uprising in Slovakia, did not receive Soviet support and were subsequently suppressed by the Germans.
[261] The Soviet Red Army's strategic offensive in eastern Romania cut off and destroyed the considerable German troops there and triggered a successful coup d'état in Romania and in Bulgaria, followed by those countries' shift to the Allied side.
In September 1944, Soviet troops advanced into Yugoslavia and forced the rapid withdrawal of German Army Groups E and F in Greece, Albania and Yugoslavia to rescue them from being cut off.
[263] By this point, the Communist-led Partisans under Marshal Josip Broz Tito, who had led an increasingly successful guerrilla campaign against the occupation since 1941, controlled much of the territory of Yugoslavia and engaged in delaying efforts against German forces further south.
In northern Serbia, the Soviet Red Army, with limited support from Bulgarian forces, assisted the Partisans in a joint liberation of the capital city of Belgrade on 20 October.
A few days later, the Soviets launched a massive assault against German-occupied Hungary that lasted until the fall of Budapest in February 1945.
[264] Unlike impressive Soviet victories in the Balkans, bitter Finnish resistance to the Soviet offensive in the Karelian Isthmus denied the Soviets occupation of Finland and led to a Soviet-Finnish armistice on relatively mild conditions,[265] although Finland was forced to fight their former ally Germany.
By the start of July 1944, Commonwealth forces in Southeast Asia had repelled the Japanese sieges in Assam, pushing the Japanese back to the Chindwin River[267] while the Chinese captured Myitkyina.
In September 1944, Chinese forces captured Mount Song and reopened the Burma Road.
[268] In China, the Japanese had more successes, having finally captured Changsha in mid-June and the city of Hengyang by early August.
[269] Soon after, they invaded the province of Guangxi, winning major engagements against Chinese forces at Guilin and Liuzhou by the end of November[270] and successfully linking up their forces in China and Indochina by mid-December.
In the Pacific, US forces continued to press back the Japanese perimeter.
In mid-June 1944, they began their offensive against the Mariana and Palau islands, and decisively defeated Japanese forces in the Battle of the Philippine Sea.
These defeats led to the resignation of the Japanese Prime Minister, Hideki Tojo, and provided the United States with air bases to launch intensive heavy bomber attacks on the Japanese home islands.
In late October, American forces invaded the Filipino island of Leyte; soon after, Allied naval forces scored another large victory in the Battle of Leyte Gulf, one of the largest naval battles in history.
On 16 December 1944, Germany made a last attempt on the Western Front by using most of its remaining reserves to launch a massive counter-offensive in the Ardennes and along the French–German border to split the Western Allies, encircle large portions of Western Allied troops and capture their primary supply port at Antwerp to prompt a political settlement.
[273] By January, the offensive had been repulsed with no strategic objectives fulfilled.
[273] In Italy, the Western Allies remained stalemated at the German defensive line.
In mid-January 1945, the Soviets and Poles attacked in Poland, pushing from the Vistula to the Oder river in Germany, and overran East Prussia.
[274] On 4 February, Soviet, British and US leaders met for the Yalta Conference.
They agreed on the occupation of post-war Germany, and on when the Soviet Union would join the war against Japan.
In February, the Soviets entered Silesia and Pomerania, while Western Allies entered western Germany and closed to the Rhine river.
By March, the Western Allies crossed the Rhine north and south of the Ruhr, encircling the German Army Group B.
[276] In early March, in an attempt to protect its last oil reserves in Hungary and to retake Budapest, Germany launched its last major offensive against Soviet troops near Lake Balaton.
In two weeks, the offensive had been repulsed, the Soviets advanced to Vienna, and captured the city.
In early April, Soviet troops captured Königsberg, while the Western Allies finally pushed forward in Italy and swept across western Germany capturing Hamburg and Nuremberg.
American and Soviet forces met at the Elbe river on 25 April, leaving several unoccupied pockets in southern Germany and around Berlin.
Soviet and Polish forces stormed and captured Berlin in late April.
In Italy, German forces surrendered on 29 April.
On 30 April, the Reichstag was captured, signalling the military defeat of Nazi Germany,[277] Berlin garrison surrendered on 2 May.
Several changes in leadership occurred during this period.
On 12 April, President Roosevelt died and was succeeded by Harry S. Truman.
Benito Mussolini was killed by Italian partisans on 28 April.
[278] Two days later, Hitler committed suicide in besieged Berlin, and he was succeeded by Grand Admiral Karl Dönitz.
Total and unconditional surrender was signed on 7 and 8 May, to be effective by the end of 8 May.
[280] German Army Group Centre resisted in Prague until 11 May.
In the Pacific theatre, American forces accompanied by the forces of the Philippine Commonwealth advanced in the Philippines, clearing Leyte by the end of April 1945.
They landed on Luzon in January 1945 and recaptured Manila in March.
Fighting continued on Luzon, Mindanao, and other islands of the Philippines until the end of the war.
[282] Meanwhile, the United States Army Air Forces launched a massive firebombing campaign of strategic cities in Japan in an effort to destroy Japanese war industry and civilian morale.
A devastating bombing raid on Tokyo of 9–10 March was the deadliest conventional bombing raid in history.
In May 1945, Australian troops landed in Borneo, over-running the oilfields there.
British, American, and Chinese forces defeated the Japanese in northern Burma in March, and the British pushed on to reach Rangoon by 3 May.
[284] Chinese forces started a counterattack in the Battle of West Hunan that occurred between 6 April and 7 June 1945.
American naval and amphibious forces also moved towards Japan, taking Iwo Jima by March, and Okinawa by the end of June.
[285] At the same time, American submarines cut off Japanese imports, drastically reducing Japan's ability to supply its overseas forces.
On 11 July, Allied leaders met in Potsdam, Germany.
They confirmed earlier agreements about Germany,[287] and the American, British and Chinese governments reiterated the demand for unconditional surrender of Japan, specifically stating that "the alternative for Japan is prompt and utter destruction".
[288] During this conference, the United Kingdom held its general election, and Clement Attlee replaced Churchill as Prime Minister.
The call for unconditional surrender was rejected by the Japanese government, which believed it would be capable of negotiating for more favourable surrender terms.
[290] In early August, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki.
Between the two bombings, the Soviets, pursuant to the Yalta agreement, invaded Japanese-held Manchuria and quickly defeated the Kwantung Army, which was the largest Japanese fighting force,[291] thereby persuading previously adamant Imperial Army leaders to accept surrender terms.
[292] The Red Army also captured the southern part of Sakhalin Island and the Kuril Islands.
On 15 August 1945, Japan surrendered, with the surrender documents finally signed at Tokyo Bay on the deck of the American battleship USS Missouri on 2 September 1945, ending the war.
The Allies established occupation administrations in Austria and Germany.
The former became a neutral state, non-aligned with any political bloc.
The latter was divided into western and eastern occupation zones controlled by the Western Allies and the Soviet Union.
A denazification programme in Germany led to the prosecution of Nazi war criminals in the Nuremberg trials and the removal of ex-Nazis from power, although this policy moved towards amnesty and re-integration of ex-Nazis into West German society.
Germany lost a quarter of its pre-war (1937) territory.
Among the eastern territories, Silesia, Neumark and most of Pomerania were taken over by Poland,[295] and East Prussia was divided between Poland and the Soviet Union, followed by the expulsion to Germany of the nine million Germans from these provinces,[296][297] as well as three million Germans from the Sudetenland in Czechoslovakia.
By the 1950s, one-fifth of West Germans were refugees from the east.
The Soviet Union also took over the Polish provinces east of the Curzon line,[298] from which 2 million Poles were expelled;[297][299] north-east Romania,[300][301] parts of eastern Finland,[302] and the three Baltic states were incorporated into the Soviet Union.
In an effort to maintain world peace,[305] the Allies formed the United Nations, which officially came into existence on 24 October 1945,[306] and adopted the Universal Declaration of Human Rights in 1948 as a common standard for all member nations.
[307] The great powers that were the victors of the war—France, China, the United Kingdom, the Soviet Union and the United States—became the permanent members of the UN's Security Council.
[7] The five permanent members remain so to the present, although there have been two seat changes, between the Republic of China and the People's Republic of China in 1971, and between the Soviet Union and its successor state, the Russian Federation, following the dissolution of the Soviet Union in 1991.
The alliance between the Western Allies and the Soviet Union had begun to deteriorate even before the war was over.
Germany had been de facto divided, and two independent states, the Federal Republic of Germany and the German Democratic Republic,[309] were created within the borders of Allied and Soviet occupation zones.
The rest of Europe was also divided into Western and Soviet spheres of influence.
[310] Most eastern and central European countries fell into the Soviet sphere, which led to establishment of Communist-led regimes, with full or partial support of the Soviet occupation authorities.
As a result, East Germany,[311] Poland, Hungary, Romania, Czechoslovakia, and Albania[312] became Soviet satellite states.
Communist Yugoslavia conducted a fully independent policy, causing tension with the Soviet Union.
Post-war division of the world was formalised by two international military alliances, the United States-led NATO and the Soviet-led Warsaw Pact.
[314] The long period of political tensions and military competition between them, the Cold War, would be accompanied by an unprecedented arms race and proxy wars.
In Asia, the United States led the occupation of Japan and administrated Japan's former islands in the Western Pacific, while the Soviets annexed Sakhalin and the Kuril Islands.
[316] Korea, formerly under Japanese rule, was divided and occupied by the Soviet Union in the North and the United States in the South between 1945 and 1948.
Separate republics emerged on both sides of the 38th parallel in 1948, each claiming to be the legitimate government for all of Korea, which led ultimately to the Korean War.
In China, nationalist and communist forces resumed the civil war in June 1946.
Communist forces were victorious and established the People's Republic of China on the mainland, while nationalist forces retreated to Taiwan in 1949.
[318] In the Middle East, the Arab rejection of the United Nations Partition Plan for Palestine and the creation of Israel marked the escalation of the Arab–Israeli conflict.
While European powers attempted to retain some or all of their colonial empires, their losses of prestige and resources during the war rendered this unsuccessful, leading to decolonisation.
The global economy suffered heavily from the war, although participating nations were affected differently.
The United States emerged much richer than any other nation, leading to a baby boom, and by 1950 its gross domestic product per person was much higher than that of any of the other powers, and it dominated the world economy.
[321] The UK and US pursued a policy of industrial disarmament in Western Germany in the years 1945–1948.
[322] Because of international trade interdependencies this led to European economic stagnation and delayed European recovery for several years.
Recovery began with the mid-1948 currency reform in Western Germany, and was sped up by the liberalisation of European economic policy that the Marshall Plan (1948–1951) both directly and indirectly caused.
[325][326] The post-1948 West German recovery has been called the German economic miracle.
[327] Italy also experienced an economic boom[328] and the French economy rebounded.
[329] By contrast, the United Kingdom was in a state of economic ruin,[330] and although receiving a quarter of the total Marshall Plan assistance, more than any other European country,[331] it continued in relative economic decline for decades.
The Soviet Union, despite enormous human and material losses, also experienced rapid increase in production in the immediate post-war era.
[333] Japan experienced incredibly rapid economic growth, becoming one of the most powerful economies in the world by the 1980s.
[334] China returned to its pre-war industrial production by 1952.
Estimates for the total number of casualties in the war vary, because many deaths went unrecorded.
Most suggest that some 60 million people died in the war, including about 20 million military personnel and 40 million civilians.
Many of the civilians died because of deliberate genocide, massacres, mass-bombings, disease, and starvation.
[citation needed]
The Soviet Union lost around 27 million people during the war,[339] 8.7 million military and 19 million civilian deaths.
[340] A quarter of the people in the Soviet Union were wounded or killed.
[341] Germany sustained 5.3 million military losses, mostly on the Eastern Front and during the final battles in Germany.
Of the total number of deaths in World War II, approximately 85 per cent—mostly Soviet and Chinese—were on the Allied side.
[343] Many of these deaths were caused by war crimes committed by German and Japanese forces in occupied territories.
An estimated 11[344] to 17 million[345] civilians died as a direct or as an indirect result of Nazi racist policies, including mass killing of around 6 million Jews, along with Roma, homosexuals, at least 1.9 million ethnic Poles[346][347] and millions of other Slavs (including Russians, Ukrainians and Belarusians), and other ethnic and minority groups.
[348][345] Between 1941 and 1945, over 200,000 ethnic Serbs, along with gypsies and Jews, were persecuted and murdered by the Axis-aligned Croatian Ustaše in Yugoslavia.
[349] Also, over 100,000 Poles were massacred by the Ukrainian Insurgent Army in the Volhynia massacres, between 1943 and 1945.
[350] At the same time about 10,000–15,000 Ukrainians were killed by the Polish Home Army and other Polish units, in reprisal attacks.
In Asia and the Pacific, between 3 million and more than 10 million civilians, mostly Chinese (estimated at 7.5 million[352]), were killed by the Japanese occupation forces.
[353] The most infamous Japanese atrocity was the Nanking Massacre, in which fifty to three hundred thousand Chinese civilians were raped and murdered.
[354] Mitsuyoshi Himeta reported that 2.7 million casualties occurred during the Sankō Sakusen.
General Yasuji Okamura implemented the policy in Heipei and Shantung.
Axis forces employed biological and chemical weapons.
The Imperial Japanese Army used a variety of such weapons during its invasion and occupation of China (see Unit 731)[356][357] and in early conflicts against the Soviets.
[358] Both the Germans and Japanese tested such weapons against civilians,[359] and sometimes on prisoners of war.
The Soviet Union was responsible for the Katyn massacre of 22,000 Polish officers,[361] and the imprisonment or execution of thousands of political prisoners by the NKVD, along with mass civilian deportations to Siberia, in the Baltic states and eastern Poland annexed by the Red Army.
The mass-bombing of cities in Europe and Asia has often been called a war crime, although no positive or specific customary international humanitarian law with respect to aerial warfare existed before or during World War II.
[363] The USAAF firebombed a total of 67 Japanese cities, killing 393,000 civilians and destroying 65% of built-up areas.
The German government led by Adolf Hitler and the Nazi Party was responsible for the Holocaust (killing approximately 6 million Jews), as well as for killing 2.7 million ethnic Poles[365] and 4 million others who were deemed "unworthy of life" (including the disabled and mentally ill, Soviet prisoners of war, Romani, homosexuals, Freemasons, and Jehovah's Witnesses) as part of a programme of deliberate extermination.
Soviet POWs were kept in especially unbearable conditions, and, although their extermination was not an official goal, 3.6 million Soviet POWs out of 5.7 died in Nazi camps during the war.
[366][367] In addition to concentration camps, death camps were created in Nazi Germany to exterminate people at an industrial scale.
Nazi Germany extensively used forced labourers; about 12 million Europeans from German occupied countries were used as a slave work force in German agriculture and war economy.
The Soviet Gulag became a de facto system of deadly camps during 1942–43, when wartime privation and hunger caused numerous deaths of inmates,[369] including foreign citizens of Poland and other countries occupied in 1939–40 by the Soviet Union, as well as Axis POWs.
[370] By the end of the war, most Soviet POWs liberated from Nazi camps and many repatriated civilians were detained in special filtration camps where they were subjected to NKVD evaluation, and a significant part of them were sent to the Gulag as real or perceived Nazi collaborators.
Japanese prisoner-of-war camps, many of which were used as labour camps, also had high death rates.
The International Military Tribunal for the Far East found the death rate of Western prisoners was 27.1 per cent (for American POWs, 37 per cent),[372] seven times that of POWs under the Germans and Italians.
[373] While 37,583 prisoners from the UK, 28,500 from the Netherlands, and 14,473 from the United States were released after the surrender of Japan, the number of Chinese released was only 56.
At least five million Chinese civilians from northern China and Manchukuo were enslaved between 1935 and 1941 by the East Asia Development Board, or Kōain, for work in mines and war industries.
After 1942, the number reached 10 million.
[375] In Java, between 4 and 10 million rōmusha (Japanese: "manual labourers"), were forced to work by the Japanese military.
About 270,000 of these Javanese labourers were sent to other Japanese-held areas in South East Asia, and only 52,000 were repatriated to Java.
In Europe, occupation came under two forms.
In Western, Northern, and Central Europe (France, Norway, Denmark, the Low Countries, and the annexed portions of Czechoslovakia) Germany established economic policies through which it collected roughly 69.5 billion reichmarks (27.8 billion US dollars) by the end of the war; this figure does not include the sizeable plunder of industrial products, military equipment, raw materials and other goods.
[377] Thus, the income from occupied nations was over 40 per cent of the income Germany collected from taxation, a figure which increased to nearly 40 per cent of total German income as the war went on.
In the East, the intended gains of Lebensraum were never attained as fluctuating front-lines and Soviet scorched earth policies denied resources to the German invaders.
[379] Unlike in the West, the Nazi racial policy encouraged extreme brutality against what it considered to be the "inferior people" of Slavic descent; most German advances were thus followed by mass executions.
[380] Although resistance groups formed in most occupied territories, they did not significantly hamper German operations in either the East[381] or the West[382] until late 1943.
In Asia, Japan termed nations under its occupation as being part of the Greater East Asia Co-Prosperity Sphere, essentially a Japanese hegemony which it claimed was for purposes of liberating colonised peoples.
[383] Although Japanese forces were originally welcomed as liberators from European domination in some territories, their excessive brutality turned local public opinion against them within weeks.
[384] During Japan's initial conquest it captured 4,000,000 barrels (640,000 m3) of oil (~5.5×105 tonnes) left behind by retreating Allied forces, and by 1943 was able to get production in the Dutch East Indies up to 50 million barrels (~6.8×10^6 t), 76 per cent of its 1940 output rate.
In Europe, before the outbreak of the war, the Allies had significant advantages in both population and economics.
In 1938, the Western Allies (United Kingdom, France, Poland and British Dominions) had a 30 per cent larger population and a 30 per cent higher gross domestic product than the European Axis powers (Germany and Italy); if colonies are included, it then gives the Allies more than a 5:1 advantage in population and nearly 2:1 advantage in GDP.
[385] In Asia at the same time, China had roughly six times the population of Japan, but only an 89 per cent higher GDP; this is reduced to three times the population and only a 38 per cent higher GDP if Japanese colonies are included.
The United States provided about two-thirds of all the ordnance used by the Allies in terms of warships, transports, warplanes, artillery, tanks, trucks, and ammunition.
[386] Though the Allies' economic and population advantages were largely mitigated during the initial rapid blitzkrieg attacks of Germany and Japan, they became the decisive factor by 1942, after the United States and Soviet Union joined the Allies, as the war largely settled into one of attrition.
[387] While the Allies' ability to out-produce the Axis is often attributed to the Allies having more access to natural resources, other factors, such as Germany and Japan's reluctance to employ women in the labour force,[388] Allied strategic bombing,[389] and Germany's late shift to a war economy[390] contributed significantly.
Additionally, neither Germany nor Japan planned to fight a protracted war, and were not equipped to do so.
[391] To improve their production, Germany and Japan used millions of slave labourers;[392] Germany used about 12 million people, mostly from Eastern Europe,[368] while Japan used more than 18 million people in Far East Asia.
Aircraft were used for reconnaissance, as fighters, bombers, and ground-support, and each role was advanced considerably.
Innovation included airlift (the capability to quickly move limited high-priority supplies, equipment, and personnel);[393] and of strategic bombing (the bombing of enemy industrial and population centres to destroy the enemy's ability to wage war).
[394] Anti-aircraft weaponry also advanced, including defences such as radar and surface-to-air artillery.
The use of the jet aircraft was pioneered and, though late introduction meant it had little impact, it led to jets becoming standard in air forces worldwide.
[395] Although guided missiles were being developed, they were not advanced enough to reliably target aircraft until some years after the war.
Advances were made in nearly every aspect of naval warfare, most notably with aircraft carriers and submarines.
Although aeronautical warfare had relatively little success at the start of the war, actions at Taranto, Pearl Harbor, and the Coral Sea established the carrier as the dominant capital ship in place of the battleship.
[396][397][398] In the Atlantic, escort carriers proved to be a vital part of Allied convoys, increasing the effective protection radius and helping to close the Mid-Atlantic gap.
[399] Carriers were also more economical than battleships because of the relatively low cost of aircraft[400] and their not requiring to be as heavily armoured.
[401] Submarines, which had proved to be an effective weapon during the First World War,[402] were anticipated by all sides to be important in the second.
The British focused development on anti-submarine weaponry and tactics, such as sonar and convoys, while Germany focused on improving its offensive capability, with designs such as the Type VII submarine and wolfpack tactics.
[403] Gradually, improving Allied technologies such as the Leigh light, hedgehog, squid, and homing torpedoes proved victorious.
[citation needed]
Land warfare changed from the static front lines of trench warfare of World War I, which had relied on improved artillery that outmatched the speed of both infantry and cavalry, to increased mobility and combined arms.
The tank, which had been used predominantly for infantry support in the First World War, had evolved into the primary weapon.
[404] In the late 1930s, tank design was considerably more advanced than it had been during World War I,[405] and advances continued throughout the war with increases in speed, armour and firepower.
[citation needed] At the start of the war, most commanders thought enemy tanks should be met by tanks with superior specifications.
[406] This idea was challenged by the poor performance of the relatively light early tank guns against armour, and German doctrine of avoiding tank-versus-tank combat.
This, along with Germany's use of combined arms, were among the key elements of their highly successful blitzkrieg tactics across Poland and France.
[404] Many means of destroying tanks, including indirect artillery, anti-tank guns (both towed and self-propelled), mines, short-ranged infantry antitank weapons, and other tanks were used.
[406] Even with large-scale mechanisation, infantry remained the backbone of all forces,[407] and throughout the war, most infantry were equipped similarly to World War I.
[408] The portable machine gun spread, a notable example being the German MG34, and various submachine guns which were suited to close combat in urban and jungle settings.
[408] The assault rifle, a late war development incorporating many features of the rifle and submachine gun, became the standard postwar infantry weapon for most armed forces.
Most major belligerents attempted to solve the problems of complexity and security involved in using large codebooks for cryptography by designing ciphering machines, the most well known being the German Enigma machine.
[410] Development of SIGINT (signals intelligence) and cryptanalysis enabled the countering process of decryption.
Notable examples were the Allied decryption of Japanese naval codes[411] and British Ultra, a pioneering method for decoding Enigma benefiting from information given to the United Kingdom by the Polish Cipher Bureau, which had been decoding early versions of Enigma before the war.
[412] Another aspect of military intelligence was the use of deception, which the Allies used to great effect, such as in operations Mincemeat and Bodyguard.
Other technological and engineering feats achieved during, or as a result of, the war include the world's first programmable computers (Z3, Colossus, and ENIAC), guided missiles and modern rockets, the Manhattan Project's development of nuclear weapons, operations research and the development of artificial harbours and oil pipelines under the English Channel.
[citation needed]
Barack Hussein Obama II (/bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ (listen);[1] born August 4, 1961) is an American attorney and politician who served as the 44th president of the United States from 2009 to 2017.
A member of the Democratic Party, he was the first African American to be elected to the presidency.
He previously served as a U.S. senator from Illinois from 2005 to 2008.
Obama was born in Honolulu, Hawaii.
After graduating from Columbia University in 1983, he worked as a community organizer in Chicago.
In 1988, he enrolled in Harvard Law School, where he was the first black president of the Harvard Law Review.
After graduating, he became a civil rights attorney and an academic, teaching constitutional law at the University of Chicago Law School from 1992 to 2004.
He represented the 13th district for three terms in the Illinois Senate from 1997 until 2004 when he ran for the U.S. Senate.
He received national attention in 2004 with his March primary win, his well-received July Democratic National Convention keynote address, and his landslide November election to the Senate.
In 2008, he was nominated for president a year after his campaign began and after a close primary campaign against Hillary Clinton.
He was elected over Republican John McCain and was inaugurated on January 20, 2009.
Nine months later, he was named the 2009 Nobel Peace Prize laureate.
Regarded as a centrist New Democrat, Obama signed many landmark bills into law during his first two years in office.
The main reforms that were passed include the Patient Protection and Affordable Care Act (often referred to as "Obamacare", shortened as the "Affordable Care Act"), the Dodd–Frank Wall Street Reform and Consumer Protection Act, and the Don't Ask, Don't Tell Repeal Act of 2010.
The American Recovery and Reinvestment Act of 2009 and Tax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010 served as economic stimulus amidst the Great Recession.
After a lengthy debate over the national debt limit, he signed the Budget Control and the American Taxpayer Relief Acts.
In foreign policy, he increased U.S. troop levels in Afghanistan, reduced nuclear weapons with the United States–Russia New START treaty, and ended military involvement in the Iraq War.
He ordered military involvement in Libya in opposition to Muammar Gaddafi; Gaddafi was killed by NATO-assisted forces.
He also ordered the military operations that resulted in the deaths of Osama bin Laden and suspected Yemeni Al-Qaeda operative Anwar al-Awlaki.
After winning re-election by defeating Republican opponent Mitt Romney, Obama was sworn in for a second term in 2013.
During this term, he promoted inclusiveness for LGBT Americans.
His administration filed briefs that urged the Supreme Court to strike down same-sex marriage bans as unconstitutional (United States v. Windsor and Obergefell v. Hodges); same-sex marriage was fully legalized in 2015 after the Court ruled that a same-sex marriage ban was unconstitutional in Obergefell.
He advocated for gun control in response to the Sandy Hook Elementary School shooting, indicating support for a ban on assault weapons, and issued wide-ranging executive actions concerning climate change and immigration.
In foreign policy, he ordered military intervention in Iraq in response to gains made by ISIL after the 2011 withdrawal from Iraq, continued the process of ending U.S. combat operations in Afghanistan in 2016, promoted discussions that led to the 2015 Paris Agreement on global climate change, initiated sanctions against Russia following the invasion in Ukraine and again after Russian interference in the 2016 United States elections, brokered a nuclear deal with Iran, and normalized U.S. relations with Cuba.
Obama nominated three justices to the Supreme Court: Sonia Sotomayor and Elena Kagan were confirmed as justices, while Merrick Garland faced unprecedented partisan obstruction  and was ultimately not confirmed.
During his term in office, America's reputation in global polling significantly improved.
[2] Evaluations of his presidency among historians, political scientists, and the general public place him among the upper tier of American presidents.
Obama left office and retired in January 2017 and currently resides in Washington, D.C.[3][4] A December 2018 Gallup poll found Obama to be the most admired man in America for an unprecedented 11th consecutive year, although Dwight D. Eisenhower was selected most admired in twelve non-consecutive years.
Obama was born on August 4, 1961,[6] at Kapiolani Medical Center for Women and Children in Honolulu, Hawaii.
[7][8][9] He is the only president who was born outside of the contiguous 48 states.
[10] He was born to a white mother and a black father.
His mother, Ann Dunham (1942–1995), was born in Wichita, Kansas; she was mostly of English descent,[11] with some German, Irish, Scottish, Swiss, and Welsh ancestry.
[12] His father, Barack Obama Sr. (1936–1982), was a Luo Kenyan from Nyang'oma Kogelo.
Obama's parents met in 1960 in a Russian language class at the University of Hawaii at Manoa, where his father was a foreign student on a scholarship.
[13][14] The couple married in Wailuku, Hawaii, on February 2, 1961, six months before Obama was born.
In late August 1961 (a few weeks after he was born), Barack and his mother moved to the University of Washington in Seattle, where they lived for a year.
During that time, the elder Obama completed his undergraduate degree in economics in Hawaii, graduating in June 1962.
He then left to attend graduate school on a scholarship at Harvard University, where he earned an M.A.
in economics.
Obama's parents divorced in March 1964.
[17] Obama Sr. returned to Kenya in 1964, where he married for a third time and worked for the Kenyan government as the Senior Economic Analyst in the Ministry of Finance.
[18] He visited his son in Hawaii only once, at Christmas time in 1971,[19] before he was killed in an automobile accident in 1982, when Obama was 21 years old.
[20] Recalling his early childhood, Obama said, "That my father looked nothing like the people around me – that he was black as pitch, my mother white as milk – barely registered in my mind.
"[14] He described his struggles as a young adult to reconcile social perceptions of his multiracial heritage.
In 1963, Dunham met Lolo Soetoro at the University of Hawaii; he was an Indonesian East–West Center graduate student in geography.
The couple married on Molokai on March 15, 1965.
[22] After two one-year extensions of his J-1 visa, Lolo returned to Indonesia in 1966.
His wife and stepson followed sixteen months later in 1967.
The family initially lived in the Menteng Dalam neighborhood in the Tebet sub district of south Jakarta.
From 1970, they lived in a wealthier neighborhood in the Menteng sub district of central Jakarta.
Obama started out in St. Francis Pre-Education from age three to five.
From age six to ten, he then attended local Indonesian-language schools: Sekolah Dasar Katolik Santo Fransiskus Asisi (St. Francis of Assisi Catholic Elementary School) for two years and Sekolah Dasar Negeri Menteng 01 (State Elementary School Menteng 01/Besuki school) for one and a half years, supplemented by English-language Calvert School homeschooling by his mother.
[24][25] As a result of those four years in Jakarta, he was able to speak Indonesian fluently as a child.
[26][27][28] During his time in Indonesia, Obama's step-father taught him to be resilient and gave him "a pretty hardheaded assessment of how the world works".
In 1971, Obama returned to Honolulu to live with his maternal grandparents, Madelyn and Stanley Dunham.
He attended Punahou School—a private college preparatory school—with the aid of a scholarship from fifth grade until he graduated from high school in 1979.
[30] In his youth, Obama went by the nickname "Barry".
[31] Obama lived with his mother and half-sister, Maya Soetoro, in Hawaii for three years from 1972 to 1975 while his mother was a graduate student in anthropology at the University of Hawaii.
[32] Obama chose to stay in Hawaii with his grandparents for high school at Punahou when his mother and half-sister returned to Indonesia in 1975 so his mother could begin anthropology field work.
[33] His mother spent most of the next two decades in Indonesia, divorcing Lolo in 1980 and earning a PhD degree in 1992, before dying in 1995 in Hawaii following unsuccessful treatment for ovarian and uterine cancer.
Obama later reflected on his years in Honolulu and wrote: "The opportunity that Hawaii offered – to experience a variety of cultures in a climate of mutual respect – became an integral part of my world view, and a basis for the values that I hold most dear.
"[35] Obama has also written and talked about using alcohol, marijuana, and cocaine during his teenage years to "push questions of who I was out of my mind".
[36] Obama was also a member of the "choom gang", a self-named group of friends that spent time together and occasionally smoked marijuana.
After graduating from high school in 1979, Obama moved to Los Angeles to attend Occidental College.
In February 1981, Obama made his first public speech, calling for Occidental to participate in the disinvestment from South Africa in response to that nation's policy of apartheid.
[39] In mid-1981, Obama traveled to Indonesia to visit his mother and half-sister Maya, and visited the families of college friends in Pakistan and India for three weeks.
[39] Later in 1981, he transferred as a junior to Columbia University in New York City, where he majored in political science with a specialty in international relations[40] and in English literature[41] and lived off-campus on West 109th Street.
[42] He graduated with a BA degree in 1983 and worked for about a year at the Business International Corporation, where he was a financial researcher and writer,[43][44] then as a project coordinator for the New York Public Interest Research Group on the City College of New York campus for three months in 1985.
In a 2006 interview, Obama highlighted the diversity of his extended family: "It's like a little mini-United Nations", he said.
"I've got relatives who look like Bernie Mac, and I've got relatives who look like Margaret Thatcher.
"[48] Obama has a half-sister with whom he was raised (Maya Soetoro-Ng) and seven other half-siblings from his Kenyan father's family—six of them living.
[49] Obama's mother was survived by her Kansas-born mother, Madelyn Dunham,[50] until her death on November 2, 2008,[51] two days before his election to the Presidency.
Obama also has roots in Ireland; he met with his Irish cousins in Moneygall in May 2011.
[52] In Dreams from My Father, Obama ties his mother's family history to possible Native American ancestors and distant relatives of Jefferson Davis, President of the Confederate States of America during the American Civil War.
He also shares distant ancestors in common with George W. Bush and Dick Cheney, among others.
Obama is a supporter of the Chicago White Sox, and he threw out the first pitch at the 2005 ALCS when he was still a senator.
[54] In 2009, he threw out the ceremonial first pitch at the All-Star Game while wearing a White Sox jacket.
[55] He is also primarily a Chicago Bears football fan in the NFL, but in his childhood and adolescence was a fan of the Pittsburgh Steelers, and rooted for them ahead of their victory in Super Bowl XLIII 12 days after he took office as president.
[56] In 2011, Obama invited the 1985 Chicago Bears to the White House; the team had not visited the White House after their Super Bowl win in 1986 due to the Space Shuttle Challenger disaster.
[57] He plays basketball, a sport he participated in as a member of his high school's varsity team,[58] and he is left-handed.
Obama lived with anthropologist Sheila Miyoshi Jager while he was a community organizer in Chicago in the 1980s.
[60] He proposed to her twice, but both Jager and her parents turned him down.
[60][61] The relationship was only made public in May 2017, several months after Obama's presidency had ended.
In June 1989, Obama met Michelle Robinson when he was employed as a summer associate at the Chicago law firm of Sidley Austin.
[62] Robinson was assigned for three months as Obama's adviser at the firm, and she joined him at several group social functions but declined his initial requests to date.
[63] They began dating later that summer, became engaged in 1991, and were married on October 3, 1992.
[64] The couple's first daughter, Malia Ann, was born in 1998,[65] followed by a second daughter, Natasha ("Sasha"), in 2001.
[66] The Obama daughters attended the University of Chicago Laboratory Schools.
When they moved to Washington, D.C., in January 2009, the girls started at the Sidwell Friends School.
[67] The Obamas have two Portuguese Water Dogs; the first, a male named Bo, was a gift from Senator Ted Kennedy.
[68] In 2013, Bo was joined by Sunny, a female.
In 2005, the family applied the proceeds of a book deal and moved from a Hyde Park, Chicago condominium to a $1.6 million house (equivalent to $2.1 million in 2018) in neighboring Kenwood, Chicago.
[70] The purchase of an adjacent lot—and sale of part of it to Obama by the wife of developer, campaign donor and friend Tony Rezko—attracted media attention because of Rezko's subsequent indictment and conviction on political corruption charges that were unrelated to Obama.
In December 2007, Money Magazine estimated Obama's net worth at $1.3 million (equivalent to $1.6 million in 2018) .
[72] Their 2009 tax return showed a household income of $5.5 million—up from about $4.2 million in 2007 and $1.6 million in 2005—mostly from sales of his books.
[73][74] On his 2010 income of $1.7 million, he gave 14% to non-profit organizations, including $131,000 to Fisher House Foundation, a charity assisting wounded veterans' families, allowing them to reside near where the veteran is receiving medical treatments.
[75][76] Per his 2012 financial disclosure, Obama may be worth as much as $10 million.
In early 2010, Michelle spoke about her husband's smoking habit and said that Barack had quit smoking.
On his 55th birthday, August 4, 2016, Obama penned an essay in Glamour, in which he described how his daughters and the presidency have made him a feminist.
Obama is a Protestant Christian whose religious views developed in his adult life.
[83] He wrote in The Audacity of Hope that he "was not raised in a religious household".
He described his mother, raised by non-religious parents, as being detached from religion, yet "in many ways the most spiritually awakened person that I have ever known."
He described his father as a "confirmed atheist" by the time his parents met, and his stepfather as "a man who saw religion as not particularly useful."
Obama explained how, through working with black churches as a community organizer while in his twenties, he came to understand "the power of the African-American religious tradition to spur social change.
In January 2008, Obama told Christianity Today: "I am a Christian, and I am a devout Christian.
I believe in the redemptive death and resurrection of Jesus Christ.
I believe that faith gives me a path to be cleansed of sin and have eternal life.
"[85] On September 27, 2010, Obama released a statement commenting on his religious views saying, "I'm a Christian by choice.
My family didn't – frankly, they weren't folks who went to church every week.
And my mother was one of the most spiritual people I knew, but she didn't raise me in the church.
So I came to my Christian faith later in life, and it was because the precepts of Jesus Christ spoke to me in terms of the kind of life that I would want to lead – being my brothers' and sisters' keeper, treating others as they would treat me.
Obama met Trinity United Church of Christ pastor Rev.
Jeremiah Wright in October 1987 and became a member of Trinity in 1992.
[88] During Obama's first presidential campaign in May 2008, he resigned from Trinity after some of Wright's statements were criticized.
[89] Since moving to Washington, D.C., in 2009, the Obama family has attended several Protestant churches, including Shiloh Baptist Church and St. John's Episcopal Church, as well as Evergreen Chapel at Camp David, but the members of the family do not attend church on a regular basis.
Two years after graduating from Columbia, Obama was back in Chicago when he was hired as director of the Developing Communities Project, a church-based community organization originally comprising eight Catholic parishes in Roseland, West Pullman, and Riverdale on Chicago's South Side.
He worked there as a community organizer from June 1985 to May 1988.
[46][93] He helped set up a job training program, a college preparatory tutoring program, and a tenants' rights organization in Altgeld Gardens.
[94] Obama also worked as a consultant and instructor for the Gamaliel Foundation, a community organizing institute.
[95] In mid-1988, he traveled for the first time in Europe for three weeks and then for five weeks in Kenya, where he met many of his paternal relatives for the first time.
Obama entered Harvard Law School in the fall of 1988, living in nearby Somerville, Massachusetts.
[99] He was selected as an editor of the Harvard Law Review at the end of his first year,[100] president of the journal in his second year,[94][101] and research assistant to the constitutional scholar Laurence Tribe while at Harvard for two years.
[102] During his summers, he returned to Chicago, where he worked as an associate at the law firms of Sidley Austin in 1989 and Hopkins & Sutter in 1990.
[103] After graduating with a JD degree magna cum laude[104] from Harvard in 1991, he returned to Chicago.
[100] Obama's election as the first black president of the Harvard Law Review gained national media attention[94][101] and led to a publishing contract and advance for a book about race relations,[105] which evolved into a personal memoir.
The manuscript was published in mid-1995 as Dreams from My Father.
In 1991, Obama accepted a two-year position as Visiting Law and Government Fellow at the University of Chicago Law School to work on his first book.
[105][106] He then taught constitutional law at the University of Chicago Law School for twelve years, first as a lecturer from 1992 to 1996, and then as a senior lecturer from 1996 to 2004.
From April to October 1992, Obama directed Illinois's Project Vote, a voter registration campaign with ten staffers and seven hundred volunteer registrars; it achieved its goal of registering 150,000 of 400,000 unregistered African Americans in the state, leading Crain's Chicago Business to name Obama to its 1993 list of "40 under Forty" powers to be.
He joined Davis, Miner, Barnhill & Galland, a 13-attorney law firm specializing in civil rights litigation and neighborhood economic development, where he was an associate for three years from 1993 to 1996, then of counsel from 1996 to 2004.
In 1994, he was listed as one of the lawyers in Buycks-Roberson v. Citibank Fed.
Bank, 94 C 4094 (N.D.
[109] This class action lawsuit was filed in 1994 with Selma Buycks-Roberson as lead plaintiff and alleged that Citibank Federal Savings Bank had engaged in practices forbidden under the Equal Credit Opportunity Act and the Fair Housing Act.
[110] The case was settled out of court.
[111] Final judgment was issued on May 13, 1998, with Citibank Federal Savings Bank agreeing to pay attorney fees.
[112] His law license became inactive in 2007.
From 1994 to 2002, Obama served on the boards of directors of the Woods Fund of Chicago—which in 1985 had been the first foundation to fund the Developing Communities Project—and of the Joyce Foundation.
[46] He served on the board of directors of the Chicago Annenberg Challenge from 1995 to 2002, as founding president and chairman of the board of directors from 1995 to 1999.
Obama was elected to the Illinois Senate in 1996, succeeding Democratic State Senator Alice Palmer from Illinois's 13th District, which, at that time, spanned Chicago South Side neighborhoods from Hyde Park–Kenwood south to South Shore and west to Chicago Lawn.
[115] Once elected, Obama gained bipartisan support for legislation that reformed ethics and health care laws.
[116] He sponsored a law that increased tax credits for low-income workers, negotiated welfare reform, and promoted increased subsidies for childcare.
[117] In 2001, as co-chairman of the bipartisan Joint Committee on Administrative Rules, Obama supported Republican Governor Ryan's payday loan regulations and predatory mortgage lending regulations aimed at averting home foreclosures.
He was reelected to the Illinois Senate in 1998, defeating Republican Yesse Yehudah in the general election, and was re-elected again in 2002.
[119] In 2000, he lost a Democratic primary race for Illinois's 1st congressional district in the United States House of Representatives to four-term incumbent Bobby Rush by a margin of two to one.
In January 2003, Obama became chairman of the Illinois Senate's Health and Human Services Committee when Democrats, after a decade in the minority, regained a majority.
[121] He sponsored and led unanimous, bipartisan passage of legislation to monitor racial profiling by requiring police to record the race of drivers they detained, and legislation making Illinois the first state to mandate videotaping of homicide interrogations.
[117][122] During his 2004 general election campaign for the U.S. Senate, police representatives credited Obama for his active engagement with police organizations in enacting death penalty reforms.
[123] Obama resigned from the Illinois Senate in November 2004 following his election to the U.S.
In May 2002, Obama commissioned a poll to assess his prospects in a 2004 U.S. Senate race.
He created a campaign committee, began raising funds, and lined up political media consultant David Axelrod by August 2002.
Obama formally announced his candidacy in January 2003.
Obama was an early opponent of the George W. Bush administration's 2003 invasion of Iraq.
[126] On October 2, 2002, the day President Bush and Congress agreed on the joint resolution authorizing the Iraq War,[127] Obama addressed the first high-profile Chicago anti-Iraq War rally,[128] and spoke out against the war.
[129] He addressed another anti-war rally in March 2003 and told the crowd that "it's not too late" to stop the war.
Decisions by Republican incumbent Peter Fitzgerald and his Democratic predecessor Carol Moseley Braun to not participate in the election resulted in wide-open Democratic and Republican primary contests involving 15 candidates.
[131] In the March 2004 primary election, Obama won in an unexpected landslide—which overnight made him a rising star within the national Democratic Party, started speculation about a presidential future, and led to the reissue of his memoir, Dreams from My Father.
[132] In July 2004, Obama delivered the keynote address at the 2004 Democratic National Convention,[133] seen by 9.1 million viewers.
His speech was well received and elevated his status within the Democratic Party.
Obama's expected opponent in the general election, Republican primary winner Jack Ryan, withdrew from the race in June 2004.
[135] Six weeks later, Alan Keyes accepted the Republican nomination to replace Ryan.
[136] In the November 2004 general election, Obama won with 70% of the vote.
Obama was sworn in as a senator on January 3, 2005,[138] becoming the only Senate member of the Congressional Black Caucus.
[139] CQ Weekly characterized him as a "loyal Democrat" based on analysis of all Senate votes from 2005 to 2007.
Obama announced on November 13, 2008, that he would resign his Senate seat on November 16, 2008, before the start of the lame-duck session, to focus on his transition period for the presidency.
Obama cosponsored the Secure America and Orderly Immigration Act.
[141] He introduced two initiatives that bore his name: Lugar–Obama, which expanded the Nunn–Lugar Cooperative Threat Reduction concept to conventional weapons;[142] and the Federal Funding Accountability and Transparency Act of 2006, which authorized the establishment of USAspending.gov, a web search engine on federal spending.
[143] On June 3, 2008, Senator Obama—along with Senators Tom Carper, Tom Coburn, and John McCain—introduced follow-up legislation: Strengthening Transparency and Accountability in Federal Spending Act of 2008.
Obama sponsored legislation that would have required nuclear plant owners to notify state and local authorities of radioactive leaks, but the bill failed to pass in the full Senate after being heavily modified in committee.
[145] Regarding tort reform, Obama voted for the Class Action Fairness Act of 2005 and the FISA Amendments Act of 2008, which grants immunity from civil liability to telecommunications companies complicit with NSA warrantless wiretapping operations.
In December 2006, President Bush signed into law the Democratic Republic of the Congo Relief, Security, and Democracy Promotion Act, marking the first federal legislation to be enacted with Obama as its primary sponsor.
[148] In January 2007, Obama and Senator Feingold introduced a corporate jet provision to the Honest Leadership and Open Government Act, which was signed into law in September 2007.
[149] Obama also introduced two unsuccessful bills: the Deceptive Practices and Voter Intimidation Prevention Act to criminalize deceptive practices in federal elections,[150] and the Iraq War De-Escalation Act of 2007.
Later in 2007, Obama sponsored an amendment to the Defense Authorization Act to add safeguards for personality-disorder military discharges.
[152] This amendment passed the full Senate in the spring of 2008.
[153] He sponsored the Iran Sanctions Enabling Act supporting divestment of state pension funds from Iran's oil and gas industry, which was never enacted but later incorporated in the Comprehensive Iran Sanctions, Accountability, and Divestment Act of 2010;[154] and co-sponsored legislation to reduce risks of nuclear terrorism.
[155] Obama also sponsored a Senate amendment to the State Children's Health Insurance Program, providing one year of job protection for family members caring for soldiers with combat-related injuries.
Obama held assignments on the Senate Committees for Foreign Relations, Environment and Public Works and Veterans' Affairs through December 2006.
[157] In January 2007, he left the Environment and Public Works committee and took additional assignments with Health, Education, Labor and Pensions and Homeland Security and Governmental Affairs.
[158] He also became Chairman of the Senate's subcommittee on European Affairs.
[159] As a member of the Senate Foreign Relations Committee, Obama made official trips to Eastern Europe, the Middle East, Central Asia and Africa.
He met with Mahmoud Abbas before Abbas became President of the Palestinian National Authority, and gave a speech at the University of Nairobi in which he condemned corruption within the Kenyan government.
On February 10, 2007, Obama announced his candidacy for President of the United States in front of the Old State Capitol building in Springfield, Illinois.
[161][162] The choice of the announcement site was viewed as symbolic because it was also where Abraham Lincoln delivered his historic "House Divided" speech in 1858.
[161][163] Obama emphasized issues of rapidly ending the Iraq War, increasing energy independence, and reforming the health care system,[164] in a campaign that projected themes of hope and change.
Numerous candidates entered the Democratic Party presidential primaries.
The field narrowed to a duel between Obama and Senator Hillary Clinton after early contests, with the race remaining close throughout the primary process but with Obama gaining a steady lead in pledged delegates due to better long-range planning, superior fundraising, dominant organizing in caucus states, and better exploitation of delegate allocation rules.
[166] On June 7, 2008, Clinton ended her campaign and endorsed Obama.
On August 23, Obama announced his selection of Delaware Senator Joe Biden as his vice presidential running mate.
[168] Obama selected Biden from a field speculated to include former Indiana Governor and Senator Evan Bayh and Virginia Governor Tim Kaine.
[168] At the Democratic National Convention in Denver, Colorado, Hillary Clinton called for her supporters to endorse Obama, and she and Bill Clinton gave convention speeches in his support.
[169] Obama delivered his acceptance speech, not at the center where the Democratic National Convention was held, but at Invesco Field at Mile High to a crowd of approximately 84,000 people; the speech was viewed by over 38 million people worldwide.
During both the primary process and the general election, Obama's campaign set numerous fundraising records, particularly in the quantity of small donations.
[173] On June 19, 2008, Obama became the first major-party presidential candidate to turn down public financing in the general election since the system was created in 1976.
John McCain was nominated as the Republican candidate, and he selected Sarah Palin as his running mate.
The two candidates engaged in three presidential debates in September and October 2008.
[175] On November 4, Obama won the presidency with 365 electoral votes to 173 received by McCain.
[176] Obama won 52.9% of the popular vote to McCain's 45.7%.
[177] He became the first African American to be elected president.
[178] Obama delivered his victory speech before hundreds of thousands of supporters in Chicago's Grant Park.
On April 4, 2011, Obama announced his reelection campaign for 2012 in a video titled "It Begins with Us" that he posted on his website and filed election papers with the Federal Election Commission.
[180][181][182] As the incumbent president he ran virtually unopposed in the Democratic Party presidential primaries,[183] and on April 3, 2012, Obama had secured the 2778 convention delegates needed to win the Democratic nomination.
At the Democratic National Convention in Charlotte, North Carolina, Obama and Joe Biden were formally nominated by former President Bill Clinton as the Democratic Party candidates for president and vice president in the general election.
Their main opponents were Republicans Mitt Romney, the former governor of Massachusetts, and Representative Paul Ryan of Wisconsin.
On November 6, 2012, Obama won 332 electoral votes, exceeding the 270 required for him to be reelected as president.
[186][187][188] With 51.1% of the popular vote,[189] Obama became the first Democratic president since Franklin D. Roosevelt to win the majority of the popular vote twice.
[190][191] Obama addressed supporters and volunteers at Chicago's McCormick Place after his reelection and said: "Tonight you voted for action, not politics as usual.
You elected us to focus on your jobs, not ours.
And in the coming weeks and months, I am looking forward to reaching out and working with leaders of both parties.
The inauguration of Barack Obama as the 44th President took place on January 20, 2009.
In his first few days in office, Obama issued executive orders and presidential memoranda directing the U.S. military to develop plans to withdraw troops from Iraq.
[194] He ordered the closing of the Guantanamo Bay detention camp,[195] but Congress prevented the closure by refusing to appropriate the required funds[196][197][198] and preventing moving any Guantanamo detainee into the U.S. or to other countries.
[199] Obama reduced the secrecy given to presidential records.
[200] He also revoked President George W. Bush's restoration of President Ronald Reagan's Mexico City Policy prohibiting federal aid to international family planning organizations that perform or provide counseling about abortion.
The first bill signed into law by Obama was the Lilly Ledbetter Fair Pay Act of 2009, relaxing the statute of limitations for equal-pay lawsuits.
[202] Five days later, he signed the reauthorization of the State Children's Health Insurance Program (SCHIP) to cover an additional 4 million uninsured children.
[203] In March 2009, Obama reversed a Bush-era policy that had limited funding of embryonic stem cell research and pledged to develop "strict guidelines" on the research.
Obama appointed two women to serve on the Supreme Court in the first two years of his Presidency.
He nominated Sonia Sotomayor on May 26, 2009 to replace retiring Associate Justice David Souter; she was confirmed on August 6, 2009,[205] becoming the first Supreme Court Justice of Hispanic descent.
[206] Obama nominated Elena Kagan on May 10, 2010 to replace retiring Associate Justice John Paul Stevens.
She was confirmed on August 5, 2010, bringing the number of women sitting simultaneously on the Court to three justices for the first time in American history.
On March 30, 2010, Obama signed the Health Care and Education Reconciliation Act, a reconciliation bill that ended the process of the federal government giving subsidies to private banks to give out federally insured loans, increased the Pell Grant scholarship award, and made changes to the Patient Protection and Affordable Care Act.
In a major space policy speech in April 2010, Obama announced a planned change in direction at NASA, the U.S. space agency.
He ended plans for a return of human spaceflight to the moon and development of the Ares I rocket, Ares V rocket and Constellation program, in favor of funding Earth science projects, a new rocket type, and research and development for an eventual manned mission to Mars, and ongoing missions to the International Space Station.
President Obama's 2011 State of the Union Address focused on themes of education and innovation, stressing the importance of innovation economics to make the United States more competitive globally.
He spoke of a five-year freeze in domestic spending, eliminating tax breaks for oil companies and reversing tax cuts for the wealthiest Americans, banning congressional earmarks, and reducing healthcare costs.
He promised that the United States would have one million electric vehicles on the road by 2015 and would be 80% reliant on "clean" electricity.
On October 8, 2009, Obama signed the Matthew Shepard and James Byrd Jr.
Hate Crimes Prevention Act, a measure that expanded the 1969 United States federal hate-crime law to include crimes motivated by a victim's actual or perceived gender, sexual orientation, gender identity, or disability.
On October 30, 2009, Obama lifted the ban on travel to the United States by those infected with HIV, which was celebrated by Immigration Equality.
On December 22, 2010, Obama signed the Don't Ask, Don't Tell Repeal Act of 2010, which fulfilled a key promise made in the 2008 presidential campaign[215][216] to end the Don't ask, don't tell policy of 1993 that had prevented gay and lesbian people from serving openly in the United States Armed Forces.
[217] In 2016, the Pentagon ended the policy that also barred transgender people from serving openly in the military.
As a candidate for the Illinois state senate in 1996, Obama had said that he favored legalizing same-sex marriage.
[219] By the time of his Senate run in 2004, he said that he supported civil unions and domestic partnerships for same-sex partners, but he opposed same-sex marriages.
[220] In 2008, he reaffirmed this position by stating "I believe marriage is between a man and a woman.
I am not in favor of gay marriage.
"[221] On May 9, 2012, shortly after the official launch of his campaign for re-election as president, Obama said his views had evolved, and he publicly affirmed his personal support for the legalization of same-sex marriage, becoming the first sitting U.S. president to do so.
During his second inaugural address on January 21, 2013,[193] Obama became the first U.S. President in office to call for full equality for gay Americans: "Our journey is not complete until our gay brothers and sisters are treated like anyone else under the law – for if we are truly created equal, then surely the love we commit to one another must be equal as well."
This was the first time that a president mentioned gay rights or the word "gay" in an inaugural address.
In 2013, the Obama Administration filed briefs that urged the Supreme Court to rule in favor of same-sex couples in the cases of Hollingsworth v. Perry (regarding same-sex marriage)[226] and United States v. Windsor (regarding the Defense of Marriage Act).
[227] Then, following the Supreme Court's 2015 decision in Obergefell v. Hodges (ruling same-sex marriage to be a fundamental right), Obama asserted that, "This decision affirms what millions of Americans already believe in their hearts: When all Americans are treated as equal we are all more free.
On July 30, 2015, the White House Office of National AIDS Policy revised its strategy for addressing the disease, which included widespread testing and linkage to healthcare, which was celebrated by the Human Rights Campaign.
On March 11, 2009, Obama created the White House Council on Women and Girls, which formed part of the Office of Intergovernmental Affairs, having been established by Executive Order 13506 with a broad mandate to advise him on issues relating to the welfare of American women and girls.
[230] The Council was chaired by Senior Advisor to the President Valerie Jarrett.
[231] Obama also established the White House Task Force to Protect Students from Sexual Assault through a government memorandum on January 22, 2014, with a broad mandate to advise him on issues relating to sexual assault on college and university campuses throughout the United States.
[231][232][233] The co-chairs of the Task Force were Vice President Joe Biden and Jarrett.
[232] The Task Force was a development out of the White House Council on Women and Girls and Office of the Vice President of the United States, and prior to that, the 1994 Violence Against Women Act that was first drafted by Biden.
On February 17, 2009, Obama signed the American Recovery and Reinvestment Act of 2009, a $787 billion economic stimulus package aimed at helping the economy recover from the deepening worldwide recession.
[235] The act includes increased federal spending for health care, infrastructure, education, various tax breaks and incentives, and direct assistance to individuals.
In March, Obama's Treasury Secretary, Timothy Geithner, took further steps to manage the financial crisis, including introducing the Public–Private Investment Program for Legacy Assets, which contains provisions for buying up to two trillion dollars in depreciated real estate assets.
Obama intervened in the troubled automotive industry[238] in March 2009, renewing loans for General Motors and Chrysler to continue operations while reorganizing.
Over the following months the White House set terms for both firms' bankruptcies, including the sale of Chrysler to Italian automaker Fiat[239] and a reorganization of GM giving the U.S. government a temporary 60% equity stake in the company, with the Canadian government taking a 12% stake.
[240] In June 2009, dissatisfied with the pace of economic stimulus, Obama called on his cabinet to accelerate the investment.
[241] He signed into law the Car Allowance Rebate System, known colloquially as "Cash for Clunkers", that temporarily boosted the economy.
The Bush and Obama administrations authorized spending and loan guarantees from the Federal Reserve and the Treasury Department.
These guarantees totaled about $11.5 trillion, but only $3 trillion was spent by the end of November 2009.
[245] Obama and the Congressional Budget Office predicted the 2010 budget deficit would be $1.5 trillion or 10.6% of the nation's gross domestic product (GDP) compared to the 2009 deficit of $1.4 trillion or 9.9% of GDP.
[246][247] For 2011, the administration predicted the deficit would shrink to $1.34 trillion, and the 10-year deficit would increase to $8.53 trillion or 90% of GDP.
[248] The most recent increase in the U.S. debt ceiling to $17.2 trillion took effect in February 2014.
[249] On August 2, 2011, after a lengthy congressional debate over whether to raise the nation's debt limit, Obama signed the bipartisan Budget Control Act of 2011.
The legislation enforces limits on discretionary spending until 2021, establishes a procedure to increase the debt limit, creates a Congressional Joint Select Committee on Deficit Reduction to propose further deficit reduction with a stated goal of achieving at least $1.5 trillion in budgetary savings over 10 years, and establishes automatic procedures for reducing spending by as much as $1.2 trillion if legislation originating with the new joint select committee does not achieve such savings.
[250] By passing the legislation, Congress was able to prevent a U.S. government default on its obligations.
As it did throughout 2008, the unemployment rate rose in 2009, reaching a peak in October at 10.0% and averaging 10.0% in the fourth quarter.
Following a decrease to 9.7% in the first quarter of 2010, the unemployment rate fell to 9.6% in the second quarter, where it remained for the rest of the year.
[254] Between February and December 2010, employment rose by 0.8%, which was less than the average of 1.9% experienced during comparable periods in the past four employment recoveries.
[255] By November 2012, the unemployment rate fell to 7.7%,[256] decreasing to 6.7% in the last month of 2013.
[257] During 2014, the unemployment rate continued to decline, falling to 6.3% in the first quarter.
[258] GDP growth returned in the third quarter of 2009, expanding at a rate of 1.6%, followed by a 5.0% increase in the fourth quarter.
[259] Growth continued in 2010, posting an increase of 3.7% in the first quarter, with lesser gains throughout the rest of the year.
[259] In July 2010, the Federal Reserve noted that economic activity continued to increase, but its pace had slowed, and chairman Ben Bernanke said the economic outlook was "unusually uncertain".
[260] Overall, the economy expanded at a rate of 2.9% in 2010.
The Congressional Budget Office (CBO) and a broad range of economists credit Obama's stimulus plan for economic growth.
[262][263] The CBO released a report stating that the stimulus bill increased employment by 1–2.1 million,[263][264][265][266] while conceding that "It is impossible to determine how many of the reported jobs would have existed in the absence of the stimulus package.
"[262] Although an April 2010, survey of members of the National Association for Business Economics showed an increase in job creation (over a similar January survey) for the first time in two years, 73% of 68 respondents believed that the stimulus bill has had no impact on employment.
[267] The economy of the United States has grown faster than the other original NATO members by a wider margin under President Obama than it has anytime since the end of World War II.
[268] The Organisation for Economic Co-operation and Development credits the much faster growth in the United States to the stimulus plan of the US and the austerity measures in the European Union.
Within a month of the 2010 midterm elections, Obama announced a compromise deal with the Congressional Republican leadership that included a temporary, two-year extension of the 2001 and 2003 income tax rates, a one-year payroll tax reduction, continuation of unemployment benefits, and a new rate and exemption amount for estate taxes.
[270] The compromise overcame opposition from some in both parties, and the resulting $858 billion Tax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010 passed with bipartisan majorities in both houses of Congress before Obama signed it on December 17, 2010.
In December 2013, Obama declared that growing income inequality is a "defining challenge of our time" and called on Congress to bolster the safety net and raise wages.
This came on the heels of the nationwide strikes of fast-food workers and Pope Francis' criticism of inequality and trickle-down economics.
Obama urged Congress to ratify a 12-nation free trade pact called the Trans-Pacific Partnership.
On September 30, 2009, the Obama administration proposed new regulations on power plants, factories, and oil refineries in an attempt to limit greenhouse gas emissions and to curb global warming.
On April 20, 2010, an explosion destroyed an offshore drilling rig at the Macondo Prospect in the Gulf of Mexico, causing a major sustained oil leak.
Obama visited the Gulf, announced a federal investigation, and formed a bipartisan commission to recommend new safety standards, after a review by Secretary of the Interior Ken Salazar and concurrent Congressional hearings.
He then announced a six-month moratorium on new deepwater drilling permits and leases, pending regulatory review.
[276] As multiple efforts by BP failed, some in the media and public expressed confusion and criticism over various aspects of the incident, and stated a desire for more involvement by Obama and the federal government.
In July 2013, Obama expressed reservations and stated he "would reject the Keystone XL pipeline if it increased carbon pollution" or "greenhouse emissions".
[278][279] Obama's advisers called for a halt to petroleum exploration in the Arctic in January 2013.
[280] On February 24, 2015, Obama vetoed a bill that would authorize the pipeline.
[281] It was the third veto of Obama's presidency and his first major veto.
Obama emphasized the conservation of federal lands during his term in office.
He used his power under the Antiquities Act to create 25 new national monuments during his presidency and expand four others, protecting a total of 553,000,000 acres (224,000,000 ha) of federal lands and waters, more than any other U.S.
Obama called for Congress to pass legislation reforming health care in the United States, a key campaign promise and a top legislative goal.
[284] He proposed an expansion of health insurance coverage to cover the uninsured, to cap premium increases, and to allow people to retain their coverage when they leave or change jobs.
His proposal was to spend $900 billion over 10 years and include a government insurance plan, also known as the public option, to compete with the corporate insurance sector as a main component to lowering costs and improving quality of health care.
It would also make it illegal for insurers to drop sick people or deny them coverage for pre-existing conditions, and require every American to carry health coverage.
The plan also includes medical spending cuts and taxes on insurance companies that offer expensive plans.
On July 14, 2009, House Democratic leaders introduced a 1,017-page plan for overhauling the U.S. health care system, which Obama wanted Congress to approve by the end of 2009.
[284] After much public debate during the Congressional summer recess of 2009, Obama delivered a speech to a joint session of Congress on September 9 where he addressed concerns over the proposals.
[288] In March 2009, Obama lifted a ban on using federal funds for stem cell research.
On November 7, 2009, a health care bill featuring the public option was passed in the House.
[290][291] On December 24, 2009, the Senate passed its own bill—without a public option—on a party-line vote of 60–39.
[292] On March 21, 2010, the Patient Protection and Affordable Care Act (ACA) passed by the Senate in December was passed in the House by a vote of 219 to 212.
[293] Obama signed the bill into law on March 23, 2010.
The ACA includes health-related provisions, most of which took effect in 2014, including expanding Medicaid eligibility for people making up to 133% of the federal poverty level (FPL) starting in 2014,[295] subsidizing insurance premiums for people making up to 400% of the FPL ($88,000 for family of four in 2010) so their maximum "out-of-pocket" payment for annual premiums will be from 2% to 9.5% of income,[296][297] providing incentives for businesses to provide health care benefits, prohibiting denial of coverage and denial of claims based on pre-existing conditions, establishing health insurance exchanges, prohibiting annual coverage caps, and support for medical research.
According to White House and CBO figures, the maximum share of income that enrollees would have to pay would vary depending on their income relative to the federal poverty level.
The costs of these provisions are offset by taxes, fees, and cost-saving measures, such as new Medicare taxes for those in high-income brackets, taxes on indoor tanning, cuts to the Medicare Advantage program in favor of traditional Medicare, and fees on medical devices and pharmaceutical companies;[300] there is also a tax penalty for those who do not obtain health insurance, unless they are exempt due to low income or other reasons.
[301] In March 2010, the CBO estimated that the net effect of both laws will be a reduction in the federal deficit by $143 billion over the first decade.
The law faced several legal challenges, primarily based on the argument that an individual mandate requiring Americans to buy health insurance was unconstitutional.
On June 28, 2012, the Supreme Court ruled by a 5–4 vote in National Federation of Independent Business v. Sebelius that the mandate was constitutional under the U.S. Congress's taxing authority.
[303] In Burwell v. Hobby Lobby the Court ruled that "closely-held" for-profit corporations could be exempt on religious grounds under the Religious Freedom Restoration Act from regulations adopted under the ACA that would have required them to pay for insurance that covered certain contraceptives.
In June 2015, the Court ruled 6–3 in King v. Burwell that subsidies to help individuals and families purchase health insurance were authorized for those doing so on both the federal exchange and state exchanges, not only those purchasing plans "established by the State", as the statute reads.
Prior to June 2014, Obama offered substantial support for a broadly-based "All of the above" approach to domestic energy policy, which Obama has maintained since his first term and which he last confirmed at his State of the Union speech in January 2014 to a mixed reception by both parties.
In June 2014, Obama made indications that his administration would consider a shift towards an energy policy more closely tuned to the manufacturing industry and its impact on the domestic economy.
[305] Obama's approach of selectively combining regulation and incentive to various issues in the domestic energy policy, such as coal mining and oil fracking, has received mixed commentary for not being as responsive to the needs of the domestic manufacturing sector as needed, following claims that the domestic manufacturing sector utilizes as much as a third of the nation's available energy resources.
On January 16, 2013, one month after the Sandy Hook Elementary School shooting, Obama signed 23 executive orders and outlined a series of sweeping proposals regarding gun control.
[308] He urged Congress to reintroduce an expired ban on military-style assault weapons, such as those used in several recent mass shootings, impose limits on ammunition magazines to 10 rounds, introduce background checks on all gun sales, pass a ban on possession and sale of armor-piercing bullets, introduce harsher penalties for gun-traffickers, especially unlicensed dealers who buy arms for criminals and approving the appointment of the head of the federal Bureau of Alcohol, Tobacco, Firearms and Explosives for the first time since 2006.
[309] On January 5, 2016, Obama announced new executive actions extending background check requirements to more gun sellers.
[310] In a 2016 editorial in the New York Times, Obama compared the struggle for what he termed "common-sense gun reform" to women's suffrage and other civil rights movements in American history.
Obama called the November 2, 2010 election, where the Democratic Party lost 63 seats in, and control of, the House of Representatives,[312] "humbling" and a "shellacking".
[313] He said that the results came because not enough Americans had felt the effects of the economic recovery.
On November 10, 2014, President Obama recommended the Federal Communications Commission reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality.
[315][316] On February 12, 2013, President Obama signed Executive Order 13636, "Improving Critical Infrastructure Cybersecurity.
In February and March 2009, Vice President Joe Biden and Secretary of State Hillary Clinton made separate overseas trips to announce a "new era" in U.S. foreign relations with Russia and Europe, using the terms "break" and "reset" to signal major changes from the policies of the preceding administration.
[318] Obama attempted to reach out to Arab leaders by granting his first interview to an Arab satellite TV network, Al Arabiya.
On March 19, Obama continued his outreach to the Muslim world, releasing a New Year's video message to the people and government of Iran.
[320][321] In April, Obama gave a speech in Ankara, Turkey, which was well received by many Arab governments.
[322] On June 4, 2009, Obama delivered a speech at Cairo University in Egypt calling for "A New Beginning" in relations between the Islamic world and the United States and promoting Middle East peace.
On June 26, 2009, Obama responded to the Iranian government's actions towards protesters following Iran's 2009 presidential election by saying: "The violence perpetrated against them is outrageous.
We see it and we condemn it.
"[324] While in Moscow on July 7, he responded to Vice President Biden's comment on a possible Israeli military strike on Iran by saying: "We have said directly to the Israelis that it is important to try and resolve this in an international setting in a way that does not create major conflict in the Middle East.
On September 24, 2009, Obama became the first sitting U.S. President to preside over a meeting of the United Nations Security Council.
In March 2010, Obama took a public stance against plans by the government of Israeli Prime Minister Benjamin Netanyahu to continue building Jewish housing projects in predominantly Arab neighborhoods of East Jerusalem.
[327][328] During the same month, an agreement was reached with the administration of Russian President Dmitry Medvedev to replace the 1991 Strategic Arms Reduction Treaty with a new pact reducing the number of long-range nuclear weapons in the arsenals of both countries by about one-third.
[329] Obama and Medvedev signed the New START treaty in April 2010, and the U.S. Senate ratified it in December 2010.
In December 2011, Obama instructed agencies to consider LGBT rights when issuing financial aid to foreign countries.
[331] In August 2013, he criticized Russia's law that discriminated against gays,[332] but he stopped short of advocating a boycott of the upcoming 2014 Winter Olympics in Sochi, Russia.
In December 2014, Obama announced that he intended to normalize relationships between Cuba and the United States.
[334] The countries' respective "interests sections" in one another's capitals were upgraded to embassies on July 20, 2015.
In March 2015, Obama declared that he had authorized U.S. forces to provide logistical and intelligence support to the Saudis in their military intervention in Yemen, establishing a "Joint Planning Cell" with Saudi Arabia.
[335][336] In 2016, the Obama administration proposed a series of arms deals with Saudi Arabia worth $115 billion.
[337] Obama halted the sale of guided munition technology to Saudi Arabia after Saudi warplanes targeted a funeral in Yemen's capital Sanaa, killing more than 140 people.
Before leaving office, Obama said German Chancellor Angela Merkel had been his "closest international partner" throughout his tenure as president.
On February 27, 2009, Obama announced that combat operations in Iraq would end within 18 months.
His remarks were made to a group of Marines preparing for deployment to Afghanistan.
Obama said, "Let me say this as plainly as I can: by August 31, 2010, our combat mission in Iraq will end.
"[340] The Obama administration scheduled the withdrawal of combat troops to be completed by August 2010, decreasing troop's levels from 142,000 while leaving a transitional force of about 50,000 in Iraq until the end of 2011.
On August 19, 2010, the last U.S. combat brigade exited Iraq.
Remaining troops transitioned from combat operations to counter-terrorism and the training, equipping, and advising of Iraqi security forces.
[341][342] On August 31, 2010, Obama announced that the United States combat mission in Iraq was over.
[343] On October 21, 2011 President Obama announced that all U.S. troops would leave Iraq in time to be "home for the holidays".
In June 2014, following the capture of Mosul by ISIS, Obama sent 275 troops to provide support and security for U.S. personnel and the U.S. Embassy in Baghdad.
ISIS continued to gain ground and to commit widespread massacres and ethnic cleansing.
In August 2014, during the Sinjar massacre, Obama ordered a campaign of U.S. airstrikes against ISIS.
By the end of 2014, 3,100 American ground troops were committed to the conflict[348] and 16,000 sorties were flown over the battlefield, primarily by U.S. Air Force and Navy pilots.
In early 2015, with the addition of the "Panther Brigade" of the 82nd Airborne Division the number of U.S. ground troops in Iraq surged to 4,400,[350] and by July American-led coalition air forces counted 44,000 sorties over the battlefield.
Early in his presidency, Obama moved to bolster U.S. troop strength in Afghanistan.
[352] He announced an increase in U.S. troop levels to 17,000 military personnel in February 2009 to "stabilize a deteriorating situation in Afghanistan", an area he said had not received the "strategic attention, direction and resources it urgently requires".
[353] He replaced the military commander in Afghanistan, General David D. McKiernan, with former Special Forces commander Lt. Gen. Stanley A. McChrystal in May 2009, indicating that McChrystal's Special Forces experience would facilitate the use of counterinsurgency tactics in the war.
[354] On December 1, 2009, Obama announced the deployment of an additional 30,000 military personnel to Afghanistan and proposed to begin troop withdrawals 18 months from that date;[355] this took place in July 2011.
David Petraeus replaced McChrystal in June 2010, after McChrystal's staff criticized White House personnel in a magazine article.
[356] In February 2013, Obama said the U.S. military would reduce the troop level in Afghanistan from 68,000 to 34,000 U.S. troops by February 2014.
In October 2015, the White House announced a plan to keep U.S.
Forces in Afghanistan indefinitely in light of the deteriorating security situation.
In 2011, the United States vetoed a Security Council resolution condemning Israeli settlements, with the United States being the only nation to do so.
[359] Obama supports the two-state solution to the Arab–Israeli conflict based on the 1967 borders with land swaps.
In June 2011, Obama said that the bond between the United States and Israel is "unbreakable".
[361] During the initial years of the Obama administration, the U.S. increased military cooperation with Israel, including increased military aid, re-establishment of the U.S.-Israeli Joint Political Military Group and the Defense Policy Advisory Group, and an increase in visits among high-level military officials of both countries.
[362] The Obama administration asked Congress to allocate money toward funding the Iron Dome program in response to the waves of Palestinian rocket attacks on Israel.
In 2013, Jeffrey Goldberg reported that, in Obama's view, "with each new settlement announcement, Netanyahu is moving his country down a path toward near-total isolation.
"[364] In 2014, Obama likened the Zionist movement to the Civil Rights Movement in the United States.
He said that both movements seek to bring justice and equal rights to historically persecuted peoples.
He explained, "To me, being pro-Israel and pro-Jewish is part and parcel with the values that I've been fighting for since I was politically conscious and started getting involved in politics.
"[365] Obama expressed support for Israel's right to defend itself during the 2014 Israel–Gaza conflict.
[366] In 2015, Obama was harshly criticized by Israel for advocating and signing the Iran Nuclear Deal; Israeli Prime Minister Benjamin Netanyahu, who had advocated the U.S. congress to oppose it, said the deal was "dangerous" and "bad".
On December 23, 2016, under the Obama Administration, the United States abstained from United Nations Security Council Resolution 2334, which condemned Israeli settlement building in the occupied Palestinian territories as a violation of international law, effectively allowing it to pass.
[368] Netanyahu strongly criticized the Obama Administration's actions,[369][370] and the Israeli government withdrew its annual dues from the organization, which totaled $6 million, on January 6, 2017.
[371] On January 5, 2017, the United States House of Representatives voted 342–80 to condemn the UN Resolution.
In February 2011, protests in Libya began against long-time dictator Muammar Gaddafi as part of the Arab Spring.
They soon turned violent.
In March, as forces loyal to Gaddafi advanced on rebels across Libya, calls for a no-fly zone came from around the world, including Europe, the Arab League, and a resolution[374] passed unanimously by the U.S.
[375] In response to the unanimous passage of United Nations Security Council Resolution 1973 on March 17, Gaddafi—who had previously vowed to "show no mercy" to the rebels of Benghazi[376]—announced an immediate cessation of military activities,[377] yet reports came in that his forces continued shelling Misrata.
The next day, on Obama's orders, the U.S. military took part in air strikes to destroy the Libyan government's air defense capabilities to protect civilians and enforce a no-fly-zone,[378] including the use of Tomahawk missiles, B-2 Spirits, and fighter jets.
[379][380][381] Six days later, on March 25, by unanimous vote of all of its 28 members, NATO took over leadership of the effort, dubbed Operation Unified Protector.
[382] Some Representatives[383] questioned whether Obama had the constitutional authority to order military action in addition to questioning its cost, structure and aftermath.
On August 18, 2011, several months after the start of the Syrian Civil War, Obama issued a written statement that said: "The time has come for President Assad to step aside.
"[386][387] This stance was reaffirmed in November 2015.
[388] In 2012, Obama authorized multiple programs run by the CIA and the Pentagon to train anti-Assad rebels.
[389] The Pentagon-run program was later found to have failed and was formally abandoned in October 2015.
In the wake of a chemical weapons attack in Syria, formally blamed by the Obama administration on the Assad government, Obama chose not to enforce the "red line" he had pledged[392] and, rather than authorize the promised military action against Assad, went along with the Russia-brokered deal that led to Assad giving up chemical weapons; however attacks with chlorine gas continued.
[393][394] In 2014, Obama authorized an air campaign aimed primarily at ISIL, but repeatedly promised that the U.S. would not deploy ground troops in Syria.
Starting with information received from Central Intelligence Agency operatives in July 2010, the CIA developed intelligence over the next several months that determined what they believed to be the hideout of Osama bin Laden.
He was living in seclusion in a large compound in Abbottabad, Pakistan, a suburban area 35 miles (56 km) from Islamabad.
[397] CIA head Leon Panetta reported this intelligence to President Obama in March 2011.
[397] Meeting with his national security advisers over the course of the next six weeks, Obama rejected a plan to bomb the compound, and authorized a "surgical raid" to be conducted by United States Navy SEALs.
[397] The operation took place on May 1, 2011, and resulted in the shooting death of bin Laden and the seizure of papers, computer drives and disks from the compound.
[398][399] DNA testing was one of five methods used to positively identify bin Laden's corpse,[400] which was buried at sea several hours later.
[401] Within minutes of the President's announcement from Washington, DC, late in the evening on May 1, there were spontaneous celebrations around the country as crowds gathered outside the White House, and at New York City's Ground Zero and Times Square.
[398][402] Reaction to the announcement was positive across party lines, including from former presidents Bill Clinton and George W.
In November 2013, the Obama administration opened negotiations with Iran to prevent it from acquiring nuclear weapons, which included an interim agreement.
Negotiations took two years with numerous delays, with a deal being announced July 14, 2015.
The deal, titled the "Joint Comprehensive Plan of Action", saw the removal of sanctions in exchange for measures that would prevent Iran from producing nuclear weapons.
While Obama hailed the agreement as being a step towards a more hopeful world, the deal drew strong criticism from Republican and conservative quarters, and from Israeli prime minister Benjamin Netanyahu.
[404][405][406] In order to advance the deal, the Obama administration shielded Hezbollah from the Drug Enforcement Administration's Project Cassandra investigation regarding drug smuggling and from the Central Intelligence Agency.
Since the spring of 2013, secret meetings were conducted between the United States and Cuba in the neutral locations of Canada and Vatican City.
[409] The Vatican first became involved in 2013 when Pope Francis advised the U.S. and Cuba to exchange prisoners as a gesture of goodwill.
[410] On December 10, 2013, Cuban President Raúl Castro, in a significant public moment, greeted and shook hands with Obama at the Nelson Mandela memorial service in Johannesburg.
In December 2014, after the secret meetings, it was announced that Obama, with Pope Francis as an intermediary, had negotiated a restoration of relations with Cuba, after nearly sixty years of détente.
[412] Popularly dubbed the Cuban Thaw, The New Republic deemed the Cuban Thaw to be "Obama's finest foreign policy achievement.
"[413] On July 1, 2015, President Barack Obama announced that formal diplomatic relations between Cuba and the United States would resume, and embassies would be opened in Washington and Havana.
[414] The countries' respective "interests sections" in one another's capitals were upgraded to embassies on July 20 and August 13, 2015, respectively.
Obama visited Havana, Cuba for two days in March 2016, becoming the first sitting U.S. President to arrive since Calvin Coolidge in 1928.
Obama spoke in front of the African Union in Addis Ababa, Ethiopia, on July 29, 2015, the first sitting U.S. president to do so.
He gave a speech encouraging the world to increase economic ties via investments and trade with the continent, and lauded the progress made in education, infrastructure, and economy.
He also criticized the lack of democracy and leaders who refuse to step aside, discrimination against minorities (LGBT people, religious groups and ethnicities), and corruption.
He suggested an intensified democratization and free trade, to significantly improve the quality of life for Africans.
[417][418] During his July 2015 trip, Obama also was the first U.S. president ever to visit Kenya, which is the homeland of his father.
On May 27, 2016, Obama became the first sitting American president to visit Hiroshima, Japan, 71 years after the U.S. atomic bombing of Hiroshima that ended World War II.
Accompanied by Japanese Prime Minister Shinzō Abe, Obama paid tribute to the victims of the bombing at the Hiroshima Peace Memorial Museum.
After Russia's invasion of Crimea in 2014, military intervention in Syria in 2015, and the interference in the 2016 U.S. presidential election,[421] Obama's Russia policy was widely seen as a failure.
[422] George Robertson, a former UK defense secretary and NATO secretary-general, said that Obama had "allowed Putin to jump back on the world stage and test the resolve of the West", adding that the legacy of this disaster would last.
Obama's family history, upbringing, and Ivy League education differ markedly from those of African-American politicians who launched their careers in the 1960s through participation in the civil rights movement.
[424] Expressing puzzlement over questions about whether he is "black enough", Obama told an August 2007 meeting of the National Association of Black Journalists that "we're still locked in this notion that if you appeal to white folks then there must be something wrong.
"[425] Obama acknowledged his youthful image in an October 2007 campaign speech, saying: "I wouldn't be here if, time and again, the torch had not been passed to a new generation.
Obama is frequently referred to as an exceptional orator.
[427] During his pre-inauguration transition period and continuing into his presidency, Obama delivered a series of weekly Internet video addresses.
[428] Former presidential campaign surrogate and Georgetown professor, Michael Eric Dyson, is both critical and sympathetic of President Obama's leadership in race relations, indicating that Obama's speeches and action on racial disparity and justice have been somewhat reactive and reluctant when, especially in the later part of his second term, racial violence demanded immediate presidential action and conversation.
According to the Gallup Organization, Obama began his presidency with a 68% approval rating[430] before gradually declining for the rest of the year, and eventually bottoming out at 41% in August 2010,[431] a trend similar to Ronald Reagan's and Bill Clinton's first years in office.
[432] He experienced a small poll bounce shortly after the death of Osama bin Laden on May 2, 2011.
This bounce lasted until around June 2011, when his approval numbers dropped back to where they were previously.
[433][434] His approval ratings rebounded around the same time as his reelection in 2012, with polls showing an average job approval of 52% shortly after his second inauguration.
[435] Despite approval ratings dropping to 39% in late-2013 due to the ACA roll-out, they climbed to 50% in January 2015 according to Gallup.
Polls showed strong support for Obama in other countries both before and during his presidency.
[437][438] In a February 2009 poll conducted in Western Europe and the U.S. by Harris Interactive for France 24 and the International Herald Tribune, Obama was rated as the most respected world leader, as well as the most powerful.
[439] In a similar poll conducted by Harris in May 2009, Obama was rated as the most popular world leader, as well as the one figure most people would pin their hopes on for pulling the world out of the economic downturn.
Obama won Best Spoken Word Album Grammy Awards for abridged audiobook versions of Dreams from My Father in February 2006 and for The Audacity of Hope in February 2008.
[442] His concession speech after the New Hampshire primary was set to music by independent artists as the music video "Yes We Can", which was viewed 10 million times on YouTube in its first month[443] and received a Daytime Emmy Award.
[444] In December 2008 and in 2012, Time magazine named Obama as its Person of the Year.
[445] The 2008 awarding was for his historic candidacy and election, which Time described as "the steady march of seemingly impossible accomplishments".
[446] On May 25, 2011, Obama became the first President of the United States to address both houses of the UK Parliament in Westminster Hall, London.
This was only the fifth occurrence since the start of the 20th century of a head of state being extended this invitation, following Charles de Gaulle in 1960, Nelson Mandela in 1996, Queen Elizabeth II in 2002 and Pope Benedict XVI in 2010.
On October 9, 2009, the Norwegian Nobel Committee announced that Obama had won the 2009 Nobel Peace Prize "for his extraordinary efforts to strengthen international diplomacy and cooperation between peoples".
[449] Obama accepted this award in Oslo, Norway on December 10, 2009, with "deep gratitude and great humility.
"[450] The award drew a mixture of praise and criticism from world leaders and media figures.
[451][452][453][454] Obama's peace prize was called a "stunning surprise" by The New York Times.
[455] Obama is the fourth U.S. president to be awarded the Nobel Peace Prize and the third to become a Nobel laureate while in office.
[456] Obama's Nobel Prize has been viewed skeptically in subsequent years, especially after the director of the Nobel Institute, Geir Lundestad, said Obama's Peace Prize did not have the desired effect.
Barack Obama's presidency ended at noon on January 20, 2017, immediately following the inauguration of his Republican successor, Donald Trump.
After the inauguration, Obama lifted off on Executive One, circled the White House, and flew to Joint Base Andrews.
[458] The family currently rents a house in Kalorama, Washington, D.C.[459]
A 2018 survey of historians by the American Political Science Association ranked Obama the 8th-greatest American President.
[4] Obama gained 10 spots from the same survey in 2015 from the Brookings Institution that ranked Obama the 18th-greatest American President.
On March 2, 2017, the John F. Kennedy Presidential Library and Museum awarded the annual Profile in Courage Award to Obama "for his enduring commitment to democratic ideals and elevating the standard of political courage.
"[461] In his first public appearance out of office, Obama appeared at a seminar at the University of Chicago on April 24.
The seminar was aimed at the engagement with a new generation as well as an appeal for their participation in politics.
[462] On May 4, three days ahead of the French presidential election, Obama publicly endorsed Emmanuel Macron: "He appeals to people's hopes and not their fears, and I enjoyed speaking to Emmanuel recently to hear about his independent movement and his vision for the future of France.
"[463] Macron went on to win the election.
While in Berlin on May 25, Obama made a joint public appearance with Chancellor Angela Merkel where he stressed inclusion and for leaders to question themselves, Obama having been formally invited to Berlin while still in office as part of an effort to boost Merkel's re-election campaign.
[464] Obama traveled to Kensington Palace in England and met with Prince Harry on May 27, 2017; Obama tweeted afterward that the two discussed their foundations and offering condolences in the wake of the Manchester Arena bombing that occurred five days prior.
Barack and Michelle Obama signed a deal on May 22 to produce docu-series, documentaries and features for Netflix under the Obama's newly formed production company, Higher Ground Productions.
On the deal, Michelle said "I have always believed in the power of storytelling to inspire us, to make us think differently about the world around us, and to help us open our minds and hearts to others".
After President Trump announced his withdrawal of the United States from the Paris Agreement on June 1, Obama released a statement disagreeing with the choice: "But even in the absence of American leadership; even as this administration joins a small handful of nations that reject the future; I'm confident that our states, cities, and businesses will step up and do even more to lead the way, and help protect for future generations the one planet we've got.
After Senate Republicans revealed the Better Care Reconciliation Act of 2017, their discussion draft of a health care bill to replace the Affordable Care Act, on June 22, Obama released a Facebook post calling the bill "a massive transfer of wealth from middle-class and poor families to the richest people in America.
"[469] On September 19, while delivering the keynote address at Goalkeepers, Obama admitted his frustration with Republicans backing "a bill that will raise costs, reduce coverage, and roll back protections for older Americans and people with pre-existing conditions".
After Attorney General Jeff Sessions announced the termination of the Deferred Action for Childhood Arrivals (DACA) program on September 5, Obama released a Facebook post rebuking the decision.
[471] Two days later, he partnered with former presidents Jimmy Carter, George H. W. Bush, Bill Clinton, and George W. Bush to work with One America Appeal to help the victims of Hurricane Harvey and Hurricane Irma in the Gulf Coast and Texas communities.
Obama hosted the inaugural summit of the Obama Foundation in Chicago on October 31.
Obama intends for the foundation to be the central focus of his post-presidency and part of his ambitions for his subsequent activities following his presidency to be more consequential than his time in office.
[473] Obama has also been working on a Presidential memoir, in a reported $65 million deal with Penguin Random House.
Obama went on an international trip from November 28 to December 2, 2017, and visited China, India and France.
In China, he delivered remarks at the Global Alliance of SMEs Summit in Shanghai and met with Chinese President Xi Jinping in Beijing.
[475][476] He then went to India, where he spoke at the Hindustan Times Leadership Summit before meeting with Indian Prime Minister Narendra Modi over lunch.
In addition, he held a town hall for young leaders, organized by the Obama Foundation.
[477][478] He also met with the Dalai Lama while in New Delhi.
[479] He ended his five-day trip in France where he met with French President Emmanuel Macron, former President Francois Hollande and Paris Mayor Anne Hidalgo and then spoke at an invitation-only event, touching on climate issues.
Obama endorsed Oregon Governor Kate Brown for re-election on October 1, 2018.
[481] On October 22, Obama traveled to Las Vegas to speak at a campaign rally to discuss economic growth, campaigning for Nevada Democratic candidates.
A package that contained a pipe bomb was sent to Obama's home in Washington, D.C, on October 24, 2018.
The package was intercepted by the Secret Service during routine mail screenings.
Similar packages were sent to several other Democratic leaders, mostly those who voiced strong objections to the policies of Donald Trump and to CNN.
Debbie Wasserman Schultz was addressed as the sender of the package.
On October 26, 2018, Cesar Sayoc was arrested and faces five federal charges in Manhattan carrying a combined maximum sentence of 48 years behind bars in relation to the pipe bombs.
Obama's most significant legacy is generally considered to be the Patient Protection and Affordable Care Act (PPACA), provisions of which went into effect from 2010 to 2020.
Many attempts by Senate Republicans to repeal the PPACA, including a "skinny repeal", have thus far failed.
[485] Together with the Health Care and Education Reconciliation Act amendment, it represents the U.S. healthcare system's most significant regulatory overhaul and expansion of coverage since the passage of Medicare and Medicaid in 1965.
Many commentators credit Obama with averting a threatened depression and pulling the economy back from the Great Recession.
[485] According to the U.S. Bureau of Labor Statistics, the Obama administration created 11.3 million jobs from the month after his first inauguration to the end of his term.
[490] In 2010, Obama signed into effect the Dodd–Frank Wall Street Reform and Consumer Protection Act.
Passed as a response to the financial crisis of 2007–08, it brought the most significant changes to financial regulation in the United States since the regulatory reform that followed the Great Depression under Democratic President Franklin D.
In 2009, Obama signed into law the National Defense Authorization Act for Fiscal Year 2010, which contained in it the Matthew Shepard and James Byrd Jr.
Hate Crimes Prevention Act, the first addition to existing federal hate crime law in the United States since Democratic President Bill Clinton signed into law the Church Arson Prevention Act of 1996.
The Matthew Shepard and James Byrd Jr.
Hate Crimes Prevention Act expanded existing federal hate crime laws in the United States to apply to crimes motivated by a victim's actual or perceived gender, sexual orientation, gender identity, or disability, and dropped the prerequisite that the victim be engaging in a federally protected activity.
As president, Obama advanced LGBT rights.
[492] In 2010, he signed the Don't Ask, Don't Tell Repeal Act, which brought an end to "don't ask, don't tell" policy in the U.S. armed forces that banned open service from LGB people; the law went into effect the following year.
[493] In 2016, his administration brought an end to the ban on transgender people serving openly in the U.S. armed forces.
[494][218] A Gallup poll, taken in the final days of Obama's term, showed that 68% of Americans believed that the U.S. had made progress in the situation for gays and lesbians during Obama's eight years in office.
Obama substantially escalated the use of drone strikes against suspected militants and terrorists associated with al-Qaeda and the Taliban.
[496][497] In 2016, the last year of his presidency, the US dropped 26,171 bombs on seven different countries.
[498][499] Obama left about 8,400 US troops in Afghanistan, 5,262 in Iraq, 503 in Syria, 133 in Pakistan, 106 in Somalia, 7 in Yemen, and 2 in Libya at the end of his presidency.
According to Pew Research Center and United States Bureau of Justice Statistics, from December 31, 2009 to December 31, 2015, that inmates sentenced in US federal custody declined by 5% under Obama.
This is the largest decline in sentenced inmates in US federal custody since Democratic President Jimmy Carter.
By contrast, the federal prison population increased significantly under presidents Ronald Reagan, George H. W. Bush, Bill Clinton, and George W.
Obama left office in January 2017 with a 60% approval rating.
[503][504] A 2017 C-SPAN Presidential Historians Survey ranked Obama as the 12th-best US president.
The Barack Obama Presidential Center is Obama's planned presidential library.
It will be hosted by the University of Chicago and located in Jackson Park on the South Side of Chicago.
The September 11 attacks (also referred to as 9/11)[a] were a series of four coordinated terrorist attacks by the Islamic terrorist group al-Qaeda[2][3][4] against the United States on the morning of Tuesday, September 11, 2001.
The attacks killed 2,996 people, injured over 6,000 others, and caused at least $10 billion in infrastructure and property damage.
[5][6] Additional people died of 9/11-related cancer and respiratory diseases in the months and years following the attacks.
Four passenger airliners operated by two major U.S. passenger air carriers (United Airlines and American Airlines)—all of which departed from airports in the northeastern United States bound for California—were hijacked by 19 al-Qaeda terrorists.
Two of the planes, American Airlines Flight 11 and United Airlines Flight 175, were crashed into the North and South towers, respectively, of the World Trade Center complex in Lower Manhattan.
Within an hour and 42 minutes, both 110-story towers collapsed.
Debris and the resulting fires caused a partial or complete collapse of all other buildings in the World Trade Center complex, including the 47-story 7 World Trade Center tower, as well as significant damage to ten other large surrounding structures.
A third plane, American Airlines Flight 77, was crashed into the Pentagon (the headquarters of the U.S. Department of Defense) in Arlington County, Virginia, which led to a partial collapse of the building's west side.
The fourth plane, United Airlines Flight 93, was initially flown toward Washington, D.C., but crashed into a field in Stonycreek Township near Shanksville, Pennsylvania, after its passengers thwarted the hijackers.
9/11 is the single deadliest terrorist attack in human history and the single deadliest incident for firefighters and law enforcement officers[7] in the history of the United States, with 343 and 72 killed, respectively.
Suspicion quickly fell on al-Qaeda.
The United States responded by launching the War on Terror and invaded Afghanistan to depose the Taliban, which had failed to comply with U.S. demands to extradite Osama bin Laden and expel al-Qaeda from Afghanistan.
Many countries strengthened their anti-terrorism legislation and expanded the powers of law enforcement and intelligence agencies to prevent terrorist attacks.
Although Osama bin Laden, al-Qaeda's leader, initially denied any involvement, in 2004 he claimed responsibility for the attacks.
[1] Al-Qaeda and bin Laden cited U.S. support of Israel, the presence of U.S. troops in Saudi Arabia, and sanctions against Iraq as motives.
After evading capture for almost a decade, bin Laden was located in Pakistan and killed by SEAL Team Six of the U.S. Navy in May 2011.
The destruction of the World Trade Center and nearby infrastructure seriously harmed the economy of Lower Manhattan and had a significant effect on global markets, which resulted in the closing of Wall Street until September 17 and the civilian airspace in the U.S. and Canada until September 13.
Many closings, evacuations, and cancellations followed, out of respect or fear of further attacks.
Cleanup of the World Trade Center site was completed in May 2002, and the Pentagon was repaired within a year.
On November 18, 2006, construction of One World Trade Center began at the World Trade Center site.
The building was officially opened on November 3, 2014.
[8][9] Numerous memorials have been constructed, including the National September 11 Memorial & Museum in New York City, the Pentagon Memorial in Arlington County, Virginia, and the Flight 93 National Memorial in a field in Stonycreek Township near Shanksville, Pennsylvania.
Although not confirmed, there is evidence of alleged Saudi Arabian involvement in the attacks.
[10] Given as main evidence in these charges are the contents of the 28 redacted pages of the December 2002 Joint Inquiry into Intelligence Community Activities before and after the Terrorist Attacks of September 11, 2001 conducted by the Senate Select Committee on Intelligence and the House Permanent Select Committee on Intelligence.
These 28 pages contain information regarding the material and financial assistance given to the hijackers and their affiliates leading up to the attacks by the Saudi Arabian government.
The origins of al-Qaeda can be traced to 1979 when the Soviet Union invaded Afghanistan.
Osama bin Laden traveled to Afghanistan and helped organize Arab mujahideen to resist the Soviets.
[12] Under the guidance of Ayman al-Zawahiri, bin Laden became more radical.
[13] In 1996, bin Laden issued his first fatwā, calling for American soldiers to leave Saudi Arabia.
In a second fatwā in 1998, bin Laden outlined his objections to American foreign policy with respect to Israel, as well as the continued presence of American troops in Saudi Arabia after the Gulf War.
[15] Bin Laden used Islamic texts to exhort Muslims to attack Americans until the stated grievances are reversed.
Muslim legal scholars "have throughout Islamic history unanimously agreed that the jihad is an individual duty if the enemy destroys the Muslim countries", according to bin Laden.
Bin Laden orchestrated the attacks and initially denied involvement but later recanted his false statements.
[1][16][17] Al Jazeera broadcast a statement by bin Laden on September 16, 2001, stating, "I stress that I have not carried out this act, which appears to have been carried out by individuals with their own motivation.
"[18] In November 2001, U.S. forces recovered a videotape from a destroyed house in Jalalabad, Afghanistan.
In the video, bin Laden is seen talking to Khaled al-Harbi and admits foreknowledge of the attacks.
[19] On December 27, 2001, a second bin Laden video was released.
In the video, he said:
It has become clear that the West in general and America in particular have an unspeakable hatred for Islam.
It is the hatred of crusaders.
Terrorism against America deserves to be praised because it was a response to injustice, aimed at forcing America to stop its support for Israel, which kills our people.
... We say that the end of the United States is imminent, whether Bin Laden or his followers are alive or dead, for the awakening of the Muslim umma (nation) has occurred
but he stopped short of admitting responsibility for the attacks.
[20] The transcript refers several times to the United States specifically targeting Muslims.
Shortly before the U.S. presidential election in 2004, bin Laden used a taped statement to publicly acknowledge al-Qaeda's involvement in the attacks on the United States.
He admitted his direct link to the attacks and said they were carried out because:
we are free ... and want to regain freedom for our nation.
As you undermine our security, we undermine yours.
 Bin Laden said he had personally directed his followers to attack the World Trade Center and the Pentagon.
[22][23] Another video obtained by Al Jazeera in September 2006 shows bin Laden with Ramzi bin al-Shibh, as well as two hijackers, Hamza al-Ghamdi and Wail al-Shehri, as they make preparations for the attacks.
[24] The U.S. never formally indicted bin Laden for the 9/11 attacks, but he was on the FBI's Most Wanted List for the bombings of the U.S. Embassies in Dar es Salaam, Tanzania, and Nairobi, Kenya.
[25][26] After a 10-year manhunt, bin Laden was killed by American special forces in a compound in Abbottabad, Pakistan on May 2, 2011.
Journalist Yosri Fouda of the Arabic television channel Al Jazeera reported that in April 2002 Khalid Sheikh Mohammed admitted his involvement in the attacks, along with Ramzi bin al-Shibh.
[29][30][31] The 9/11 Commission Report determined that the animosity towards the United States felt by Mohammed, the principal architect of the 9/11 attacks, stemmed from his "violent disagreement with U.S. foreign policy favoring Israel".
[32] Mohammed was also an adviser and financier of the 1993 World Trade Center bombing and the uncle of Ramzi Yousef, the lead bomber in that attack.
Mohammed was arrested on March 1, 2003, in Rawalpindi, Pakistan, by Pakistani security officials working with the CIA.
He was then held at multiple CIA secret prisons and Guantanamo Bay where he was interrogated and tortured with methods including waterboarding.
[35][36][37] During U.S. hearings at Guantanamo Bay in March 2007, Mohammed again confessed his responsibility for the attacks, stating he "was responsible for the 9/11 operation from A to Z" and that his statement was not made under duress.
In "Substitution for Testimony of Khalid Sheikh Mohammed" from the trial of Zacarias Moussaoui, five people are identified as having been completely aware of the operation's details.
They are bin Laden, Khalid Sheikh Mohammed, Ramzi bin al-Shibh, Abu Turab al-Urduni, and Mohammed Atef.
[39] To date, only peripheral figures have been tried or convicted for the attacks.
On September 26, 2005, the Spanish high court sentenced Abu Dahdah to 27 years in prison for conspiracy on the 9/11 attacks and being a member of the terrorist organization al-Qaeda.
At the same time, another 17 al-Qaeda members were sentenced to penalties of between six and eleven years.
[40] On February 16, 2006, the Spanish Supreme Court reduced the Abu Dahdah penalty to 12 years because it considered that his participation in the conspiracy was not proven.
Also in 2006, Moussaoui—who some originally suspected might have been the assigned 20th hijacker—was convicted for the lesser role of conspiracy to commit acts of terrorism and air piracy.
He was sentenced to life without parole in the United States.
[42][43] Mounir el-Motassadeq, an associate of the Hamburg-based hijackers, served 15 years in Germany for his role in helping the hijackers prepare for the attacks.
He was released in October 2018, and deported to Morocco.
The Hamburg cell in Germany included radical Islamists who eventually came to be key operatives in the 9/11 attacks.
[45] Mohamed Atta, Marwan al-Shehhi, Ziad Jarrah, Ramzi bin al-Shibh, and Said Bahaji were all members of al-Qaeda's Hamburg cell.
Osama bin Laden's declaration of a holy war against the United States, and a 1998 fatwā signed by bin Laden and others, calling for the killing of Americans,[15] are seen by investigators as evidence of his motivation.
[47] In bin Laden's November 2002 "Letter to America", he explicitly stated that al-Qaeda's motives for their attacks include:
After the attacks, bin Laden and al-Zawahiri released additional videotapes and audio recordings, some of which repeated those reasons for the attacks.
Two particularly important publications were bin Laden's 2002 "Letter to America",[51] and a 2004 videotape by bin Laden.
Bin Laden interpreted Muhammad as having banned the "permanent presence of infidels in Arabia".
[53] In 1996, bin Laden issued a fatwā calling for American troops to leave Saudi Arabia.
In 1998, al-Qaeda wrote, "for over seven years the United States has been occupying the lands of Islam in the holiest of places, the Arabian Peninsula, plundering its riches, dictating to its rulers, humiliating its people, terrorizing its neighbors, and turning its bases in the Peninsula into a spearhead through which to fight the neighboring Muslim peoples.
In a December 1999 interview, bin Laden said he felt that Americans were "too near to Mecca", and considered this a provocation to the entire Muslim world.
[55] One analysis of suicide terrorism suggested that without U.S. troops in Saudi Arabia, al-Qaeda likely would not have been able to get people to commit to suicide missions.
In the 1998 fatwā, al-Qaeda identified the Iraq sanctions as a reason to kill Americans, condemning the "protracted blockade"[54] among other actions that constitute a declaration of war against "Allah, his messenger, and Muslims.
"[54] The fatwā declared that "the ruling to kill the Americans and their allies – civilians and military – is an individual duty for every Muslim who can do it in any country in which it is possible to do it, in order to liberate the al-Aqsa Mosque and the holy mosque of Mecca from their grip, and in order for their [the Americans'] armies to move out of all the lands of Islam, defeated and unable to threaten any Muslim.
In 2004, Bin Laden claimed that the idea of destroying the towers had first occurred to him in 1982, when he witnessed Israel's bombardment of high-rise apartment buildings during the 1982 Lebanon War.
[58][59] Some analysts, including Mearsheimer and Walt, also claimed that U.S. support of Israel was one motive for the attacks.
[49][55] In 2004 and 2010, bin Laden again connected the September 11 attacks with U.S. support of Israel, although most of the letter expressed bin Laden's disdain for President Bush and bin Laden's hope to "destroy and bankrupt" the U.S.[60][61]
Other motives have been suggested in addition to those stated by bin Laden and al-Qaeda, including western support of Islamic and non-Islamic authoritarian regimes in Saudi Arabia, Iran, Egypt, Iraq, Pakistan and northern Africa, and the presence of western troops in some of these countries.
[62][page needed] Some authors suggested the "humiliation" that resulted from the Islamic world falling behind the Western world – this discrepancy was rendered especially visible by the globalization trend[63][64] and a desire to provoke the U.S. into a broader war against the Islamic world in the hope of motivating more allies to support al-Qaeda.
Similarly, others have argued that 9/11 was a strategic move with the objective of provoking America into a war that would incite a pan-Islamic revolution.
The idea for the attacks came from Khalid Sheikh Mohammed, who first presented it to Osama bin Laden in 1996.
[67] At that time, bin Laden and al-Qaeda were in a period of transition, having just relocated back to Afghanistan from Sudan.
[68] The 1998 African Embassy bombings and bin Laden's February 1998 fatwā marked a turning point of al-Qaeda's terrorist operation,[69] as bin Laden became intent on attacking the United States.
In late 1998 or early 1999, bin Laden gave approval for Mohammed to go forward with organizing the plot.
[70] A series of meetings occurred in the spring of 1999, involving Mohammed, bin Laden, and his deputy Mohammed Atef.
[71] Atef provided operational support for the plot, including target selections and helping arrange travel for the hijackers.
[68] Bin Laden overruled Mohammed, rejecting some potential targets such as the U.S. Bank Tower in Los Angeles because "there was not enough time to prepare for such an operation".
Bin Laden provided leadership and financial support for the plot, and was involved in selecting participants.
[74] Bin Laden initially selected Nawaf al-Hazmi and Khalid al-Mihdhar, both experienced jihadists who had fought in Bosnia.
Hazmi and Mihdhar arrived in the United States in mid-January 2000.
In spring 2000, Hazmi and Mihdhar took flying lessons in San Diego, California, but both spoke little English, performed poorly with flying lessons, and eventually served as secondary – or "muscle" – hijackers.
In late 1999, a group of men from Hamburg, Germany arrived in Afghanistan; the group included Mohamed Atta, Marwan al-Shehhi, Ziad Jarrah, and Ramzi bin al-Shibh.
[77] Bin Laden selected these men because they were educated, could speak English, and had experience living in the West.
[78] New recruits were routinely screened for special skills and al-Qaeda leaders consequently discovered that Hani Hanjour already had a commercial pilot's license.
[79] Mohammed later said that he helped the hijackers blend in by teaching them how to order food in restaurants and dress in Western clothing.
Hanjour arrived in San Diego on December 8, 2000, joining Hazmi.
[81]:6–7 They soon left for Arizona, where Hanjour took refresher training.
[81]:7 Marwan al-Shehhi arrived at the end of May 2000, while Atta arrived on June 3, 2000, and Jarrah arrived on June 27, 2000.
[81]:6 Bin al-Shibh applied several times for a visa to the United States, but as a Yemeni, he was rejected out of concerns he would overstay his visa and remain as an illegal immigrant.
[81]:4, 14 Bin al-Shibh stayed in Hamburg, providing coordination between Atta and Mohammed.
[81]:16 The three Hamburg cell members all took pilot training in South Florida.
In spring of 2001, the secondary hijackers began arriving in the United States.
[82] In July 2001, Atta met with bin al-Shibh in Spain, where they coordinated details of the plot, including final target selection.
Bin al-Shibh also passed along bin Laden's wish for the attacks to be carried out as soon as possible.
[83] Some of the hijackers received passports from corrupt Saudi officials who were family members, or used fraudulent passports to gain entry.
In late 1999, al-Qaeda associate Walid bin Attash ("Khallad") contacted Mihdhar, telling him to meet him in Kuala Lumpur, Malaysia; Hazmi and Abu Bara al Yemeni would also be in attendance.
The NSA intercepted a telephone call mentioning the meeting, Mihdhar, and the name "Nawaf" (Hazmi).
While the agency feared that "Something nefarious might be afoot", it took no further action.
The CIA had already been alerted by Saudi intelligence to the status of Mihdhar and Hazmi as al-Qaeda members, and a CIA team broke into Mihdhar's Dubai hotel room and discovered that Mihdhar had a U.S. visa.
While Alec Station alerted intelligence agencies worldwide about this fact, it did not share this information with the FBI.
The Malaysian Special Branch observed the January 5, 2000, meeting of the two al-Qaeda members, and informed the CIA that Mihdhar, Hazmi, and Khallad were flying to Bangkok, but the CIA never notified other agencies of this, nor did it ask the State Department to put Mihdhar on its watchlist.
An FBI liaison to Alec Station asked permission to inform the FBI of the meeting, but was told that "'This is not a matter for the FBI.
By late June, senior counter-terrorism official Richard Clarke and CIA director George Tenet were "convinced that a major series of attacks was about to come", although the CIA believed that the attacks would likely occur in Saudi Arabia or Israel.
[86] In early July, Clarke put domestic agencies on "full alert", telling them that "Something really spectacular is going to happen here ...
He asked the FBI and the State Department to alert the embassies and police departments, and the Defense Department to go to "Threat Condition Delta.
"[87][88] Clarke would later write that "Somewhere in CIA there was information that two known al Qaeda terrorists had come into the United States.
... in [the] FBI there was information that strange things had been going on at flight schools in the United States.
They had specific information about individual terrorists.
... None of that information got to me or the White House.
On July 13, Tom Wilshire, a CIA agent assigned to the FBI's international terrorism division, emailed his superiors at the CIA's Counterterrorism Center (CTC), requesting permission to inform the FBI that Hazmi was in the country and that Mihdhar had a U.S. visa.
The CIA never responded.
The same day in July, Margarette Gillespie, an FBI analyst working in the CTC, was told to review material about the Malaysia meeting.
She was not told of the participants' presence in the U.S.
The CIA gave Gillespie surveillance photos of Mihdhar and Hazmi from the meeting to show to FBI counterterrorism, but did not tell her their significance.
The Intelink database informed her not to share intelligence material on the meeting to criminal investigators.
When shown the photos, the FBI were refused more details on their significance, and also did not receive Mihdhar's date of birth or passport number.
[91] In late August 2001, Gillespie told the INS, the State Department, the Customs Service, and the FBI to put Hazmi and Mihdhar on their watchlists, but the FBI was prohibited from using criminal agents in the search for the duo, which hindered their efforts.
Also in July, a Phoenix-based FBI agent sent a message to FBI headquarters, Alec Station, and to FBI agents in New York, alerting them to "the possibility of a coordinated effort by Osama bin Laden to send students to the United States to attend civil aviation universities and colleges."
The agent, Kenneth Williams, suggested the need to interview all flight school managers and identify all Arab students seeking flight training.
[93] In July, Jordan alerted the U.S. that al-Qaeda was planning an attack on the U.S.; "months later", Jordan notified the U.S. that the attack's codename was "The Big Wedding", and that it involved airplanes.
On August 6, 2001, the CIA's Presidential Daily Brief, designated "For the President Only", was entitled "Bin Ladin Determined to Strike in U.S." The memo noted that "The FBI information ... indicates patterns of suspicious activity in this country consistent with preparations for hijackings or other types of attacks.
In mid-August, one Minnesota flight school alerted the FBI to Zacarias Moussaoui, who had asked "suspicious questions."
The FBI found that he was a radical who had traveled to Pakistan, and the INS arrested him for overstaying his French visa.
Their request to search his laptop was denied by FBI headquarters due to the lack of probable cause.
The failures in intelligence-sharing were attributed to 1995 Justice Department policies limiting intelligence sharing, combined with CIA and NSA reluctance in revealing "sensitive sources and methods" such as tapped phones.
[97] Testifying before the 9/11 Commission in April 2004, then-Attorney General John Ashcroft recalled that the "single greatest structural cause for the September 11th problem was the wall that segregated or separated criminal investigators and intelligence agents.
"[98] Clarke also wrote that "There were failures in the organizations ... failures to get information to the right place at the right time.
Early on the morning of September 11, 2001, 19 hijackers took control of four commercial airliners (two Boeing 757s and two Boeing 767s) en route to California (three headed to LAX in Los Angeles and one to SFO in San Francisco) after takeoffs from Logan International Airport in Boston, Massachusetts; Newark Liberty International Airport in Newark, New Jersey; and Washington Dulles International Airport in Loudoun and Fairfax counties in Virginia.
[100] Large planes with long flights were selected for hijacking because they would be full of fuel.
The four flights were:
Media coverage was extensive during the attacks and aftermath, beginning moments after the first crash into the World Trade Center.
At 8:46 am, five hijackers crashed American Airlines Flight 11 into the northern façade of the World Trade Center's North Tower (1 WTC).
At 9:03 am, another five hijackers crashed United Airlines Flight 175 into the southern façade of the South Tower (2 WTC).
[103][104] Five hijackers flew American Airlines Flight 77 into the Pentagon at 9:37 am.
[105] A fourth flight, United Airlines Flight 93, crashed near Shanksville, Pennsylvania, southeast of Pittsburgh, at 10:03 a.m. after the passengers fought the four hijackers.
Flight 93's target is believed to have been either the Capitol or the White House.
[101] Flight 93's cockpit voice recorder revealed crew and passengers tried to seize control of the plane from the hijackers after learning through phone calls that Flights 11, 77, and 175 had been crashed into buildings that morning.
[106] Once it became evident that the passengers might gain control, the hijackers rolled the plane and intentionally crashed it.
Some passengers and crew members who called from the aircraft using the cabin airphone service and mobile phones provided details: several hijackers were aboard each plane; they used mace, tear gas, or pepper spray to overcome attendants; and some people aboard had been stabbed.
[109] Reports indicated hijackers stabbed and killed pilots, flight attendants, and one or more passengers.
[100][110] According to the 9/11 Commission's final report, the hijackers had recently purchased multi-function hand tools and assorted Leatherman-type utility knives with locking blades, which were not forbidden to passengers at the time, but were not found among the possessions left behind by the hijackers.
[111][112] A flight attendant on Flight 11, a passenger on Flight 175, and passengers on Flight 93 said the hijackers had bombs, but one of the passengers said he thought the bombs were fake.
The FBI found no traces of explosives at the crash sites, and the 9/11 Commission concluded that the bombs were probably fake.
Three buildings in the World Trade Center collapsed due to fire-induced structural failure.
[113] The South Tower collapsed at 9:59 a.m. after burning for 56 minutes in a fire caused by the impact of United Airlines Flight 175 and the explosion of its fuel.
[113] The North Tower collapsed at 10:28 a.m. after burning for 102 minutes.
[113] When the North Tower collapsed, debris fell on the nearby 7 World Trade Center building (7 WTC), damaging it and starting fires.
These fires burned for hours, compromising the building's structural integrity, and 7 WTC collapsed at 5:21 pm.
[114][115] The west side of the Pentagon sustained significant damage.
At 9:42 am, the Federal Aviation Administration (FAA) grounded all civilian aircraft within the continental U.S., and civilian aircraft already in flight were told to land immediately.
[117] All international civilian aircraft were either turned back or redirected to airports in Canada or Mexico, and were banned from landing on United States territory for three days.
[118] The attacks created widespread confusion among news organizations and air traffic controllers.
Among the unconfirmed and often contradictory news reports aired throughout the day, one of the most prevalent said a car bomb had been detonated at the U.S. State Department's headquarters in Washington, D.C.[119] Another jet—Delta Air Lines Flight 1989—was suspected of having been hijacked, but the aircraft responded to controllers and landed safely in Cleveland, Ohio.
In an April 2002 interview, Khalid Sheikh Mohammed and Ramzi bin al-Shibh, who are believed to have organized the attacks, said Flight 93's intended target was the United States Capitol, not the White House.
[121] During the planning stage of the attacks, Mohamed Atta, the hijacker and pilot of Flight 11, thought the White House might be too tough a target and sought an assessment from Hani Hanjour (who hijacked and piloted Flight 77).
[122] Atta said al-Qaeda initially planned to target nuclear installations rather than the World Trade Center and the Pentagon, but decided against it, fearing things could "get out of control".
[123] Final decisions on targets, according to Atta, were left in the hands of the pilots.
The attacks caused the deaths of 2,996 people and the injuries of more than 6,000 others.
[124] The death toll included 265 on the four planes (from which there were no survivors), 2,606 in the World Trade Center and in the surrounding area, and 125 at the Pentagon.
[125][126] Most of those who perished were civilians, with the exception of 343 firefighters, 72 law enforcement officers, 55 military personnel, and the 19 terrorists who died in the attacks.
[127][128] After New York, New Jersey lost the most state citizens, with the city of Hoboken having the most New Jersey citizens who died in the attacks.
[129] More than 90 countries lost citizens in the September 11 attacks;[130] for example, the 67 Britons who died were more than in any other terrorist attack anywhere as of October 2002[update].
[131] The attacks killed about 500 more people than the attack on Pearl Harbor on December 7, 1941, and are the deadliest terrorist attacks in world history.
In Arlington County, Virginia, 125 Pentagon workers lost their lives when Flight 77 crashed into the western side of the building.
Of these, 70 were civilians and 55 were military personnel, many of whom worked for the United States Army or the United States Navy.
The Army lost 47 civilian employees, six civilian contractors, and 22 soldiers, while the Navy lost six civilian employees, three civilian contractors, and 33 sailors.
Seven Defense Intelligence Agency (DIA) civilian employees were also among the dead in the attack, as well as an Office of the Secretary of Defense (OSD) contractor.
[132][133][134] Lieutenant General Timothy Maude, an Army Deputy Chief of Staff, was the highest-ranking military official killed at the Pentagon.
In New York City, more than 90% of the workers and visitors who died in the towers had been at or above the points of impact.
[136] In the North Tower, 1,355 people at or above the point of impact were trapped and died of smoke inhalation, fell or jumped from the tower to escape the smoke and flames, or were killed in the building's eventual collapse.
The destruction of all three staircases in the tower when Flight 11 hit made it impossible for anyone above the impact zone to escape.
107 people below the point of impact died as well.
In the South Tower, one stairwell, Stairwell A, was left intact after Flight 175 hit, allowing 14 people located on the floors of impact (including one man who saw the plane coming at him) and four more from the floors above to escape.
New York City 9-1-1 operators who received calls from people inside the tower were not well informed of the situation as it rapidly unfolded and as a result, told callers not to descend the tower on their own.
[137] In total 630 people died in that tower, fewer than half the number killed in the North Tower.
[136] Casualties in the South Tower were significantly reduced because some occupants decided to start evacuating as soon as the North Tower was struck.
[138] The failure to fully evacuate the South Tower after the first jet crash into the North Tower was described by USA Today as "one of the day's great tragedies".
At least 200 people fell or jumped to their deaths from the burning towers (as exemplified in the photograph The Falling Man), landing on the streets and rooftops of adjacent buildings hundreds of feet below.
[140] Some occupants of each tower above the point of impact made their way toward the roof in the hope of helicopter rescue, but the roof access doors were locked.
[141] No plan existed for helicopter rescues, and the combination of roof equipment, thick smoke, and intense heat prevented helicopters from approaching.
A total of 411 emergency workers died as they tried to rescue people and fight fires.
The New York City Fire Department (FDNY) lost 343 firefighters, including a chaplain and two paramedics.
[143] The New York City Police Department (NYPD) lost 23 officers.
[144] The Port Authority Police Department (PAPD) lost 37 officers.
[145] Eight emergency medical technicians (EMTs) and paramedics from private emergency medical services units were killed.
Cantor Fitzgerald L.P., an investment bank on the 101st–105th floors of the North Tower, lost 658 employees, considerably more than any other employer.
[147] Marsh Inc., located immediately below Cantor Fitzgerald on floors 93–100, lost 358 employees,[148][149] and 175 employees of Aon Corporation were also killed.
[150] The National Institute of Standards and Technology (NIST) estimated that about 17,400 civilians were in the World Trade Center complex at the time of the attacks.
Turnstile counts from the Port Authority suggest 14,154 people were typically in the Twin Towers by 8:45 am.
[151][152] Most people below the impact zone safely evacuated the buildings.
Weeks after the attack, the death toll was estimated to be over 6,000, more than twice the number of deaths eventually confirmed.
[160] The city was only able to identify remains for about 1,600 of the World Trade Center victims.
The medical examiner's office collected "about 10,000 unidentified bone and tissue fragments that cannot be matched to the list of the dead".
[161] Bone fragments were still being found in 2006 by workers who were preparing to demolish the damaged Deutsche Bank Building.
In 2010, a team of anthropologists and archaeologists searched for human remains and personal items at the Fresh Kills Landfill, where 72 more human remains were recovered, bringing the total found to 1,845.
DNA profiling continues in an attempt to identify additional victims.
[162][163][164] The remains are being held in storage in Memorial Park, outside the New York City Medical Examiner's facilities.
It was expected that the remains would be moved in 2013 to a repository behind a wall at the 9/11 museum.
In July 2011, a team of scientists at the Office of Chief Medical Examiner was still trying to identify remains, in the hope that improved technology will allow them to identify other victims.
[164] On August 7, 2017, the 1,641st victim was identified as a result of newly available DNA technology,[165] and a 1,642nd on July 26, 2018.
[166] A further 1,111 victims are yet to be identified.
Along with the 110-floor Twin Towers, numerous other buildings at the World Trade Center site were destroyed or badly damaged, including WTC buildings 3 through 7 and St. Nicholas Greek Orthodox Church.
[168] The North Tower, South Tower, the Marriott Hotel (3 WTC), and 7 WTC were destroyed.
The U.S. Customs House (6 World Trade Center), 4 World Trade Center, 5 World Trade Center, and both pedestrian bridges connecting buildings were severely damaged.
The Deutsche Bank Building on 130 Liberty Street was partially damaged and demolished some years later, starting in 2007.
[169][170] The two buildings of the World Financial Center also suffered damage.
[169] The last fires at the World Trade Center site were extinguished on December 20, exactly 100 days after the attacks.
The Deutsche Bank Building across Liberty Street from the World Trade Center complex was later condemned as uninhabitable because of toxic conditions inside the office tower, and was deconstructed.
[172][173] The Borough of Manhattan Community College's Fiterman Hall at 30 West Broadway was condemned due to extensive damage in the attacks, and was reopened in 2012.
[174] Other neighboring buildings (including 90 West Street and the Verizon Building) suffered major damage but have been restored.
[175] World Financial Center buildings, One Liberty Plaza, the Millenium Hilton, and 90 Church Street had moderate damage and have since been restored.
[176] Communications equipment on top of the North Tower was also destroyed, with only WCBS-TV maintaining a backup transmitter on the Empire State Building, but media stations were quickly able to reroute the signals and resume their broadcasts.
The PATH train system's World Trade Center station was located under the complex.
As a result, the entire station was demolished completely when the towers collapsed, and the tunnels leading to Exchange Place station in Jersey City, New Jersey were flooded with water.
[178] The station was rebuilt as the $4 billion World Trade Center Transportation Hub, which reopened in March 2015.
[179][180] The WTC Cortlandt Street station on the New York City Subway's IRT Broadway-Seventh Avenue Line was also within close proximity to the World Trade Center complex, and the entire station, along with the surrounding track, was reduced to rubble.
[181] The latter station was rebuilt and reopened to the public on September 8, 2018.
The Pentagon was severely damaged by the impact of American Airlines Flight 77 and ensuing fires, causing one section of the building to collapse.
[183] As the airplane approached the Pentagon, its wings knocked down light poles and its right engine hit a power generator before crashing into the western side of the building.
[184][185] The plane hit the Pentagon at the first-floor level.
The front part of the fuselage disintegrated on impact, while the mid and tail sections kept moving for another fraction of a second.
[186] Debris from the tail section penetrated furthest into the building, breaking through 310 feet (94 m) of the three outermost of the building's five rings.
The New York City Fire Department deployed 200 units (half of the department) to the World Trade Center.
Their efforts were supplemented by numerous off-duty firefighters and emergency medical technicians.
[188][189][190] The New York City Police Department sent Emergency Service Units and other police personnel and deployed its aviation unit.
Once on the scene, the FDNY, the NYPD, and the PAPD did not coordinate efforts and performed redundant searches for civilians.
[188][191] As conditions deteriorated, the NYPD aviation unit relayed information to police commanders, who issued orders for its personnel to evacuate the towers; most NYPD officers were able to safely evacuate before the buildings collapsed.
[191][192] With separate command posts set up and incompatible radio communications between the agencies, warnings were not passed along to FDNY commanders.
After the first tower collapsed, FDNY commanders issued evacuation warnings.
Due to technical difficulties with malfunctioning radio repeater systems, many firefighters never heard the evacuation orders.
9-1-1 dispatchers also received information from callers that was not passed along to commanders on the scene.
[189] Within hours of the attack, a substantial search and rescue operation was launched.
After months of around-the-clock operations, the World Trade Center site was cleared by the end of May 2002.
The aftermath of the 9/11 attack resulted in immediate responses to the event, including domestic reactions, hate crimes, Muslim American responses to the event, international responses to the attack, and military responses to the events.
An extensive compensation program was quickly established by Congress in the aftermath to compensate the victims and families of victims of the 9/11 attack as well.
At 8:32 am FAA officials were notified Flight 11 had been hijacked and they, in turn, notified the North American Aerospace Defense Command (NORAD).
NORAD scrambled two F-15s from Otis Air National Guard Base in Massachusetts and they were airborne by 8:53 am.
[196] Because of slow and confused communication from FAA officials, NORAD had 9 minutes' notice that Flight 11 had been hijacked, and no notice about any of the other flights before they crashed.
[196] After both of the Twin Towers had already been hit, more fighters were scrambled from Langley Air Force Base in Virginia at 9:30 am.
[196] At 10:20 am Vice President Dick Cheney issued orders to shoot down any commercial aircraft that could be positively identified as being hijacked.
These instructions were not relayed in time for the fighters to take action.
[196][197][198][199] Some fighters took to the air without live ammunition, knowing that to prevent the hijackers from striking their intended targets, the pilots might have to intercept and crash their fighters into the hijacked planes, possibly ejecting at the last moment.
For the first time in U.S. history, SCATANA was invoked,[201] thus stranding tens of thousands of passengers across the world.
[202] Ben Sliney, in his first day as the National Operations Manager of the FAA,[203] ordered that American airspace would be closed to all international flights, causing about five hundred flights to be turned back or redirected to other countries.
Canada received 226 of the diverted flights and launched Operation Yellow Ribbon to deal with the large numbers of grounded planes and stranded passengers.
The 9/11 attacks had immediate effects on the American people.
[205] Police and rescue workers from around the country took a leave of absence from their jobs and traveled to New York City to help recover bodies from the twisted remnants of the Twin Towers.
[206] Blood donations across the U.S. surged in the weeks after 9/11.
The deaths of adults in the attacks resulted in over 3,000 children losing a parent.
[209] Subsequent studies documented children's reactions to these actual losses and to feared losses of life, the protective environment in the aftermath of the attacks, and effects on surviving caregivers.
Following the attacks, President George W. Bush's approval rating soared to 90%.
[213] On September 20, 2001, he addressed the nation and a joint session of the United States Congress regarding the events of September 11 and the subsequent nine days of rescue and recovery efforts, and described his intended response to the attacks.
New York City mayor Rudy Giuliani's highly visible role won him high praise in New York and nationally.
Many relief funds were immediately set up to assist victims of the attacks, with the task of providing financial assistance to the survivors of the attacks and to the families of victims.
By the deadline for victim's compensation on September 11, 2003, 2,833 applications had been received from the families of those who were killed.
Contingency plans for the continuity of government and the evacuation of leaders were implemented soon after the attacks.
[202] Congress was not told that the United States had been under a continuity of government status until February 2002.
In the largest restructuring of the U.S. government in contemporary history, the United States enacted the Homeland Security Act of 2002, creating the Department of Homeland Security.
Congress also passed the USA PATRIOT Act, saying it would help detect and prosecute terrorism and other crimes.
[217] Civil liberties groups have criticized the PATRIOT Act, saying it allows law enforcement to invade the privacy of citizens and that it eliminates judicial oversight of law enforcement and domestic intelligence.
[218][219][220] In an effort to effectively combat future acts of terrorism, the National Security Agency (NSA) was given broad powers.
NSA commenced warrantless surveillance of telecommunications, which was sometimes criticized since it permitted the agency "to eavesdrop on telephone and e-mail communications between the United States and people overseas without a warrant".
[221] In response to requests by various intelligence agencies, the United States Foreign Intelligence Surveillance Court permitted an expansion of powers by the U.S. government in seeking, obtaining, and sharing information on U.S. citizens as well as non-U.S. people from around the world.
Shortly after the attacks, President Bush made a public appearance at Washington's largest Islamic Center and acknowledged the "incredibly valuable contribution" that millions of American Muslims made to their country and called for them "to be treated with respect.
"[223] Numerous incidents of harassment and hate crimes against Muslims and South Asians were reported in the days following the attacks.
[224][225][226] Sikhs were also targeted because Sikh males usually wear turbans, which are stereotypically associated with Muslims.
There were reports of attacks on mosques and other religious buildings (including the firebombing of a Hindu temple), and assaults on people, including one murder: Balbir Singh Sodhi, a Sikh mistaken for a Muslim, was fatally shot on September 15, 2001, in Mesa, Arizona.
[226] Two dozen members of Osama bin Laden's family were urgently evacuated out of the country on a private charter plane under FBI supervision three days after the attacks.
According to an academic study, people perceived to be Middle Eastern were as likely to be victims of hate crimes as followers of Islam during this time.
The study also found a similar increase in hate crimes against people who may have been perceived as Muslims, Arabs, and others thought to be of Middle Eastern origin.
[228] A report by the South Asian American advocacy group known as South Asian Americans Leading Together, documented media coverage of 645 bias incidents against Americans of South Asian or Middle Eastern descent between September 11 and 17.
Various crimes such as vandalism, arson, assault, shootings, harassment, and threats in numerous places were documented.
Muslim organizations in the United States were swift to condemn the attacks and called "upon Muslim Americans to come forward with their skills and resources to help alleviate the sufferings of the affected people and their families".
[231] These organizations included the Islamic Society of North America, American Muslim Alliance, American Muslim Council, Council on American-Islamic Relations, Islamic Circle of North America, and the Shari'a Scholars Association of North America.
Along with monetary donations, many Islamic organizations launched blood drives and provided medical assistance, food, and shelter for victims.
The attacks were denounced by mass media and governments worldwide.
Across the globe, nations offered pro-American support and solidarity.
[235] Leaders in most Middle Eastern countries, and Afghanistan, condemned the attacks.
Iraq was a notable exception, with an immediate official statement that, "the American cowboys are reaping the fruit of their crimes against humanity".
[236] The government of Saudi Arabia officially condemned the attacks, but privately many Saudis favored bin Laden's cause.
[237][238] Although Palestinian Authority (PA) president Yasser Arafat also condemned the attacks, there were reports of celebrations in the West Bank, Gaza Strip, and East Jerusalem—with a celebration involving 3,000 Palestinians dancing in the streets and handing out candy being filmed in Nablus despite alleged PA warnings that it could not guarantee the safety of journalists attempting to document the event.
Similar demonstrations took place in Amman, Jordan, where there is a large population of Palestinian descent.
[239] As in the United States, the aftermath of the attacks saw tensions increase in other countries between Muslims and non-Muslims.
United Nations Security Council Resolution 1368 condemned the attacks, and expressed readiness to take all necessary steps to respond and combat all forms of terrorism in accordance with their Charter.
[241] Numerous countries introduced anti-terrorism legislation and froze bank accounts they suspected of al-Qaeda ties.
[242][243] Law enforcement and intelligence agencies in a number of countries arrested alleged terrorists.
British Prime Minister Tony Blair said Britain stood "shoulder to shoulder" with the United States.
[246] A few days later, Blair flew to Washington to affirm British solidarity with the United States.
In a speech to Congress, nine days after the attacks, which Blair attended as a guest, President Bush declared "America has no truer friend than Great Britain.
"[247] Subsequently, Prime Minister Blair embarked on two months of diplomacy to rally international support for military action; he held 54 meetings with world leaders and travelled more than 40,000 miles (60,000 km).
In the aftermath of the attacks, tens of thousands of people attempted to flee Afghanistan due to the possibility of a military retaliation by the United States.
Pakistan, already home to many Afghan refugees from previous conflicts, closed its border with Afghanistan on September 17, 2001.
Approximately one month after the attacks, the United States led a broad coalition of international forces to overthrow the Taliban regime from Afghanistan for their harboring of al-Qaeda.
[249] Though Pakistani authorities were initially reluctant to align themselves with the United States against the Taliban, they permitted the coalition access to their military bases, and arrested and handed over to the U.S. over 600 suspected al-Qaeda members.
The U.S. set up the Guantanamo Bay detention camp to hold inmates they defined as "illegal enemy combatants".
The legitimacy of these detentions has been questioned by the European Union and human rights organizations.
On September 25, 2001, Iran's fifth president, Mohammad Khatami meeting British Foreign Secretary, Jack Straw, said: "Iran fully understands the feelings of the Americans about the terrorist attacks in New York and Washington on September 11."
He said although the American administrations had been at best indifferent about terrorist operations in Iran (since 1979), the Iranians instead felt differently and had expressed their sympathetic feelings with bereaved Americans in the tragic incidents in the two cities.
He also stated that "Nations should not be punished in place of terrorists.
"[255] According to Radio Farda's website, when the attacks' news was released, some Iranian citizens gathered in front of the Embassy of Switzerland in Tehran, which serves as the protecting power of the United States in Iran (U.S. interests protecting office in Iran), to express their sympathy and some of them lit candles as a symbol of mourning.
This piece of news at Radio Farda's website also states that in 2011, on the anniversary of the attacks, United States Department of State, published a post at its blog, in which the Department thanked Iranian people for their sympathy and stated that they would never forget Iranian people's kindness on those harsh days.
[256] After the attacks, both the President[257][258] and the Supreme Leader of Iran, condemned the attacks.
The BBC and Time magazine published reports on holding candlelit vigils for the victims by Iranian citizens at their websites.
[259][260] According to Politico Magazine, following the attacks, Sayyed Ali Khamenei, the Supreme Leader of Iran, "suspended the usual 'Death to America' chants at Friday prayers" temporarily.
In a speech by the Nizari Ismaili Imam at the Nobel Institute in 2005, Aga Khan IV stated that the "9/11 attack on the United States was a direct consequence of the international community ignoring the human tragedy that was Afghanistan at that time".
In September 2001, shortly after the attacks, Greek soccer fans burned an Israeli flag and unsuccessfully tried to burn an American flag.
Though the American flag did not catch fire, the fans booed during a moment of silence for victims of the attacks.
At 2:40 p.m. in the afternoon of September 11, Secretary of Defense Donald Rumsfeld was issuing rapid orders to his aides to look for evidence of Iraqi involvement.
According to notes taken by senior policy official Stephen Cambone, Rumsfeld asked for, "Best info fast.
Judge whether good enough hit S.H."
(Saddam Hussein) "at same time.
Not only UBL" (Osama bin Laden).
[264] Cambone's notes quoted Rumsfeld as saying, "Need to move swiftly – Near term target needs – go massive – sweep it all up.
Things related and not.
"[265][266] In a meeting at Camp David on September 15 the Bush administration rejected the idea of attacking Iraq in response to 9/11.
[267] Nonetheless, they later invaded the country with allies, citing "Saddam Hussein's support for terrorism".
[268] At the time, as many as 7 in 10 Americans believed the Iraqi president played a role in the 9/11 attacks.
[269] Three years later, Bush conceded that he had not.
The NATO council declared that the terrorist attacks on the United States were an attack on all NATO nations that satisfied Article 5 of the NATO charter.
This marked the first invocation of Article 5, which had been written during the Cold War with an attack by the Soviet Union in mind.
[271] Australian Prime Minister John Howard who was in Washington D.C. during the attacks invoked Article IV of the ANZUS treaty.
[272] The Bush administration announced a War on Terror, with the stated goals of bringing bin Laden and al-Qaeda to justice and preventing the emergence of other terrorist networks.
[273] These goals would be accomplished by imposing economic and military sanctions against states harboring terrorists, and increasing global surveillance and intelligence sharing.
On September 14, 2001, the U.S. Congress passed the Authorization for Use of Military Force Against Terrorists.
Still in effect, it grants the President the authority to use all "necessary and appropriate force" against those whom he determined "planned, authorized, committed or aided" the September 11 attacks, or who harbored said persons or groups.
On October 7, 2001, the War in Afghanistan began when U.S. and British forces initiated aerial bombing campaigns targeting Taliban and al-Qaeda camps, then later invaded Afghanistan with ground troops of the Special Forces.
[276] This eventually led to the overthrow of the Taliban rule of Afghanistan with the Fall of Kandahar on December 7, 2001, by U.S.-led coalition forces.
[277] Conflict in Afghanistan between the Taliban insurgency and the Afghan forces backed by NATO Resolute Support Mission is ongoing.
The Philippines and Indonesia, among other nations with their own internal conflicts with Islamic terrorism, also increased their military readiness.
The military forces of the United States of America and the Islamic Republic of Iran cooperated with each other to overthrow the Taliban regime which had had conflicts with the government of Iran.
[261] Iran's Quds Force helped U.S. forces and Afghan rebels in the 2001 uprising in Herat.
Hundreds of thousands of tons of toxic debris containing more than 2,500 contaminants, including known carcinogens, were spread across Lower Manhattan due to the collapse of the Twin Towers.
[283][284] Exposure to the toxins in the debris is alleged to have contributed to fatal or debilitating illnesses among people who were at Ground Zero.
[285][286] The Bush administration ordered the Environmental Protection Agency (EPA) to issue reassuring statements regarding air quality in the aftermath of the attacks, citing national security, but the EPA did not determine that air quality had returned to pre-September 11 levels until June 2002.
Health effects extended to residents, students, and office workers of Lower Manhattan and nearby Chinatown.
[288] Several deaths have been linked to the toxic dust, and the victims' names were included in the World Trade Center memorial.
[289] Approximately 18,000 people have been estimated to have developed illnesses as a result of the toxic dust.
[290] There is also scientific speculation that exposure to various toxic products in the air may have negative effects on fetal development.
A notable children's environmental health center is currently[when?]
analyzing the children whose mothers were pregnant during the WTC collapse, and were living or working nearby.
[291] A study of rescue workers released in April 2010 found that all those studied had impaired lung functions, and that 30–40% were reporting little or no improvement in persistent symptoms that started within the first year of the attack.
Years after the attacks, legal disputes over the costs of illnesses related to the attacks were still in the court system.
On October 17, 2006, a federal judge rejected New York City's refusal to pay for health costs for rescue workers, allowing for the possibility of numerous suits against the city.
[293] Government officials have been faulted for urging the public to return to lower Manhattan in the weeks shortly after the attacks.
Christine Todd Whitman, administrator of the EPA in the aftermath of the attacks, was heavily criticized by a U.S. District Judge for incorrectly saying that the area was environmentally safe.
[294] Mayor Giuliani was criticized for urging financial industry personnel to return quickly to the greater Wall Street area.
On December 22, 2010, the United States Congress passed the James L. Zadroga 9/11 Health and Compensation Act, which President Barack Obama signed into law on January 2, 2011.
It allocated $4.2 billion to create the World Trade Center Health Program, which provides testing and treatment for people suffering from long-term health problems related to the 9/11 attacks.
[296][297] The WTC Health Program replaced preexisting 9/11-related health programs such as the Medical Monitoring and Treatment Program and the WTC Environmental Health Center program.
The attacks had a significant economic impact on United States and world markets.
[298] The stock exchanges did not open on September 11 and remained closed until September 17.
Reopening, the Dow Jones Industrial Average (DJIA) fell 684 points, or 7.1%, to 8921, a record-setting one-day point decline.
[299] By the end of the week, the DJIA had fallen 1,369.7 points (14.3%), at the time its largest one-week point drop in history.
[300] In 2001 dollars, U.S. stocks lost $1.4 trillion in valuation for the week.
In New York City, about 430,000 job-months and $2.8 billion dollars in wages were lost in the first three months after the attacks.
The economic effects were mainly on the economy's export sectors.
[301] The city's GDP was estimated to have declined by $27.3 billion for the last three months of 2001 and all of 2002.
The U.S. government provided $11.2 billion in immediate assistance to the Government of New York City in September 2001, and $10.5 billion in early 2002 for economic development and infrastructure needs.
Also hurt were small businesses in Lower Manhattan near the World Trade Center, 18,000 of which were destroyed or displaced, resulting in lost jobs and their consequent wages.
Assistance was provided by Small Business Administration loans, federal government Community Development Block Grants, and Economic Injury Disaster Loans.
[302] Some 31,900,000 square feet (2,960,000 m2) of Lower Manhattan office space was damaged or destroyed.
[303] Many wondered whether these jobs would return, and if the damaged tax base would recover.
[304] Studies of the economic effects of 9/11 show the Manhattan office real-estate market and office employment were less affected than first feared, because of the financial services industry's need for face-to-face interaction.
North American air space was closed for several days after the attacks and air travel decreased upon its reopening, leading to a nearly 20% cutback in air travel capacity, and exacerbating financial problems in the struggling U.S. airline industry.
The September 11 attacks also led to the U.S. wars in Afghanistan and Iraq,[308] as well as additional homeland security spending, totaling at least $5 trillion.
The impact of 9/11 extends beyond geopolitics into society and culture in general.
Immediate responses to 9/11 included greater focus on home life and time spent with family, higher church attendance, and increased expressions of patriotism such as the flying of flags.
[310] The radio industry responded by removing certain songs from playlists, and the attacks have subsequently been used as background, narrative, or thematic elements in film, television, music, and literature.
Novels inspired or directly influenced by 9/11 include Crazy Gran by Gary Botting, about a girl who discovers a direct family connection to the terrorists.
The action begins at "9 am., Tuesday, 9/11/2001" and continues for a harrowing week as her uncle attempts to silence her, applying precepts of Sharia law.
[311] Already-running television shows as well as programs developed after 9/11 have reflected post-9/11 cultural concerns.
[312] 9/11 conspiracy theories have become social phenomena, despite lack of support from expert scientists, engineers, and historians.
[313] 9/11 has also had a major impact on the religious faith of many individuals; for some it strengthened, to find consolation to cope with the loss of loved ones and overcome their grief; others started to question their faith or lost it entirely, because they could not reconcile it with their view of religion.
The culture of America succeeding the attacks is noted for heightened security and an increased demand thereof, as well as paranoia and anxiety regarding future terrorist attacks that includes most of the nation.
Psychologists have also confirmed that there has been an increased amount of national anxiety in commercial air travel.
As a result of the attacks, many governments across the world passed legislation to combat terrorism.
[317] In Germany, where several of the 9/11 terrorists had resided and taken advantage of that country's liberal asylum policies, two major anti-terrorism packages were enacted.
The first removed legal loopholes that permitted terrorists to live and raise money in Germany.
The second addressed the effectiveness and communication of intelligence and law enforcement.
[318] Canada passed the Canadian Anti-Terrorism Act, that nation's first anti-terrorism law.
[319] The United Kingdom passed the Anti-terrorism, Crime and Security Act 2001 and the Prevention of Terrorism Act 2005.
[320][321] New Zealand enacted the Terrorism Suppression Act 2002.
In the United States, the Department of Homeland Security was created by the Homeland Security Act to coordinate domestic anti-terrorism efforts.
The USA Patriot Act gave the federal government greater powers, including the authority to detain foreign terror suspects for a week without charge, to monitor telephone communications, e-mail, and Internet use by terror suspects, and to prosecute suspected terrorists without time restrictions.
The FAA ordered that airplane cockpits be reinforced to prevent terrorists gaining control of planes, and assigned sky marshals to flights.
Further, the Aviation and Transportation Security Act made the federal government, rather than airports, responsible for airport security.
The law created the Transportation Security Administration to inspect passengers and luggage, causing long delays and concern over passenger privacy.
[323] After suspected abuses of the USA Patriot Act were brought to light in June 2013 with articles about collection of American call records by the NSA and the PRISM program (see 2013 mass surveillance disclosures), Representative Jim Sensenbrenner, Republican of Wisconsin, who introduced the Patriot Act in 2001, said that the National Security Agency overstepped its bounds.
Immediately after the attacks, the Federal Bureau of Investigation started PENTTBOM, the largest criminal inquiry in the history of the United States.
At its height, more than half of the FBI's agents worked on the investigation and followed a half-million leads.
[326] The FBI concluded that there was "clear and irrefutable" evidence linking al-Qaeda and bin Laden to the attacks.
The FBI was quickly able to identify the hijackers, including leader Mohamed Atta, when his luggage was discovered at Boston's Logan Airport.
Atta had been forced to check two of his three bags due to space limitations on the 19-seat commuter flight he took to Boston.
Due to a new policy instituted to prevent flight delays, the luggage failed to make it aboard American Airlines Flight 11 as planned.
The luggage contained the hijackers' names, assignments, and al-Qaeda connections.
"It had all these Arab-language  [sic] papers that amounted to the Rosetta stone of the investigation", said one FBI agent.
[328] Within hours of the attacks, the FBI released the names and in many cases the personal details of the suspected pilots and hijackers.
[329][330] On September 27, 2001, they released photos of all 19 hijackers, along with information about possible nationalities and aliases.
[331] Fifteen of the men were from Saudi Arabia, two from the United Arab Emirates, one from Egypt, and one from Lebanon.
By midday, the U.S. National Security Agency and German intelligence agencies had intercepted communications pointing to Osama bin Laden.
[333] Two of the hijackers were known to have travelled with a bin Laden associate to Malaysia in 2000[334] and hijacker Mohammed Atta had previously gone to Afghanistan.
[335] He and others were part of a terrorist cell in Hamburg.
[336] One of the members of the Hamburg cell was discovered to have been in communication with Khalid Sheik Mohammed who was identified as a member of al-Qaeda.
Authorities in the United States and Britain also obtained electronic intercepts, including telephone conversations and electronic bank transfers, which indicate that Mohammed Atef, a bin Laden deputy, was a key figure in the planning of the 9/11 attacks.
Intercepts were also obtained that revealed conversations that took place days before September 11 between bin Laden and an associate in Pakistan.
In those conversations, the two referred to "an incident that would take place in America on, or around, September 11" and they discussed potential repercussions.
In another conversation with an associate in Afghanistan, bin Laden discussed the "scale and effects of a forthcoming operation."
These conversations did not specifically mention the World Trade Center or Pentagon, or other specifics.
The FBI did not record the 2,977 deaths from the attacks in their annual violent crime index for 2001.
In a disclaimer, the FBI stated that "the number of deaths is so great that combining it with the traditional crime statistics will have an outlier effect that falsely skews all types of measurements in the program's analyses.
"[339] New York City also did not include the deaths in their annual crime statistics for 2001.
The Inspector General of the Central Intelligence Agency (CIA) conducted an internal review of the agency's pre-9/11 performance and was harshly critical of senior CIA officials for not doing everything possible to confront terrorism.
He criticized their failure to stop two of the 9/11 hijackers, Nawaf al-Hazmi and Khalid al-Mihdhar, as they entered the United States and their failure to share information on the two men with the FBI.
[341] In May 2007, senators from both major U.S. political parties drafted legislation to make the review public.
One of the backers, Senator Ron Wyden said, "The American people have a right to know what the Central Intelligence Agency was doing in those critical months before 9/11.
In February 2002, the Senate Select Committee on Intelligence and the House Permanent Select Committee on Intelligence formed a joint inquiry into the performance of the U.S. Intelligence Community.
[343] Their 832-page report released in December 2002[344] detailed failings of the FBI and CIA to use available information, including about terrorists the CIA knew were in the United States, in order to disrupt the plots.
[345] The joint inquiry developed its information about possible involvement of Saudi Arabian government officials from non-classified sources.
[346] Nevertheless, the Bush administration demanded 28 related pages remain classified.
[345] In December 2002, the inquiry's chair Bob Graham (D-FL) revealed in an interview that there was "evidence that there were foreign governments involved in facilitating the activities of at least some of the terrorists in the United States.
"[347] September 11 victim families were frustrated by the unanswered questions and redacted material from the Congressional inquiry and demanded an independent commission.
[345] September 11 victim families,[348] members of congress[349][350] and the Saudi Arabian government are still seeking release of the documents.
[351][352] In June 2016, CIA chief John Brennan says that he believes 28 redacted pages of a congressional inquiry into 9/11 will soon be made public, and that they will prove that the government of Saudi Arabia had no involvement in the September 11 attacks.
In September 2016, the Congress passed the Justice Against Sponsors of Terrorism Act that would allow relatives of victims of the September 11 attacks to sue Saudi Arabia for its government's alleged role in the attacks.
The National Commission on Terrorist Attacks Upon the United States (9/11 Commission), chaired by Thomas Kean and Lee H. Hamilton, was formed in late 2002 to prepare a thorough account of the circumstances surrounding the attacks, including preparedness for and the immediate response to the attacks.
[357] On July 22, 2004, the Commission issued the 9/11 Commission Report.
The report detailed the events of 9/11, found the attacks were carried out by members of al-Qaeda, and examined how security and intelligence agencies were inadequately coordinated to prevent the attacks.
Formed from an independent bipartisan group of mostly former Senators, Representatives, and Governors, the commissioners explained, "We believe the 9/11 attacks revealed four kinds of failures: in imagination, policy, capabilities, and management".
[358] The Commission made numerous recommendations on how to prevent future attacks, and in 2011 was dismayed that several of its recommendations had yet to be implemented.
The U.S. National Institute of Standards and Technology (NIST) investigated the collapses of the Twin Towers and 7 WTC.
The investigations examined why the buildings collapsed and what fire protection measures were in place, and evaluated how fire protection systems might be improved in future construction.
[360] The investigation into the collapse of 1 WTC and 2 WTC was concluded in October 2005 and that of 7 WTC was completed in August 2008.
NIST found that the fireproofing on the Twin Towers' steel infrastructures was blown off by the initial impact of the planes and that, had this not occurred, the towers likely would have remained standing.
[362] A 2007 study of the north tower's collapse published by researchers of Purdue University determined that, since the plane's impact had stripped off much of the structure's thermal insulation, the heat from a typical office fire would have softened and weakened the exposed girders and columns enough to initiate the collapse regardless of the number of columns cut or damaged by the impact.
The director of the original investigation stated that "the towers really did amazingly well.
The terrorist aircraft didn't bring the buildings down; it was the fire which followed.
It was proven that you could take out two-thirds of the columns in a tower and the building would still stand.
"[365] The fires weakened the trusses supporting the floors, making the floors sag.
The sagging floors pulled on the exterior steel columns causing the exterior columns to bow inward.
With the damage to the core columns, the buckling exterior columns could no longer support the buildings, causing them to collapse.
Additionally, the report found the towers' stairwells were not adequately reinforced to provide adequate emergency escape for people above the impact zones.
[366] NIST concluded that uncontrolled fires in 7 WTC caused floor beams and girders to heat and subsequently "caused a critical support column to fail, initiating a fire-induced progressive collapse that brought the building down".
On the day of the attacks, New York City mayor Rudy Giuliani stated: "We will rebuild.
We're going to come out of this stronger than before, politically stronger, economically stronger.
The skyline will be made whole again.
The damaged section of the Pentagon was rebuilt and occupied within a year of the attacks.
[368] The temporary World Trade Center PATH station opened in late 2003 and construction of the new 7 World Trade Center was completed in 2006.
Work on rebuilding the main World Trade Center site was delayed until late 2006 when leaseholder Larry Silverstein and the Port Authority of New York and New Jersey agreed on financing.
[369] The construction of One World Trade Center began on April 27, 2006, and reached its full height on May 20, 2013.
The spire was installed atop the building at that date, putting 1 WTC's height at 1,776 feet (541 m) and thus claiming the title of the tallest building in the Western Hemisphere.
[370] One WTC finished construction and opened on November 3, 2014.
On the World Trade Center site, three more office towers are expected to be built one block east of where the original towers stood.
Construction has begun on all three of these towers.
In the days immediately following the attacks, many memorials and vigils were held around the world, and photographs of the dead and missing were posted around Ground Zero.
A witness described being unable to "get away from faces of innocent victims who were killed.
Their pictures are everywhere, on phone booths, street lights, walls of subway stations.
Everything reminded me of a huge funeral, people quiet and sad, but also very nice.
Before, New York gave me a cold feeling; now people were reaching out to help each other.
One of the first memorials was the Tribute in Light, an installation of 88 searchlights at the footprints of the World Trade Center towers.
[374] In New York City, the World Trade Center Site Memorial Competition was held to design an appropriate memorial on the site.
[375] The winning design, Reflecting Absence, was selected in August 2006, and consists of a pair of reflecting pools in the footprints of the towers, surrounded by a list of the victims' names in an underground memorial space.
[376] The memorial was completed on September 11, 2011;[377] a museum also opened on site on May 21, 2014.
In Arlington County, The Pentagon Memorial was completed and opened to the public on the seventh anniversary of the attacks in 2008.
[379][380] It consists of a landscaped park with 184 benches facing the Pentagon.
[381] When the Pentagon was repaired in 2001–2002, a private chapel and indoor memorial were included, located at the spot where Flight 77 crashed into the building.
In Shanksville, a concrete and glass visitor center was opened on September 10, 2015,[383] situated on a hill overlooking the crash site and the white marble Wall of Names.
[384] An observation platform at the visitor center and the white marble wall are both aligned beneath the path of Flight 93.
[384][385] A temporary memorial is located 500 yards (457 m) from the crash site.
[386] New York City firefighters donated a cross made of steel from the World Trade Center and mounted on top of a platform shaped like the Pentagon.
[387] It was installed outside the firehouse on August 25, 2008.
[388] Many other permanent memorials are elsewhere.
Scholarships and charities have been established by the victims' families, and by many other organizations and private figures.
On every anniversary, in New York City, the names of the victims who died there are read out against a background of somber music.
The President of the United States attends a memorial service at the Pentagon,[390] and asks Americans to observe Patriot Day with a moment of silence.
Smaller services are held in Shanksville, Pennsylvania, which are usually attended by the President's spouse.
Coordinates: 40°N 100°W﻿ / ﻿40°N 100°W﻿ / 40; -100
The United States of America (USA), commonly known as the United States (U.S. or US) or America, is a country comprising 50 states, a federal district, five major self-governing territories, and various possessions.
[g] At 3.8 million square miles (9.8 million km2), the United States is the world's third or fourth largest country by total area[h] and is slightly smaller than the entire continent of Europe's 3.9 million square miles (10.1 million km2).
With a population of over 327 million people, the U.S. is the third most populous country.
The capital is Washington, D.C., and the largest city by population is New York City.
Forty-eight states and the capital's federal district are contiguous in North America between Canada and Mexico.
The State of Alaska is in the northwest corner of North America, bordered by Canada to the east and across the Bering Strait from Russia to the west.
The State of Hawaii is an archipelago in the mid-Pacific Ocean.
The U.S. territories are scattered about the Pacific Ocean and the Caribbean Sea, stretching across nine official time zones.
The extremely diverse geography, climate, and wildlife of the United States make it one of the world's 17 megadiverse countries.
Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago.
[22] European colonization began in the 16th century.
The United States emerged from the thirteen British colonies established along the East Coast.
Numerous disputes between Great Britain and the colonies following the French and Indian War led to the American Revolution, which began in 1775, and the subsequent Declaration of Independence in 1776.
The war ended in 1783 with the United States becoming the first country to gain independence from a European power.
[23] The current constitution was adopted in 1788, with the first ten amendments, collectively named the Bill of Rights, being ratified in 1791 to guarantee many fundamental civil liberties.
The United States embarked on a vigorous expansion across North America throughout the 19th century, acquiring new territories,[24] displacing Native American tribes, and gradually admitting new states until it spanned the continent by 1848.
During the second half of the 19th century, the Civil War led to the abolition of slavery.
[25][26] By the end of the century, the United States had extended into the Pacific Ocean,[27] and its economy, driven in large part by the Industrial Revolution, began to soar.
[28] The Spanish–American War and World War I confirmed the country's status as a global military power.
The United States emerged from World War II as a global superpower, the first country to develop nuclear weapons, the only country to use them in warfare, and a permanent member of the United Nations Security Council.
Sweeping civil rights legislation, notably the Civil Rights Act of 1964, the Voting Rights Act of 1965 and the Fair Housing Act of 1968, outlawed discrimination based on race or color.
During the Cold War, the United States and the Soviet Union competed in the Space Race, culminating with the 1969 U.S.
Moon landing.
The end of the Cold War and the collapse of the Soviet Union in 1991 left the United States as the world's sole superpower.
The United States is the world's oldest surviving federation.
It is a federal republic and a representative democracy.
The United States is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States (OAS), and other international organizations.
The United States is a highly developed country, with the world's largest economy by nominal GDP and second-largest economy by PPP, accounting for approximately a quarter of global GDP.
[30] The U.S. economy is largely post-industrial, characterized by the dominance of services and knowledge-based activities, although the manufacturing sector remains the second-largest in the world.
[31] The United States is the world's largest importer and the second largest exporter of goods, by value.
[32][33] Although its population is only 4.3% of the world total,[34] the U.S. holds 31% of the total wealth in the world, the largest share of global wealth concentrated in a single country.
Despite income and wealth disparities, the United States continues to rank very high in measures of socioeconomic performance, including average wage, human development, per capita GDP, and worker productivity.
[36][37] The United States is the foremost military power in the world, making up a third of global military spending,[38] and is a leading political, cultural, and scientific force internationally.
In 1507, the German cartographer Martin Waldseemüller produced a world map on which he named the lands of the Western Hemisphere America in honor of the Italian explorer and cartographer Amerigo Vespucci (Latin: Americus Vespucius).
[41] The first documentary evidence of the phrase "United States of America" is from a letter dated January 2, 1776, written by Stephen Moylan, Esq., to George Washington's aide-de-camp and Muster-Master General of the Continental Army, Lt. Col. Joseph Reed.
Moylan expressed his wish to go "with full and ample powers from the United States of America to Spain" to seek assistance in the revolutionary war effort.
[42][43][44] The first known publication of the phrase "United States of America" was in an anonymous essay in The Virginia Gazette newspaper in Williamsburg, Virginia, on April 6, 1776.
The second draft of the Articles of Confederation, prepared by John Dickinson and completed by June 17, 1776, at the latest, declared "The name of this Confederation shall be the 'United States of America'".
[46] The final version of the Articles sent to the states for ratification in late 1777 contains the sentence "The Stile of this Confederacy shall be 'The United States of America'".
[47] In June 1776, Thomas Jefferson wrote the phrase "UNITED STATES OF AMERICA" in all capitalized letters in the headline of his "original Rough draught" of the Declaration of Independence.
[46] This draft of the document did not surface until June 21, 1776, and it is unclear whether it was written before or after Dickinson used the term in his June 17 draft of the Articles of Confederation.
The short form "United States" is also standard.
Other common forms are the "U.S.", the "USA", and "America".
Colloquial names are the "U.S. of A."
and, internationally, the "States".
"Columbia", a name popular in poetry and songs of the late 18th century, derives its origin from Christopher Columbus; it appears in the name "District of Columbia".
The phrase "United States" was originally plural, a description of a collection of independent states—e.g., "the United States are"—including in the Thirteenth Amendment to the United States Constitution, ratified in 1865.
[49] The singular form—e.g., "the United States is"—became popular after the end of the American Civil War.
The singular form is now standard; the plural form is retained in the idiom "these United States".
The difference is more significant than usage; it is a difference between a collection of states and a unit.
A citizen of the United States is an "American".
"United States", "American" and "U.S." refer to the country adjectivally ("American values", "U.S. forces").
In English, the word "American" rarely refers to topics or subjects not directly connected with the United States.
It has been generally accepted that the first inhabitants of North America migrated from Siberia by way of the Bering land bridge and arrived at least 12,000 years ago; however, increasing evidence suggests an even earlier arrival.
[22][52][53] After crossing the land bridge, the first Americans moved southward along the Pacific coast[54] and through an interior ice-free corridor between the Cordilleran and Laurentide ice sheets.
[55] The Clovis culture appeared around 11,000 BC, and is considered to be an ancestor of most of the later indigenous cultures of the Americas.
[56] The Clovis culture was believed to represent the first human settlement of the Americas.
[57] Over the years, more and more evidence has advanced the idea of "pre-Clovis" cultures including tools dating back about 15,550 years ago.
It is likely these represent the first of three major waves of migrations into North America.
Over time, indigenous cultures in North America grew increasingly complex, and some, such as the pre-Columbian Mississippian culture in the southeast, developed advanced agriculture, grand architecture, and state-level societies.
[59] The Mississippian culture flourished in the south from 800 to 1600 AD, extending from the Mexican border down through Florida.
[60] Its city state Cahokia is considered the largest, most complex pre-Columbian archaeological site in the modern-day United States.
[61] In the Four Corners region, Ancestral Puebloans culture developed as the culmination of centuries of agricultural experimentation, which produced greater dependence on farming.
[62] Three UNESCO World Heritage Sites in the United States are credited to the Pueblos: Mesa Verde National Park, Chaco Culture National Historical Park, and Taos Pueblo.
[63][64] The earthworks constructed by Native Americans of the Poverty Point culture in northeastern Louisiana have also been designated a UNESCO World Heritage site.
In the southern Great Lakes region, the Iroquois Confederacy (Haudenosaunee) was established at some point between the twelfth and fifteenth centuries.
The date of the first settlements of the Hawaiian Islands is a topic of continuing debate.
[66] Archaeological evidence seems to indicate a settlement as early as 124 AD.
While estimating the original native population of North America at the time of European contact is difficult, an attempt was made in the early part of the twentieth century by James Mooney using historic records to estimate the indigenous population north of Mexico in 1600.
[68][69] In more recent years, Douglas H. Ubelaker of the Smithsonian Institution has updated these figures.
[70] While Ubelaker estimated that there was a population of 92,916 in the south Atlantic states and a population of 473,616 in the Gulf states, most academics regard the figure as too low.
[68] Anthropologist Henry F. Dobyns believed that the populations were much higher, suggestion 1,100,000 along the shores of the gulf of Mexico, 2,211,000 people living between Florida and Massachusetts, 5,250,000 in the Mississippi Valley and tributaries and 697,000 people in the Florida peninsula.
The first interaction between Europeans and Native Americans was made by the Norsemen.
A number of surviving Norse sagas provide information regarding The Maritimes and its indigenous people.
The Norse attempted to settle in North America about 500 years before Columbus.
In the early days of colonization, many European settlers were subject to food shortages, disease, and attacks from Native Americans.
Native Americans were also often at war with neighboring tribes and allied with Europeans in their colonial wars.
At the same time, however, many natives and settlers came to depend on each other.
Settlers traded for food and animal pelts, natives for guns, ammunition and other European wares.
[74] Natives taught many settlers where, when and how to cultivate corn, beans, and squash.
European missionaries and others felt it was important to "civilize" the Native Americans and urged them to adopt European agricultural techniques and lifestyles.
With the advancement of European colonization in the territories of the contemporary United States, the Native Americans were often conquered and displaced.
[78] The first Europeans to arrive in the territory of the modern United States were Spanish conquistadors such as Juan Ponce de León, who made his first visit to Florida in 1513; however, if unincorporated territories are accounted for, then credit would go to Christopher Columbus who landed in Puerto Rico on his 1493 voyage.
The Spanish set up the first settlements in Florida and New Mexico such as Saint Augustine[77] and Santa Fe.
The French established their own as well along the Mississippi River.
Successful English settlement on the eastern coast of North America began with the Virginia Colony in 1607 at Jamestown and the Pilgrims' Plymouth Colony in 1620.
Many settlers were dissenting Christian groups who came seeking religious freedom.
The continent's first elected legislative assembly, Virginia's House of Burgesses created in 1619, the Mayflower Compact, signed by the Pilgrims before disembarking, and the Fundamental Orders of Connecticut, established precedents for the pattern of representative self-government and constitutionalism that would develop throughout the American colonies.
Most settlers in every colony were small farmers, but other industries developed within a few decades as varied as the settlements.
Cash crops included tobacco, rice, and wheat.
Extraction industries grew up in furs, fishing and lumber.
Manufacturers produced rum and ships, and by the late colonial period, Americans were producing one-seventh of the world's iron supply.
[81] Cities eventually dotted the coast to support local economies and serve as trade hubs.
English colonists were supplemented by waves of Scotch-Irish and other groups.
As coastal land grew more expensive, freed indentured servants pushed further west.
A large-scale slave trade with English privateers was begun.
[83] The life expectancy of slaves was much higher in North America than further south, because of less disease and better food and treatment, leading to a rapid increase in the numbers of slaves.
[84][85] Colonial society was largely divided over the religious and moral implications of slavery, and colonies passed acts for and against the practice.
[86][87] But by the turn of the 18th century, African slaves were replacing indentured servants for cash crop labor, especially in southern regions.
With the British colonization of Georgia in 1732, the 13 colonies that would become the United States of America were established.
[89] All had local governments with elections open to most free men, with a growing devotion to the ancient rights of Englishmen and a sense of self-government stimulating support for republicanism.
[90] With extremely high birth rates, low death rates, and steady settlement, the colonial population grew rapidly.
Relatively small Native American populations were eclipsed.
[91] The Christian revivalist movement of the 1730s and 1740s known as the Great Awakening fueled interest in both religion and religious liberty.
During the Seven Years' War (in the United States, known as the French and Indian War), British forces seized Canada from the French, but the francophone population remained politically isolated from the southern colonies.
Excluding the Native Americans, who were being conquered and displaced, the 13 British colonies had a population of over 2.1 million in 1770, about one-third that of Britain.
Despite continuing new arrivals, the rate of natural increase was such that by the 1770s only a small minority of Americans had been born overseas.
[93] The colonies' distance from Britain had allowed the development of self-government, but their success motivated monarchs to periodically seek to reassert royal authority.
In 1774, the Spanish Navy ship Santiago, under Juan Pérez, entered and anchored in an inlet of Nootka Sound, Vancouver Island, in present-day British Columbia.
Although the Spanish did not land, natives paddled to the ship to trade furs for abalone shells from California.
[95] At the time, the Spanish were able to monopolize the trade between Asia and North America, granting limited licenses to the Portuguese.
When the Russians began establishing a growing fur trading system in Alaska, the Spanish began to challenge the Russians, with Pérez's voyage being the first of many to the Pacific Northwest.
During his third and final voyage, Captain James Cook became the first European to begin formal contact with Hawaii.
After his initial landfall in January 1778 at Waimea harbor, Kauai, Cook named the archipelago the "Sandwich Islands" after the fourth Earl of Sandwich—the acting First Lord of the Admiralty of the British Royal Navy.
[98] Captain James Cook's last voyage included sailing along the coast of North America and Alaska searching for a Northwest Passage for approximately nine months.
After having arrived in the Hawaiian islands in 1778, Captain Cook sailed north and then northeast to explore the west coast of North America north of the Spanish settlements in Alta California.
He made landfall on the Oregon coast at approximately 44°30′ north latitude, naming his landing point Cape Foulweather.
Bad weather forced his ships south to about 43° north before they could begin their exploration of the coast northward.
[99] In March 1778, Cook landed on Bligh Island and named the inlet "King George's Sound".
He recorded that the native name was Nutka or Nootka, apparently misunderstanding his conversations at Friendly Cove/Yuquot; his informant may have been explaining that he was on an island (itchme nutka, a place you can "go around").
There may also have been confusion with Nuu-chah-nulth, the natives' autonym (a name for themselves).
It may also have simply been based on Cook's mispronunciation of Yuquot, the native name of the place.
[100] He returned to Hawaii to resupply, initially exploring the coasts of Maui and the big island, trading with locals and then making anchor at Kealakekua Bay in January 1779.
When his ships and company left the islands, a ship's mast broke in bad weather, forcing them to return in mid-February.
Cook would be killed days later.
[101] [j][k]
The American Revolutionary War was the first successful colonial war of independence against a European power.
Americans had developed an ideology of "republicanism" asserting that government rested on the will of the people as expressed in their local legislatures.
They demanded their rights as Englishmen and "no taxation without representation".
The British insisted on administering the empire through Parliament, and the conflict escalated into war.
The Second Continental Congress unanimously adopted the Declaration of Independence on July 4, which recognized, in a long preamble, that all men are created equal and endowed by their Creator with unalienable rights and that those rights were not being protected by Great Britain, and declared, in the words of the resolution, that the thirteen United Colonies formed an independent nation and had no further allegiance to the British crown.
The fourth day of July is celebrated annually as Independence Day.
[115] The Second Continental Congress declared on September 9 "where, heretofore, the words 'United Colonies' have been used, the stile be altered for the future to the 'United States' ".
[116] In 1777, the Articles of Confederation established a weak government that operated until 1789.
Britain recognized the independence of the United States following its defeat at Yorktown in 1781.
[117] In the peace treaty of 1783, American sovereignty was recognized from the Atlantic coast west to the Mississippi River.
Nationalists led the Philadelphia Convention of 1787 in writing the United States Constitution, ratified in state conventions in 1788.
The federal government was reorganized into three branches, on the principle of creating salutary checks and balances, in 1789.
George Washington, who had led the revolutionary army to victory, was the first president elected under the new constitution.
The Bill of Rights, forbidding federal restriction of personal freedoms and guaranteeing a range of legal protections, was adopted in 1791.
Although the federal government criminalized the international slave trade in 1808, after 1820, cultivation of the highly profitable cotton crop exploded in the Deep South, and along with it, the slave population.
[119][120][121] The Second Great Awakening, especially 1800–1840, converted millions to evangelical Protestantism.
In the North, it energized multiple social reform movements, including abolitionism;[122] in the South, Methodists and Baptists proselytized among slave populations.
Americans' eagerness to expand westward prompted a long series of American Indian Wars.
[124] The Louisiana Purchase of French-claimed territory in 1803 almost doubled the nation's area.
[125] The War of 1812, declared against Britain over various grievances and fought to a draw, strengthened U.S.
[126] A series of military incursions into Florida led Spain to cede it and other Gulf Coast territory in 1819.
[127] The expansion was aided by steam power, when steamboats began traveling along America's large water systems, which were connected by new canals, such as the Erie and the I&M; then, even faster railroads began their stretch across the nation's land.
From 1820 to 1850, Jacksonian democracy began a set of reforms which included wider white male suffrage; it led to the rise of the Second Party System of Democrats and Whigs as the dominant parties from 1828 to 1854.
The Trail of Tears in the 1830s exemplified the Indian removal policy that resettled Indians into the west on Indian reservations.
The U.S. annexed the Republic of Texas in 1845 during a period of expansionist Manifest destiny.
[129] The 1846 Oregon Treaty with Britain led to U.S. control of the present-day American Northwest.
[130] Victory in the Mexican–American War resulted in the 1848 Mexican Cession of California and much of the present-day American Southwest.
The California Gold Rush of 1848–49 spurred western migration, the California Genocide[132][133][134][135] and the creation of additional western states.
[136] After the American Civil War, new transcontinental railways made relocation easier for settlers, expanded internal trade and increased conflicts with Native Americans.
[137] Over a half-century, the loss of the American bison (sometimes called "buffalo") was an existential blow to many Plains Indians culture.
[138] In 1869, a new Peace Policy nominally promised to protect Native-Americans from abuses, avoid further war, and secure their eventual U.S. citizenship.
Nonetheless, large-scale conflicts continued throughout the West into the 1900s.
Differences of opinion regarding the slavery of Africans and African Americans ultimately led to the American Civil War.
[139] Initially, states entering the Union had alternated between slave and free states, keeping a sectional balance in the Senate, while free states outstripped slave states in population and in the House of Representatives.
But with additional western territory and more free-soil states, tensions between slave and free states mounted with arguments over federalism and disposition of the territories, whether and how to expand or restrict slavery.
With the 1860 election of Abraham Lincoln, the first president from the largely anti-slavery Republican Party, conventions in thirteen slave states ultimately declared secession and formed the Confederate States of America (the "South"), while the federal government (the "Union") maintained that secession was illegal.
[140] In order to bring about this secession, military action was initiated by the secessionists, and the Union responded in kind.
The ensuing war would become the deadliest military conflict in American history, resulting in the deaths of approximately 618,000 soldiers as well as many civilians.
[141] The South fought for the freedom to own slaves, while the Union at first simply fought to maintain the country as one united whole.
Nevertheless, as casualties mounted after 1863 and Lincoln delivered his Emancipation Proclamation, the main purpose of the war from the Union's viewpoint became the abolition of slavery.
Indeed, when the Union ultimately won the war in April 1865, each of the states in the defeated South was required to ratify the Thirteenth Amendment, which prohibited slavery.
Three amendments were added to the U.S. Constitution in the years after the war: the aforementioned Thirteenth as well as the Fourteenth Amendment providing citizenship to the nearly four million African Americans who had been slaves,[142] and the Fifteenth Amendment ensuring in theory that African Americans had the right to vote.
The war and its resolution led to a substantial increase in federal power[143] aimed at reintegrating and rebuilding the South while guaranteeing the rights of the newly freed slaves.
Reconstruction began in earnest following the war.
While President Lincoln attempted to foster friendship and forgiveness between the Union and the former Confederacy, an assassin's bullet on April 14, 1865, drove a wedge between North and South again.
Republicans in the federal government made it their goal to oversee the rebuilding of the South and to ensure the rights of African Americans.
They persisted until the Compromise of 1877 when the Republicans agreed to cease protecting the rights of African Americans in the South in order for Democrats to concede the presidential election of 1876.
Southern white Democrats, calling themselves "Redeemers", took control of the South after the end of Reconstruction.
From 1890 to 1910, so-called Jim Crow laws disenfranchised most blacks and some poor whites throughout the region.
Blacks faced racial segregation, especially in the South.
[144] They also occasionally experienced vigilante violence, including lynching.
In the North, urbanization and an unprecedented influx of immigrants from Southern and Eastern Europe supplied a surplus of labor for the country's industrialization and transformed its culture.
[148] National infrastructure including telegraph and transcontinental railroads spurred economic growth and greater settlement and development of the American Old West.
The later invention of electric light and the telephone would also affect communication and urban life.
The United States fought Indian Wars west of the Mississippi River from 1810 to at least 1890.
[150] Most of these conflicts ended with the cession of Native American territory and the confinement of the latter to Indian reservations.
This further expanded acreage under mechanical cultivation, increasing surpluses for international markets.
[151] Mainland expansion also included the purchase of Alaska from Russia in 1867.
[152] In 1893, pro-American elements in Hawaii overthrew the monarchy and formed the Republic of Hawaii, which the U.S. annexed in 1898.
Puerto Rico, Guam, and the Philippines were ceded by Spain in the same year, following the Spanish–American War.
[153] American Samoa was acquired by the United States in 1900 after the end of the Second Samoan Civil War.
[154] The United States purchased the U.S. Virgin Islands from Denmark in 1917.
Rapid economic development during the late 19th and early 20th centuries fostered the rise of many prominent industrialists.
Tycoons like Cornelius Vanderbilt, John D. Rockefeller, and Andrew Carnegie led the nation's progress in railroad, petroleum, and steel industries.
Banking became a major part of the economy, with J. P. Morgan playing a notable role.
Edison and Tesla undertook the widespread distribution of electricity to industry, homes, and for street lighting.
Henry Ford revolutionized the automotive industry.
The American economy boomed, becoming the world's largest, and the United States achieved great power status.
[156] These dramatic changes were accompanied by social unrest and the rise of populist, socialist, and anarchist movements.
[157] This period eventually ended with the advent of the Progressive Era, which saw significant reforms in many societal areas, including women's suffrage, alcohol prohibition, regulation of consumer goods, greater antitrust measures to ensure competition and attention to worker conditions.
The United States remained neutral from the outbreak of World War I in 1914 until 1917, when it joined the war as an "associated power", alongside the formal Allies of World War I, helping to turn the tide against the Central Powers.
In 1919, President Woodrow Wilson took a leading diplomatic role at the Paris Peace Conference and advocated strongly for the U.S. to join the League of Nations.
However, the Senate refused to approve this and did not ratify the Treaty of Versailles that established the League of Nations.
In 1920, the women's rights movement won passage of a constitutional amendment granting women's suffrage.
[162] The 1920s and 1930s saw the rise of radio for mass communication and the invention of early television.
[163] The prosperity of the Roaring Twenties ended with the Wall Street Crash of 1929 and the onset of the Great Depression.
After his election as president in 1932, Franklin D. Roosevelt responded with the New Deal, which included the establishment of the Social Security system.
[164] The Great Migration of millions of African Americans out of the American South began before World War I and extended through the 1960s;[165] whereas the Dust Bowl of the mid-1930s impoverished many farming communities and spurred a new wave of western migration.
At first effectively neutral during World War II while Germany conquered much of continental Europe, the United States began supplying material to the Allies in March 1941 through the Lend-Lease program.
On December 7, 1941, the Empire of Japan launched a surprise attack on Pearl Harbor, prompting the United States to join the Allies against the Axis powers.
[167] During the war, the United States was referred as one of the "Four Policemen"[168] of Allies power who met to plan the postwar world, along with Britain, the Soviet Union and China.
[169][170] Although the nation lost more than 400,000 soldiers,[171] it emerged relatively undamaged from the war with even greater economic and military influence.
The United States played a leading role in the Bretton Woods and Yalta conferences with the United Kingdom, the Soviet Union, and other Allies, which signed agreements on new international financial institutions and Europe's postwar reorganization.
As an Allied victory was won in Europe, a 1945 international conference held in San Francisco produced the United Nations Charter, which became active after the war.
[173] The United States developed the first nuclear weapons and used them on Japan in the cities of Hiroshima and Nagasaki; causing the Japanese to surrender on September 2, ending World War II.
[174][175] Parades and celebrations followed in what is known as Victory Day, or V-J Day.
After World War II the United States and the Soviet Union jockeyed for power during what became known as the Cold War, driven by an ideological divide between capitalism and communism[177] and, according to the school of geopolitics, a divide between the maritime Atlantic and the continental Eurasian camps.
They dominated the military affairs of Europe, with the U.S. and its NATO allies on one side and the USSR and its Warsaw Pact allies on the other.
The U.S. developed a policy of containment towards the expansion of communist influence.
While the U.S. and Soviet Union engaged in proxy wars and developed powerful nuclear arsenals, the two countries avoided direct military conflict.
The United States often opposed Third World movements that it viewed as Soviet-sponsored, and occasionally pursued direct action for regime change against left-wing governments.
[178] American troops fought communist Chinese and North Korean forces in the Korean War of 1950–53.
[179] The Soviet Union's 1957 launch of the first artificial satellite and its 1961 launch of the first manned spaceflight initiated a "Space Race" in which the United States became the first nation to land a man on the moon in 1969.
[179] A proxy war in Southeast Asia eventually evolved into full American participation, as the Vietnam War.
At home, the U.S. experienced sustained economic expansion and a rapid growth of its population and middle class.
Construction of an Interstate Highway System transformed the nation's infrastructure over the following decades.
Millions moved from farms and inner cities to large suburban housing developments.
[180][181] In 1959 Hawaii became the 50th and last U.S. state added to the country.
[182] The growing Civil Rights Movement used nonviolence to confront segregation and discrimination, with Martin Luther King Jr. becoming a prominent leader and figurehead.
A combination of court decisions and legislation, culminating in the Civil Rights Act of 1968, sought to end racial discrimination.
[183][184][185] Meanwhile, a counterculture movement grew which was fueled by opposition to the Vietnam war, black nationalism, and the sexual revolution.
The launch of a "War on Poverty" expanded entitlements and welfare spending, including the creation of Medicare and Medicaid, two programs that provide health coverage to the elderly and poor, respectively, and the means-tested Food Stamp Program and Aid to Families with Dependent Children.
The 1970s and early 1980s saw the onset of stagflation.
After his election in 1980, President Ronald Reagan responded to economic stagnation with free-market oriented reforms.
Following the collapse of détente, he abandoned "containment" and initiated the more aggressive "rollback" strategy towards the USSR.
[187][188][189][190][191] After a surge in female labor participation over the previous decade, by 1985 the majority of women aged 16 and over were employed.
The late 1980s brought a "thaw" in relations with the USSR, and its collapse in 1991 finally ended the Cold War.
[193][194][195][196] This brought about unipolarity[197] with the U.S. unchallenged as the world's dominant superpower.
The concept of Pax Americana, which had appeared in the post-World War II period, gained wide popularity as a term for the post-Cold War new world order.
After the Cold War, the conflict in the Middle East triggered a crisis in 1990, when Iraq under Saddam Hussein invaded and attempted to annex Kuwait, an ally of the United States.
Fearing that the instability would spread to other regions, President George H.W.
Bush launched Operation Desert Shield, a defensive force buildup in Saudi Arabia, and Operation Desert Storm, in a staging titled the Gulf War; waged by coalition forces from 34 nations, led by the United States against Iraq ending in the successful expulsion of Iraqi forces from Kuwait, restoring the former monarchy.
Originating in U.S. defense networks, the Internet spread to international academic networks, and then to the public in the 1990s, greatly affecting the global economy, society, and culture.
Due to the dot-com boom, stable monetary policy under Alan Greenspan, and reduced social welfare spending, the 1990s saw the longest economic expansion in modern U.S. history, ending in 2001.
[200] Beginning in 1994, the U.S. entered into the North American Free Trade Agreement (NAFTA), linking 450 million people producing $17 trillion worth of goods and services.
The goal of the agreement was to eliminate trade and investment barriers among the U.S., Canada, and Mexico by January 1, 2008.
Trade among the three partners has soared since NAFTA went into force.
On September 11, 2001, Al-Qaeda terrorists struck the World Trade Center in New York City and the Pentagon near Washington, D.C., killing nearly 3,000 people.
[202] In response, the United States launched the War on Terror, which included war in Afghanistan and the 2003–11 Iraq War.
[203][204] In 2007, the Bush administration ordered a major troop surge in the Iraq War,[205] which successfully reduced violence and led to greater stability in the region.
Government policy designed to promote affordable housing,[208] widespread failures in corporate and regulatory governance,[209] and historically low interest rates set by the Federal Reserve[210] led to the mid-2000s housing bubble, which culminated with the 2008 financial crisis, the largest economic contraction in the nation's history since the Great Depression.
[211] Barack Obama, the first African-American[212] and multiracial[213] president, was elected in 2008 amid the crisis,[214] and subsequently passed stimulus measures and the Dodd-Frank Wall Street Reform and Consumer Protection Act in an attempt to mitigate its negative effects and ensure there would not be a repeat of the crisis.
The stimulus facilitated infrastructure improvements[215] and a relative decline in unemployment.
[216] Dodd-Frank improved financial stability and consumer protection,[217] although there has been debate about its effects on the economy.
In 2010, the Obama administration passed the Affordable Care Act, which made the most sweeping reforms to the nation's healthcare system in nearly five decades, including mandates, subsidies and insurance exchanges.
The law caused a significant reduction in the number and percentage of people without health insurance, with 24 million covered during 2016,[219] but remains controversial due to its impact on healthcare costs, insurance premiums, and economic performance.
[220] Although the recession reached its trough in June 2009, voters remained frustrated with the slow pace of the economic recovery.
The Republicans, who stood in opposition to Obama's policies, won control of the House of Representatives with a landslide in 2010 and control of the Senate in 2014.
American forces in Iraq were withdrawn in large numbers in 2009 and 2010, and the war in the region was declared formally over in December 2011.
[222] The withdrawal caused an escalation of sectarian insurgency,[223] leading to the rise of the Islamic State of Iraq and the Levant, the successor of al-Qaeda in the region.
[224] In 2014, Obama announced a restoration of full diplomatic relations with Cuba for the first time since 1961.
[needs update][225] The next year, the United States as a member of the P5+1 countries signed the Joint Comprehensive Plan of Action, an agreement aimed to slow the development of Iran's nuclear program,[226] though the U.S. withdrew from the deal in May 2018.
[227] In the United States presidential election of 2016, Republican Donald Trump was elected as the 45th president of the United States.
Trump is both the oldest and wealthiest person elected president in United States history.
The land area of the entire United States is approximately 3,800,000 square miles (9,841,955 km2),[229] with the contiguous United States making up 2,959,064 square miles (7,663,940.6 km2) of that.
Alaska, separated from the contiguous United States by Canada, is the largest state at 663,268 square miles (1,717,856.2 km2).
Hawaii, occupying an archipelago in the central Pacific, southwest of North America, is 10,931 square miles (28,311 km2) in area.
The populated territories of Puerto Rico, American Samoa, Guam, Northern Mariana Islands, and U.S. Virgin Islands together cover 9,185 square miles (23,789 km2).
[230] Measured by only land area, the United States is third in size behind Russia and China, just ahead of Canada.
The United States is the world's third- or fourth-largest nation by total area (land and water), ranking behind Russia and Canada and just above or below China.
The ranking varies depending on how two territories disputed by China and India are counted, and how the total size of the United States is measured.
[h] The Encyclopædia Britannica, for instance, lists the size of the United States as 3,677,649 square miles (9,525,067 km2), as they do not count the country's coastal or territorial waters.
[232] The World Factbook, which includes those waters, gives 3,796,742 square miles (9,833,517 km2).
The coastal plain of the Atlantic seaboard gives way further inland to deciduous forests and the rolling hills of the Piedmont.
[234] The Appalachian Mountains divide the eastern seaboard from the Great Lakes and the grasslands of the Midwest.
[235] The Mississippi–Missouri River, the world's fourth longest river system, runs mainly north–south through the heart of the country.
The flat, fertile prairie of the Great Plains stretches to the west, interrupted by a highland region in the southeast.
The Rocky Mountains, at the western edge of the Great Plains, extend north to south across the country, reaching altitudes higher than 14,000 feet (4,300 m) in Colorado.
[236] Farther west are the rocky Great Basin and deserts such as the Chihuahua and Mojave.
[237] The Sierra Nevada and Cascade mountain ranges run close to the Pacific coast, both ranges reaching altitudes higher than 14,000 feet (4,300 m).
The lowest and highest points in the contiguous United States are in the state of California,[238] and only about 84 miles (135 km) apart.
[239] At an elevation of 20,310 feet (6,190.5 m), Alaska's Denali (Mount McKinley) is the highest peak in the country and North America.
[240] Active volcanoes are common throughout Alaska's Alexander and Aleutian Islands, and Hawaii consists of volcanic islands.
The supervolcano underlying Yellowstone National Park in the Rockies is the continent's largest volcanic feature.
[241] The United States has the most ecoregions out of any country in the world.
The United States, with its large size and geographic variety, includes most climate types.
To the east of the 100th meridian, the climate ranges from humid continental in the north to humid subtropical in the south.
[243] The Great Plains west of the 100th meridian are semi-arid.
Much of the Western mountains have an alpine climate.
The climate is arid in the Great Basin, desert in the Southwest, Mediterranean in coastal California, and oceanic in coastal Oregon and Washington and southern Alaska.
Most of Alaska is subarctic or polar.
Hawaii and the southern tip of Florida are tropical, as are the populated territories in the Caribbean and the Pacific.
[244] Extreme weather is not uncommon—the states bordering the Gulf of Mexico are prone to hurricanes, and most of the world's tornadoes occur in the country, mainly in Tornado Alley areas in the Midwest and South.
The U.S. ecology is megadiverse: about 17,000 species of vascular plants occur in the contiguous United States and Alaska, and over 1,800 species of flowering plants are found in Hawaii, few of which occur on the mainland.
[247] The United States is home to 428 mammal species, 784 bird species, 311 reptile species, and 295 amphibian species.
[248] About 91,000 insect species have been described.
[249] The bald eagle is both the national bird and national animal of the United States, and is an enduring symbol of the country itself.
There are 59 national parks and hundreds of other federally managed parks, forests, and wilderness areas.
[251] Altogether, the government owns about 28% of the country's land area.
[252] Most of this is protected, though some is leased for oil and gas drilling, mining, logging, or cattle ranching; about .86% is used for military purposes.
Environmental issues have been on the national agenda since 1970.
Environmental controversies include debates on oil and nuclear energy, dealing with air and water pollution, the economic costs of protecting wildlife, logging and deforestation,[255][256] and international responses to global warming.
[257][258] Many federal and state agencies are involved.
The most prominent is the Environmental Protection Agency (EPA), created by presidential order in 1970.
[259] The idea of wilderness has shaped the management of public lands since 1964, with the Wilderness Act.
[260] The Endangered Species Act of 1973 is intended to protect threatened and endangered species and their habitats, which are monitored by the United States Fish and Wildlife Service.
The U.S. Census Bureau estimated the country's population to be 327,167,434 as of July 1, 2018, and to be adding 1 person (net gain) every 13 seconds, or about 6,646 people per day.
[34] The U.S. population almost quadrupled during the 20th century, from 76.2 million in 1900 to 281.4 million in 2000.
[264] The third most populous nation in the world, after China and India, the United States is the only major industrialized nation in which large population increases are projected.
[265] In the 1800s the average woman had 7.04 children;[266] by the 1900s this number had decreased to 3.56.
[267] Since the early 1970s the birth rate has been below the replacement rate of 2.1 with 1.76 children per woman in 2017.
[268] Foreign-born immigration has caused the U.S. population to continue its rapid increase with the foreign-born population doubling from almost 20 million in 1990 to over 40 million in 2010, representing one-third of the population increase.
[269] The foreign-born population reached 45 million in 2015.
[270] The United States has a very diverse population; 37 ancestry groups have more than one million members.
[271] German Americans are the largest ethnic group (more than 50 million) – followed by Irish Americans (circa 37 million), Mexican Americans (circa 31 million) and English Americans (circa 28 million).
White Americans (mostly European ancestry group with 73.1% of total population) are the largest racial group; black Americans are the nation's largest racial minority (note that in the U.S. Census, Hispanic and Latino Americans are counted as an ethnic group, not a "racial" group), and third-largest ancestry group.
[271] Asian Americans are the country's second-largest racial minority; the three largest Asian American ethnic groups are Chinese Americans, Filipino Americans, and Indian Americans.
[271] According to a 2015 survey, the largest American community with European ancestry is German Americans, which consists of more than 14% of the total population.
[274] In 2010, the U.S. population included an estimated 5.2 million people with some American Indian or Alaska Native ancestry (2.9 million exclusively of such ancestry) and 1.2 million with some native Hawaiian or Pacific island ancestry (0.5 million exclusively).
[275] The census counted more than 19 million people of "Some Other Race" who were "unable to identify with any" of its five official race categories in 2010, over 18.5 million (97%) of whom are of Hispanic ethnicity.
The population growth of Hispanic and Latino Americans (the terms are officially interchangeable) is a major demographic trend.
The 50.5 million Americans of Hispanic descent[275] are identified as sharing a distinct "ethnicity" by the Census Bureau; 64% of Hispanic Americans are of Mexican descent.
[276] Between 2000 and 2010, the country's Hispanic population increased 43% while the non-Hispanic population rose just 4.9%.
[277] Much of this growth is from immigration; in 2007, 12.6% of the U.S. population was foreign-born, with 54% of that figure born in Latin America.
The drop in the U.S. fertility rate from 2.08 per woman in 2007 to 1.76 in 2017 was mostly due to the declining birth rate of Hispanics, teenagers, and young women, although the birth rate for older women rose,[285] below the replacement rate of 2.1.
In 2018 the median age of the United States population was 38.1 years.
Minorities (as defined by the Census Bureau as all those beside non-Hispanic, non-multiracial whites) constituted 37.2% of the population in 2012[287] and over 50% of children under age one,[288][282] and are projected to constitute the majority by 2044.
The United States has a birth rate of 13 per 1,000, which is 5 births below the world average.
[289] Its population growth rate is positive at 0.7%, higher than that of many developed nations.
[290] In fiscal year 2016, over one million immigrants (most of whom entered through family reunification) were granted legal residence.
[291] Mexico has been the leading source of new residents since the 1965 Immigration Act.
China, India, and the Philippines have been in the top four sending countries every year since the 1990s.
[292] As of 2012[update], approximately 11.4 million residents are illegal immigrants.
[293] As of 2015[update], 47% of all immigrants are Hispanic, 26% are Asian, 18% are white and 8% are black.
The percentage of immigrants who are Asian is increasing while the percentage who are Hispanic is decreasing.
[270] The estimated number of illegal immigrants dropped to 10.7 million in 2017, down from a peak of 12.2 million in 2007.
In 2017, 33,000 refugees were resettled in the United States.
This was fewer than were resettled in the rest of the world for the first time in decades.
A 2017 Gallup poll concluded that 4.5% of adult Americans identified as LGBT with 5.1% of women identifying as LGBT, compared with 3.9% of men.
[295] The highest percentage came from the District of Columbia (10%), while the lowest state was North Dakota at 1.7%.
About 82% of Americans live in urban areas (including suburbs);[233] about half of those reside in cities with populations over 50,000.
[297] The U.S. has numerous clusters of cities known as megaregions, the largest being the Great Lakes Megalopolis followed by the Northeast Megalopolis and Southern California.
In 2008, 273 incorporated municipalities had populations over 100,000, nine cities had more than one million residents, and four global cities had over two million (New York, Los Angeles, Chicago, and Houston).
[298] There are 52 metropolitan areas with populations greater than one million.
[299] Of the 50 fastest-growing metro areas, 47 are in the West or South.
[300] The metro areas of San Bernardino, Dallas, Houston, Atlanta, and Phoenix all grew by more than a million people between 2000 and 2008.
English (American English) is the de facto national language.
Although there is no official language at the federal level, some laws—such as U.S. naturalization requirements—standardize English.
In 2010, about 230 million, or 80% of the population aged five years and older, spoke only English at home.
Spanish, spoken by 12% of the population at home, is the second most common language and the most widely taught second language.
[302][303] Some Americans advocate making English the country's official language, as it is in 32 states.
Both Hawaiian and English are official languages in Hawaii, by state law.
[305] Alaska recognizes twenty Native languages as well as English.
[306] While neither has an official language, New Mexico has laws providing for the use of both English and Spanish, as Louisiana does for English and French.
[307] Other states, such as California, mandate the publication of Spanish versions of certain government documents including court forms.
Several insular territories grant official recognition to their native languages, along with English: Samoan[309] is officially recognized by American Samoa.
Chamorro[310] is an official language of Guam.
Both Carolinian and Chamorro have official recognition in the Northern Mariana Islands.
Spanish is an official language of Puerto Rico and is more widely spoken than English there.
The most widely taught foreign languages in the United States, in terms of enrollment numbers from kindergarten through university undergraduate education, are: Spanish (around 7.2 million students), French (1.5 million), and German (500,000).
Other commonly taught languages (with 100,000 to 250,000 learners) include Latin, Japanese, ASL, Italian, and Chinese.
[313][314] 18% of all Americans claim to speak at least one language in addition to English.
The First Amendment of the U.S. Constitution guarantees the free exercise of religion and forbids Congress from passing laws respecting its establishment.
In a 2013 survey, 56% of Americans said that religion played a "very important role in their lives", a far higher figure than that of any other wealthy nation.
[319] In a 2009 Gallup poll, 42% of Americans said that they attended church weekly or almost weekly; the figures ranged from a low of 23% in Vermont to a high of 63% in Mississippi.
As with other Western countries, the U.S. is becoming less religious.
Irreligion is growing rapidly among Americans under 30.
[321] Polls show that overall American confidence in organized religion has been declining since the mid to late 1980s,[322] and that younger Americans, in particular, are becoming increasingly irreligious.
[318][323] According to a 2012 study, the Protestant share of the U.S. population had dropped to 48%, thus ending its status as religious category of the majority for the first time.
[324][325] Americans with no religion have 1.7 children compared to 2.2 among Christians.
The unaffiliated are less likely to get married with 37% marrying compared to 52% of Christians.
According to a 2014 survey, 70.6% of adults in the United States identified themselves as Christians;[327] Protestants accounted for 46.5%, while Roman Catholics, at 20.8%, formed the largest single denomination.
[328] In 2014, 5.9% of the U.S. adult population claimed a non-Christian religion.
[318] These include Judaism (1.9%), Hinduism (1.2%), Buddhism (0.9%), and Islam (0.9%).
[318] The survey also reported that 22.8% of Americans described themselves as agnostic, atheist or simply having no religion—up from 8.2% in 1990.
[328][329][330] There are also Unitarian Universalist, Scientologist, Baha'i, Sikh, Jain, Shinto, Confucian, Satanist, Taoist, Druid, Native American, Wiccan, humanist and deist communities.
Protestantism is the largest Christian religious grouping in the United States, accounting for almost half of all Americans.
Baptists collectively form the largest branch of Protestantism at 15.4%,[332] and the Southern Baptist Convention is the largest individual Protestant denomination at 5.3% of the U.S.
[332] Apart from Baptists, other Protestant categories include nondenominational Protestants, Methodists, Pentecostals, unspecified Protestants, Lutherans, Presbyterians, Congregationalists, other Reformed, Episcopalians/Anglicans, Quakers, Adventists, Holiness, Christian fundamentalists, Anabaptists, Pietists, and multiple others.
[332] Two-thirds of American Protestants consider themselves to be born again.
[332] Roman Catholicism in the United States has its origin primarily in the Spanish and French colonization of the Americas, as well as in the English colony of Maryland.
[333] It later grew because of Irish, Italian, Polish, German and Hispanic immigration.
Rhode Island has the highest percentage of Catholics, with 40 percent of the total population.
[334] Utah is the only state where Mormonism is the religion of the majority of the population.
[335] The Mormon Corridor also extends to parts of Arizona, California, Idaho, Nevada and Wyoming.
[336] Eastern Orthodoxy is claimed by 5% of people in Alaska,[337] a former Russian colony, and maintains a presence on the U.S. mainland due to recent immigration from Eastern Europe.
Finally, a number of other Christian groups are active across the country, including the Oneness Pentecostals, Jehovah's Witnesses, Restorationists, Churches of Christ, Christian Scientists, Unitarians and many others.
The Bible Belt is an informal term for a region in the Southern United States in which socially conservative evangelical Protestantism is a significant part of the culture and Christian church attendance across the denominations is generally higher than the nation's average.
By contrast, religion plays the least important role in New England and in the Western United States.
As of 2007[update], 58% of Americans age 18 and over were married, 6% were widowed, 10% were divorced, and 25% had never been married.
[338] Women now work mostly outside the home and receive a majority of bachelor's degrees.
The U.S. teenage pregnancy rate is 26.5 per 1,000 women.
The rate has declined by 57% since 1991.
[340] In 2013, the highest teenage birth rate was in Alabama, and the lowest in Wyoming.
[340][341] Abortion is legal throughout the U.S., owing to Roe v. Wade, a 1973 landmark decision by the Supreme Court of the United States.
While the abortion rate is falling, the abortion ratio of 241 per 1,000 live births and abortion rate of 15 per 1,000 women aged 15–44 remain higher than those of most Western nations.
[342] In 2013, the average age at first birth was 26 and 40.6% of births were to unmarried women.
The total fertility rate (TFR) in 2016 was 1.82 births per 1000 woman.
[344] Adoption in the United States is common and relatively easy from a legal point of view (compared to other Western countries).
[345] In 2001, with over 127,000 adoptions, the U.S. accounted for nearly half of the total number of adoptions worldwide.
[346] Same-sex marriage is legal nationwide, owing to the Supreme Court's 2015 decision in Obergefell v. Hodges, and it is legal for same-sex couples to adopt.
Polygamy is illegal throughout the U.S.[347]
The United States had a life expectancy of 78.6 years at birth in 2017, which was the third year of declines in life expectancy following decades of continuous increase.
The recent decline is largely due to sharp increases in the drug overdose and suicide rates.
Life expectancy was highest among Asians and Hispanics and lowest among blacks.
[348][349] According to CDC and Census Bureau data, deaths from suicide, alcohol and drug overdoses hit record highs in 2017.
Increasing obesity in the United States and health improvements elsewhere contributed to lowering the country's rank in life expectancy from 11th in the world in 1987, to 42nd in 2007.
[351] Obesity rates have more than doubled in the last 30 years, are the highest in the industrialized world, and are among the highest anywhere.
[352][353] Approximately one-third of the adult population is obese and an additional third is overweight.
[354] Obesity-related type 2 diabetes is considered epidemic by health care professionals.
In 2010, coronary artery disease, lung cancer, stroke, chronic obstructive pulmonary diseases, and traffic accidents caused the most years of life lost in the U.S. Low back pain, depression, musculoskeletal disorders, neck pain, and anxiety caused the most years lost to disability.
The most deleterious risk factors were poor diet, tobacco smoking, obesity, high blood pressure, high blood sugar, physical inactivity, and alcohol use.
Alzheimer's disease, drug abuse, kidney disease, cancer, and falls caused the most additional years of life lost over their age-adjusted 1990 per-capita rates.
[356] U.S. teenage pregnancy and abortion rates are substantially higher than in other Western nations, especially among blacks and Hispanics.
The U.S. is a global leader in medical innovation.
America solely developed or contributed significantly to 9 of the top 10 most important medical innovations since 1975 as ranked by a 2001 poll of physicians, while the European Union and Switzerland together contributed to five.
[358] Since 1966, more Americans have received the Nobel Prize in Medicine than the rest of the world combined.
From 1989 to 2002, four times more money was invested in private biotechnology companies in America than in Europe.
[359] The U.S. health-care system far outspends any other nation, measured in both per capita spending and percentage of GDP.
Health-care coverage in the United States is a combination of public and private efforts and is not universal.
In 2017, 12.2% of the population did not carry health insurance.
[361] The subject of uninsured and underinsured Americans is a major political issue.
[362][363] In 2006, Massachusetts became the first state to mandate universal health insurance.
[364] Federal legislation passed in early 2010 would ostensibly create a near-universal health insurance system around the country by 2014,[needs update] though the bill and its ultimate effect are issues of controversy.
American public education is operated by state and local governments, regulated by the United States Department of Education through restrictions on federal grants.
In most states, children are required to attend school from the age of six or seven (generally, kindergarten or first grade) until they turn 18 (generally bringing them through twelfth grade, the end of high school); some states allow students to leave school at 16 or 17.
About 12% of children are enrolled in parochial or nonsectarian private schools.
Just over 2% of children are homeschooled.
[368] The U.S. spends more on education per student than any nation in the world, spending more than $11,000 per elementary student in 2010 and more than $12,000 per high school student.
[369] Some 80% of U.S. college students attend public universities.
Of Americans 25 and older, 84.6% graduated from high school, 52.6% attended some college, 27.2% earned a bachelor's degree, and 9.6% earned graduate degrees.
[371] The basic literacy rate is approximately 99%.
[233][372] The United Nations assigns the United States an Education Index of 0.97, tying it for 12th in the world.
The United States has many competitive private and public institutions of higher education.
The majority of the world's top universities listed by different ranking organizations are in the U.S.[374][375][376] There are also local community colleges with generally more open admission policies, shorter academic programs, and lower tuition.
In 2018, U21, a network of research-intensive universities, ranked the United States first in the world for breadth and quality of higher education, and 15th when GDP was a factor.
As for public expenditures on higher education, the U.S. trails some other OECD nations but spends more per student than the OECD average, and more than all nations in combined public and private spending.
[369][378] As of 2018[update], student loan debt exceeded 1.5 trillion dollars, more than Americans owe on credit cards.
The United States is the world's oldest surviving federation.
It is a representative democracy, "in which majority rule is tempered by minority rights protected by law".
[381] The government is regulated by a system of checks and balances defined by the U.S. Constitution, which serves as the country's supreme legal document.
[382] For 2018, the U.S. ranked 25th on the Democracy Index[383] and 22nd on the Corruption Perceptions Index.
In the American federalist system, citizens are usually subject to three levels of government: federal, state, and local.
The local government's duties are commonly split between county and municipal governments.
In almost all cases, executive and legislative officials are elected by a plurality vote of citizens by district.
There is no proportional representation at the federal level, and it is rare at lower levels.
The federal government comprises three branches:
The House of Representatives has 435 voting members, each representing a congressional district for a two-year term.
House seats are apportioned among the states by population every tenth year.
At the 2010 census, seven states had the minimum of one representative, while California, the most populous state, had 53.
[390] The District of Columbia and the five major U.S. territories each have one member of Congress — these members are not allowed to vote.
The Senate has 100 members with each state having two senators, elected at-large to six-year terms; one-third of Senate seats are up for election every other year.
The District of Columbia and the five major U.S. territories do not have senators.
[391] The President serves a four-year term and may be elected to the office no more than twice.
The President is not elected by direct vote, but by an indirect electoral college system in which the determining votes are apportioned to the states and the District of Columbia.
[392] The Supreme Court, led by the Chief Justice of the United States, has nine members, who serve for life.
The state governments are structured in a roughly similar fashion; Nebraska uniquely has a unicameral legislature.
[394] The governor (chief executive) of each state is directly elected.
Some state judges and cabinet officers are appointed by the governors of the respective states, while others are elected by popular vote.
The original text of the Constitution establishes the structure and responsibilities of the federal government and its relationship with the individual states.
Article One protects the right to the "great writ" of habeas corpus.
The Constitution has been amended 27 times;[395] the first ten amendments, which make up the Bill of Rights, and the Fourteenth Amendment form the central basis of Americans' individual rights.
All laws and governmental procedures are subject to judicial review and any law ruled by the courts to be in violation of the Constitution is voided.
The principle of judicial review, not explicitly mentioned in the Constitution, was established by the Supreme Court in Marbury v. Madison (1803)[396] in a decision handed down by Chief Justice John Marshall.
The United States is a federal republic of 50 states, a federal district, five territories and several uninhabited island possessions.
[399][400][401] The states and territories are the principal administrative districts in the country.
These are divided into subdivisions of counties and independent cities.
The District of Columbia is a federal district that contains the capital of the United States, Washington DC.
[402] The states and the District of Columbia choose the President of the United States.
Each state has presidential electors equal to the number of their Representatives and Senators in Congress; the District of Columbia has three (because of the 23rd Amendment).
[403] Territories of the United States such as Puerto Rico do not have presidential electors, and so people in those territories cannot vote for the president.
Congressional Districts are reapportioned among the states following each decennial Census of Population.
Each state then draws single-member districts to conform with the census apportionment.
The total number of voting Representatives is 435.
There are also 6 non-voting representatives who represent the District of Columbia and the five major U.S.
The United States also observes tribal sovereignty of the American Indian nations to a limited degree, as it does with the states' sovereignty.
American Indians are U.S. citizens and tribal lands are subject to the jurisdiction of the U.S. Congress and the federal courts.
Like the states they have a great deal of autonomy, but also like the states, tribes are not allowed to make war, engage in their own foreign relations, or print and issue currency.
Citizenship is granted at birth in all states, the District of Columbia, and all major U.S. territories except American Samoa.
The United States has operated under a two-party system for most of its history.
[409] For elective offices at most levels, state-administered primary elections choose the major party nominees for subsequent general elections.
Since the general election of 1856, the major parties have been the Democratic Party, founded in 1824, and the Republican Party, founded in 1854.
Since the Civil War, only one third-party presidential candidate—former president Theodore Roosevelt, running as a Progressive in 1912—has won as much as 20% of the popular vote.
The President and Vice-president are elected through the Electoral College system.
In American political culture, the center-right Republican Party is considered "conservative" and the center-left Democratic Party is considered "liberal".
[411][412] The states of the Northeast and West Coast and some of the Great Lakes states, known as "blue states", are relatively liberal.
The "red states" of the South and parts of the Great Plains and Rocky Mountains are relatively conservative.
Republican Donald Trump, the winner of the 2016 presidential election, is serving as the 45th President of the United States.
[413] Leadership in the Senate includes Republican Vice President Mike Pence, Republican President Pro Tempore Chuck Grassley, Majority Leader Mitch McConnell, and Minority Leader Chuck Schumer.
[414] Leadership in the House includes Speaker of the House Nancy Pelosi, Majority Leader Steny Hoyer, and Minority Leader Kevin McCarthy.
In the 116th United States Congress, the House of Representatives is controlled by the Democratic Party and the Senate is controlled by the Republican Party, giving the U.S. a split Congress.
The Senate consists of 53 Republicans, and 45 Democrats with 2 Independents who caucus with the Democrats; the House consists of 235 Democrats and 199 Republicans.
[416] In state governorships, there are 27 Republicans and 23 Democrats.
[417] Among the DC mayor and the 5 territorial governors, there are 2 Republicans, 1 Democrat, 1 New Progressive, and 2 Independents.
The United States has an established structure of foreign relations.
It is a permanent member of the United Nations Security Council and New York City is home to the United Nations Headquarters.
It is a member of the G7,[421] G20, and Organisation for Economic Co-operation and Development.
Almost all countries have embassies in Washington, D.C., and many have consulates around the country.
Likewise, nearly all nations host American diplomatic missions.
However, Iran, North Korea, Bhutan, and the Republic of China (Taiwan) do not have formal diplomatic relations with the United States (although the U.S. still maintains unofficial relations with Taiwan and supplies it with military equipment).
The United States has a "Special Relationship" with the United Kingdom[423] and strong ties with Canada,[424] Australia,[425] New Zealand,[426] the Philippines,[427] Japan,[428] South Korea,[429] Israel,[430] and several European Union countries, including France, Italy, Germany, and Spain.
It works closely with fellow NATO members on military and security issues and with its neighbors through the Organization of American States and free trade agreements such as the trilateral North American Free Trade Agreement with Canada and Mexico.
In 2008, the United States spent a net $25.4 billion on official development assistance, the most in the world.
As a share of America's large gross national income (GNI), however, the U.S. contribution of 0.18% ranked last among 22 donor states.
By contrast, private overseas giving by Americans is relatively generous.
The U.S. exercises full international defense authority and responsibility for three sovereign nations through Compact of Free Association with Micronesia, the Marshall Islands and Palau.
These are Pacific island nations, once part of the U.S.-administered Trust Territory of the Pacific Islands after World War II, which gained independence in subsequent years.
On October 25, 2017, Vice President Mike Pence announced at a In Defense of Christians annual dinner meeting in Washington that the United States would stop funding United Nations relief efforts, cases tackling the persecution of Christians in the Middle East, but insisted that the U.S. would instead help and aid Christians directly through the United States Agency for International Development.
[433] Pence said that he will be visiting the Middle East in December and will meet with Israeli Prime Minister Benjamin Netanyahu and Palestinian President Mahmoud Abbas to discuss peace agreements.
Taxes in the United States are levied at the federal, state, and local government levels.
These include taxes on income, payroll, property, sales, imports, estates and gifts, as well as various fees.
Taxation in the United States is based on citizenship, not residency.
[437] Both non-resident citizens and Green Card holders living abroad are taxed on their income irrespective of where they live or where their income is earned.
It is the only country in the world, other than Eritrea, to do so.
In 2010 taxes collected by federal, state and municipal governments amounted to 24.8% of GDP.
[439] During FY2012, the federal government collected approximately $2.45 trillion in tax revenue, up $147 billion or 6% versus FY2011 revenues of $2.30 trillion.
Primary receipt categories included individual income taxes ($1,132B or 47%), Social Security/Social Insurance taxes ($845B or 35%), and corporate taxes ($242B or 10%).
[440] Based on CBO estimates,[441] under 2013 tax law the top 1% will be paying the highest average tax rates since 1979, while other income groups will remain at historic lows.
U.S. taxation has historically been generally progressive, especially the federal income taxes, though by most measures it became noticeably less progressive after 1980.
[443][444] It has sometimes been described as among the most progressive in the developed world, but this characterization is controversial.
[445][446][447][448][444] The highest 10% of income earners pay a majority of federal taxes,[449] and about half of all taxes.
[450] Payroll taxes for Social Security are a flat regressive tax, with no tax charged on income above $118,500 (for 2015 and 2016) and no tax at all paid on unearned income from things such as stocks and capital gains.
[451][452] The historic reasoning for the regressive nature of the payroll tax is that entitlement programs have not been viewed as welfare transfers.
[453][454] However, according to the Congressional Budget Office the net effect of Social Security is that the benefit to tax ratio ranges from roughly 70% for the top earnings quintile to about 170% for the lowest earning quintile, making the system progressive.
The top 10% paid 51.8% of total federal taxes in 2009, and the top 1%, with 13.4% of pre-tax national income, paid 22.3% of federal taxes.
[456] In 2013 the Tax Policy Center projected total federal effective tax rates of 35.5% for the top 1%, 27.2% for the top quintile, 13.8% for the middle quintile, and −2.7% for the bottom quintile.
[457][458] The incidence of corporate income tax has been a matter of considerable ongoing controversy for decades.
[448][459] State and local taxes vary widely, but are generally less progressive than federal taxes as they rely heavily on broadly borne regressive sales and property taxes that yield less volatile revenue streams, though their consideration does not eliminate the progressive nature of overall taxation.
During FY 2012, the federal government spent $3.54 trillion on a budget or cash basis, down $60 billion or 1.7% vs. FY 2011 spending of $3.60 trillion.
Major categories of FY 2012 spending included: Medicare & Medicaid ($802B or 23% of spending), Social Security ($768B or 22%), Defense Department ($670B or 19%), non-defense discretionary ($615B or 17%), other mandatory ($461B or 13%) and interest ($223B or 6%).
The total national debt of the United States in the United States was $18.527 trillion (106% of the GDP) in 2014.
[461][n] The United States has the largest external debt in the world and the 14th largest government debt as a % of GDP in the world.
The President holds the title of commander-in-chief of the nation's armed forces and appoints its leaders, the Secretary of Defense and the Joint Chiefs of Staff.
The United States Department of Defense administers the armed forces, including the Army, Marine Corps, Navy, and Air Force.
The Coast Guard is run by the Department of Homeland Security in peacetime and by the Department of the Navy during times of war.
In 2008, the armed forces had 1.4 million personnel on active duty.
The Reserves and National Guard brought the total number of troops to 2.3 million.
The Department of Defense also employed about 700,000 civilians, not including contractors.
Military service is voluntary, though conscription may occur in wartime through the Selective Service System.
[467] American forces can be rapidly deployed by the Air Force's large fleet of transport aircraft, the Navy's 11 active aircraft carriers, and Marine expeditionary units at sea with the Navy's Atlantic and Pacific fleets.
The military operates 865 bases and facilities abroad,[468] and maintains deployments greater than 100 active duty personnel in 25 foreign countries.
The military budget of the United States in 2011 was more than $700 billion, 41% of global military spending and equal to the next 14 largest national military expenditures combined.
At 4.7% of GDP, the rate was the second-highest among the top 15 military spenders, after Saudi Arabia.
[470] U.S. defense spending as a percentage of GDP ranked 23rd globally in 2012 according to the CIA.
[471] Defense spending plays a major role in science and technology investment, with roughly half of U.S. federal research and development funded by the Department of Defense.
[472] Defense's share of the overall U.S. economy has generally declined in recent decades, from Cold War peaks of 14.2% of GDP in 1953 and 69.5% of federal outlays in 1954 to 4.7% of GDP and 18.8% of federal outlays in 2011.
The proposed base Department of Defense budget for 2012, $553 billion, was a 4.2% increase over 2011; an additional $118 billion was proposed for the military campaigns in Iraq and Afghanistan.
[474] The last American troops serving in Iraq departed in December 2011;[475] 4,484 service members were killed during the Iraq War.
[476] Approximately 90,000 U.S. troops were serving in Afghanistan in April 2012;[477] by November 8, 2013 2,285 had been killed during the War in Afghanistan.
Law enforcement in the United States is primarily the responsibility of local police departments and sheriff's offices, with state police providing broader services.
The New York Police Department (NYPD) is the largest in the country.
Federal agencies such as the Federal Bureau of Investigation (FBI) and the U.S.
Marshals Service have specialized duties, including protecting civil rights, national security and enforcing U.S. federal courts' rulings and federal laws.
[479] At the federal level and in almost every state, a legal system operates on a common law.
State courts conduct most criminal trials; federal courts handle certain designated crimes as well as certain appeals from the state criminal courts.
Plea bargaining is very common; the vast majority of criminal cases in the country are settled by plea bargain rather than jury trial.
In 2015, there were 15,696 murders which was 1,532 more than in 2014, a 10.8% increase, the largest since 1971.
[481] The murder rate in 2015 was 4.9 per 100,000 people.
[482] In 2016 the murder rate increased by 8.6%, with 17,413 murders that year.
[483] The national clearance rate for homicides in 2015 was 64.1%, compared to 90% in 1965.
[484] In 2012 there were 4.7 murders per 100,000 persons in the United States, a 54% decline from the modern peak of 10.2 in 1980.
[485] In 2001–2, the United States had above-average levels of violent crime and particularly high levels of gun violence compared to other developed nations.
[486] A cross-sectional analysis of the World Health Organization Mortality Database from 2010 showed that United States "homicide rates were 7.0 times higher than in other high-income countries, driven by a gun homicide rate that was 25.2 times higher.
"[487] Gun ownership rights continue to be the subject of contentious political debate.
In 2016, the US murder rate of 5.4 per 100,000 was similar to the estimated global average of 5.15 per 100,000.
In 2017, there were 17,264 murders and the murder rate was 5.3 per 100,000.
Regarding weapons, 73% of murders were committed by firearm, 10% by knife and 17% by other means.
[489] The violent crime rose sharply in the 1960s until the early 1990s and declined in the late 1990s and 2000s.
[489] In 2014, the murder rate fell to the lowest level (4.5) since 1957 (4.0).
[490] The violent crime rate increased by 5.9% between 2014 and 2017 and the murder rate by 20.5%.
Of those arrested for serious violent crimes in 2017, 58.5% were white, 37.5% were black, 2.1% were American Indian or Alaska Native and 1.5% Asian.
Ethnically, 23.5% were Hispanic and 76.5% were non-Hispanic.
[491] Gun violence peaked in 1993 with 17,125 gun murders before declining to 9,527 in 1999 and steadily rising since to 12,772.
Non-gun murders reached a peak in 1980 of 8,340 and declined in most years until the early 2010s with 4,668 in 2017.
[492] The rate of robberies declined 62% between 1990 and 2017.
From 1980 through 2008 males represented 77% of homicide victims and 90% of offenders.
Blacks committed 52.5% of all homicides during that span, at a rate almost eight times that of whites ("whites" includes most Hispanics), and were victimized at a rate six times that of whites.
Most homicides were intraracial, with 93% of black victims killed by blacks and 84% of white victims killed by whites.
[493] In 2012, Louisiana had the highest rate of murder and non-negligent manslaughter in the U.S., and New Hampshire the lowest.
[494] The FBI's Uniform Crime Reports estimates that there were 3,246 violent and property crimes per 100,000 residents in 2012, for a total of over 9 million total crimes.
Capital punishment is sanctioned in the United States for certain federal and military crimes, and also at the state level in 30 states.
[496][497] No executions took place from 1967 to 1977, owing in part to a U.S. Supreme Court ruling striking down arbitrary imposition of the death penalty.
In 1976, that Court ruled that, under appropriate circumstances, capital punishment may constitutionally be imposed.
Since the decision there have been more than 1,300 executions, a majority of these taking place in three states: Texas, Virginia, and Oklahoma.
[498] Meanwhile, several states have either abolished or struck down death penalty laws.
In 2015, the country had the fifth-highest number of executions in the world, following China, Iran, Pakistan and Saudi Arabia.
The United States has the highest documented incarceration rate and largest prison population in the world.
[500] At the start of 2008, more than 2.3 million people were incarcerated, more than one in every 100 adults.
[501] In December 2012, the combined U.S. adult correctional systems supervised about 6,937,600 offenders.
About 1 in every 35 adult residents in the United States was under some form of correctional supervision in December 2012, the lowest rate observed since 1997.
[502] The prison population has quadrupled since 1980,[503] and state and local spending on prisons and jails has grown three times as much as that spent on public education during the same period.
[504] However, the imprisonment rate for all prisoners sentenced to more than a year in state or federal facilities is 478 per 100,000 in 2013[505] and the rate for pre-trial/remand prisoners is 153 per 100,000 residents in 2012.
[506] The country's high rate of incarceration is largely due to changes in sentencing guidelines and drug policies.
[507] According to the Federal Bureau of Prisons, the majority of inmates held in federal prisons are convicted of drug offenses.
[508] The privatization of prisons and prison services which began in the 1980s has been a subject of debate.
[509][510] In 2018, Oklahoma had the highest incarceration rate (1,079 per 100,000 people), and Massachusetts the lowest (324 per 100,000 people).
[511][512] Among the U.S. territories, the highest incarceration rate was in the U.S. Virgin Islands (542 per 100,000 people) and the lowest was in Puerto Rico (313 per 100,000 people).
The United States has a capitalist mixed economy[citation needed] which is fueled by abundant natural resources and high productivity.
[522] According to the International Monetary Fund, the U.S. GDP of $16.8 trillion constitutes 24% of the gross world product at market exchange rates and over 19% of the gross world product at purchasing power parity (PPP).
The nominal GDP of the U.S. is estimated to be $17.528 trillion as of 2014[update].
[524] From 1983 to 2008, U.S. real compounded annual GDP growth was 3.3%, compared to a 2.3% weighted average for the rest of the G7.
[525] The country ranks ninth in the world in nominal GDP per capita according to the United Nations (first in the Americas)[526] and sixth in GDP per capita at PPP.
[523] The U.S. dollar is the world's primary reserve currency.
The United States is the largest importer of goods and second-largest exporter, though exports per capita are relatively low.
In 2010, the total U.S. trade deficit was $635 billion.
[528] Canada, China, Mexico, Japan, and Germany are its top trading partners.
[529] In 2010, oil was the largest import commodity, while transportation equipment was the country's largest export.
[528] Japan is the largest foreign holder of U.S. public debt.
[530] The largest holder of the U.S. debt are American entities, including federal government accounts and the Federal Reserve, who hold the majority of the debt.
In 2009, the private sector was estimated to constitute 86.4% of the economy, with federal government activity accounting for 4.3% and state and local government activity (including federal transfers) the remaining 9.3%.
[540] The number of employees at all levels of government outnumber those in manufacturing by 1.7 to 1.
[541] While its economy has reached a postindustrial level of development and its service sector constitutes 67.8% of GDP, the United States remains an industrial power.
[542] The leading business field by gross business receipts is wholesale and retail trade; by net income it is manufacturing.
[543] In the franchising business model, McDonald's and Subway are the two most recognized brands in the world.
Coca-Cola is the most recognized soft drink company in the world.
Chemical products are the leading manufacturing field.
[545] The United States is the largest producer of oil in the world, as well as its second-largest importer.
[546] It is the world's number one producer of nuclear energy, as well as liquid natural gas, sulfur, phosphates, and salt.
The National Mining Association provides data pertaining to coal and minerals that include beryllium, copper, lead, magnesium, zinc, titanium and others.
Agriculture accounts for just under 1% of GDP,[542] yet the United States is the world's top producer of corn[549] and soybeans.
[550] The National Agricultural Statistics Service maintains agricultural statistics for products that include peanuts, oats, rye, wheat, rice, cotton, corn, barley, hay, sunflowers, and oilseeds.
In addition, the United States Department of Agriculture (USDA) provides livestock statistics regarding beef, poultry, pork, and dairy products.
The country is the primary developer and grower of genetically modified food, representing half of the world's biotech crops.
[551] In the contiguous 48 states, 35% of the land is used as pasture, 28% is covered by forest, and 21% is agricultural cropland, with all other uses accounting for less than 20%.
Consumer spending comprises 68% of the U.S. economy in 2015.
[553] In August 2010, the American labor force consisted of 154.1 million people.
With 21.2 million people, government is the leading field of employment.
The largest private employment sector is health care and social assistance, with 16.4 million people.
About 12% of workers are unionized, compared to 30% in Western Europe.
[554] The World Bank ranks the United States first in the ease of hiring and firing workers.
[555] The United States is ranked among the top three in the Global Competitiveness Report as well.
It has a smaller welfare state and redistributes less income through government action than European nations tend to.
The United States is the only advanced economy that does not guarantee its workers paid vacation[557] and is one of just a few countries in the world without paid family leave as a legal right, with the others being Papua New Guinea, Suriname and Liberia.
[558] While federal law does not require sick leave, it is a common benefit for government workers and full-time employees at corporations.
[559] 74% of full-time American workers get paid sick leave, according to the Bureau of Labor Statistics, although only 24% of part-time workers get the same benefits.
[559] In 2009, the United States had the third-highest workforce productivity per person in the world, behind Luxembourg and Norway.
It was fourth in productivity per hour, behind those two countries and the Netherlands.
The 2008–2012 global recession significantly affected the United States, with output still below potential according to the Congressional Budget Office.
[561] It brought high unemployment (which has been decreasing but remains above pre-recession levels), along with low consumer confidence, the continuing decline in home values and increase in foreclosures and personal bankruptcies, an escalating federal debt crisis, inflation, and rising petroleum and food prices.
The United States has been a leader in technological innovation since the late 19th century and scientific research since the mid-20th century.
Methods for producing interchangeable parts were developed by the U.S. War Department by the Federal Armories during the first half of the 19th century.
This technology, along with the establishment of a machine tool industry, enabled the U.S. to have large-scale manufacturing of sewing machines, bicycles, and other items in the late 19th century and became known as the American system of manufacturing.
Factory electrification in the early 20th century and introduction of the assembly line and other labor-saving techniques created the system called mass production.
In 1876, Alexander Graham Bell was awarded the first U.S. patent for the telephone.
Thomas Edison's research laboratory, one of the first of its kind, developed the phonograph, the first long-lasting light bulb, and the first viable movie camera.
[563] The latter led to emergence of the worldwide entertainment industry.
In the early 20th century, the automobile companies of Ransom E. Olds and Henry Ford popularized the assembly line.
The Wright brothers, in 1903, made the first sustained and controlled heavier-than-air powered flight.
The rise of fascism and Nazism in the 1920s and 1930s led many European scientists, including Albert Einstein, Enrico Fermi, and John von Neumann, to immigrate to the United States.
[565] During World War II, the Manhattan Project developed nuclear weapons, ushering in the Atomic Age, while the Space Race produced rapid advances in rocketry, materials science, and aeronautics.
The invention of the transistor in the 1950s, a key active component in practically all modern electronics, led to many technological developments and a significant expansion of the U.S. technology industry.
[568][569][570] This, in turn, led to the establishment of many new technology companies and regions around the country such as Silicon Valley in California.
Advancements by American microprocessor companies such as Advanced Micro Devices (AMD), and Intel along with both computer software and hardware companies that include Adobe Systems, Apple Inc., IBM, Microsoft, and Sun Microsystems created and popularized the personal computer.
The ARPANET was developed in the 1960s to meet Defense Department requirements, and became the first of a series of networks which evolved into the Internet.
These advancements then lead to greater personalization of technology for individual use.
[572] As of 2013[update], 83.8% of American households owned at least one computer, and 73.3% had high-speed Internet service.
[573] 91% of Americans also own a mobile phone as of May 2013[update].
[574] The United States ranks highly with regard to freedom of use of the internet.
In the 21st century, approximately two-thirds of research and development funding comes from the private sector.
[576] The United States leads the world in scientific research papers and impact factor.
Accounting for 4.4% of the global population, Americans collectively possess 41.6% of the world's total wealth,[579] and Americans make up roughly half of the world's population of millionaires.
[580] The Global Food Security Index ranked the U.S. number one for food affordability and overall food security in March 2013.
[581] Americans on average have over twice as much living space per dwelling and per person as European Union residents, and more than every EU nation.
[582] For 2017 the United Nations Development Programme ranked the United States 13th among 189 countries in its Human Development Index and 25th among 151 countries in its inequality-adjusted HDI (IHDI).
After years of stagnant growth, in 2016, according to the Census, median household income reached a record high after two consecutive years of record growth, although income inequality remains at record highs with top fifth of earners taking home more than half of all overall income.
[584] There has been a widening gap between productivity and median incomes since the 1970s.
[585] However, the gap between total compensation and productivity is not as wide because of increased employee benefits such as health insurance.
[586] The rise in the share of total annual income received by the top 1 percent, which has more than doubled from 9 percent in 1976 to 20 percent in 2011, has significantly affected income inequality,[587] leaving the United States with one of the widest income distributions among OECD nations.
[588] According to a 2018 study by the OECD, the United States has much higher income inequality and a larger percentage of low-income workers than almost any other developed nation.
This is largely because at-risk workers get almost no government support and are further set back by a very weak collective bargaining system.
[589] The top 1 percent of income-earners accounted for 52 percent of the income gains from 2009 to 2015, where income is defined as market income excluding government transfers.
[590] The extent and relevance of income inequality is a matter of debate.
Wealth, like income and taxes, is highly concentrated; the richest 10% of the adult population possess 72% of the country's household wealth, while the bottom half claim only 2%.
[595] According to a September 2017 report by the Federal Reserve, the top 1% controlled 38.6% of the country's wealth in 2016.
[596] Between June 2007 and November 2008 the global recession led to falling asset prices around the world.
Assets owned by Americans lost about a quarter of their value.
[597] Since peaking in the second quarter of 2007, household wealth was down $14 trillion, but has since increased $14 trillion over 2006 levels.
[598][599] At the end of 2014, household debt amounted to $11.8 trillion,[600] down from $13.8 trillion at the end of 2008.
There were about 578,424 sheltered and unsheltered homeless persons in the U.S. in January 2014, with almost two-thirds staying in an emergency shelter or transitional housing program.
[602] In 2011 16.7 million children lived in food-insecure households, about 35% more than 2007 levels, though only 1.1% of U.S. children, or 845,000, saw reduced food intake or disrupted eating patterns at some point during the year, and most cases were not chronic.
[603] According to a 2014 report by the Census Bureau, one in five young adults lives in poverty, up from one in seven in 1980.
[604] As of September 2017[update], 40 million people, roughly 12.7% of the U.S. population, were living in poverty, with 18.5 million of those living in deep poverty (a family income below one-half of the poverty threshold).
In 2016, 13.3 million children were living in poverty, which made up 32.6% of the impoverished population.
In 2017, the region with the lowest poverty rate was New Hampshire (7.3%), and the region with the highest poverty rate was American Samoa (65%).
[606][607][608] Among the states, the highest poverty rate was in Mississippi (21.9%).
[609] According to the UN, around five million people in the U.S. live in "third world" conditions.
Personal transportation is dominated by automobiles, which operate on a network of 4 million miles (6.4 million kilometers) of public roads,[612] including one of the world's longest highway systems at 57,000 mi (91,700 km).
[613] The world's second-largest automobile market,[614] the United States has the highest rate of per-capita vehicle ownership in the world, with 765 vehicles per 1,000 Americans (1996).
[615] About 40% of personal vehicles are vans, SUVs, or light trucks.
[616] The average American adult (accounting for all drivers and non-drivers) spends 55 minutes driving every day, traveling 29 miles (47 km).
[617] In 2017, there were 255,009,283 motor vehicles—including cars, vans, buses, freight, and other trucks, but excluding motorcycles and other two-wheelers—or 910 vehicles per 1,000 people.
Mass transit accounts for 9% of total U.S. work trips.
[620][621] Transport of goods by rail is extensive, though relatively low numbers of passengers (approximately 31 million annually) use intercity rail to travel, partly because of the low population density throughout much of the U.S.
[622][623] However, ridership on Amtrak, the national intercity passenger rail system, grew by almost 37% between 2000 and 2010.
[624] Also, light rail development has increased in recent years.
[625] Bicycle usage for work commutes is minimal.
The civil airline industry is entirely privately owned and has been largely deregulated since 1978, while
most major airports are publicly owned.
[627] The three largest airlines in the world by passengers carried are U.S.-based; American Airlines is number one after its 2013 acquisition by US Airways.
[628] Of the world's 50 busiest passenger airports, 16 are in the United States, including the busiest, Hartsfield–Jackson Atlanta International Airport, the fourth-busiest Los Angeles International Airport, and the sixth-busiest O'Hare International Airport in Chicago.
[629] In the aftermath of the 9/11 attacks of 2001, the Transportation Security Administration was created to police airports and commercial airliners.
The United States energy market is about 29,000 terawatt hours per year.
[630] Energy consumption per capita is 7.8 tons (7076 kg) of oil equivalent per year, the 10th-highest rate in the world.
In 2005, 40% of this energy came from petroleum, 23% from coal, and 22% from natural gas.
The remainder was supplied by nuclear power and renewable energy sources.
[631] The United States is the world's largest consumer of petroleum.
[632] The United States has 27% of global coal reserves.
[633] It is the world's largest producer of natural gas and crude oil.
For decades, nuclear power has played a limited role relative to many other developed countries, in part because of public perception following the Three Mile Island accident in 1979.
In 2007, several applications for new nuclear plants were filed.
Since 2007, the total greenhouse gas emissions by the United States are the second highest by country, exceeded only by China.
[636][637] The United States has historically been the world's largest producer of greenhouse gases and greenhouse gas emissions per capita remain high.
Issues that affect water supply in the United States include droughts in the West, water scarcity, pollution, a backlog of investment, concerns about the affordability of water for the poorest, and a rapidly retiring workforce.
Increased variability and intensity of rainfall as a result of climate change is expected to produce both more severe droughts and flooding, with potentially serious consequences for water supply and for pollution from combined sewer overflows.
The United States is home to many cultures and a wide variety of ethnic groups, traditions, and values.
[643][644] Aside from the Native American, Native Hawaiian, and Native Alaskan populations, nearly all Americans or their ancestors settled or immigrated within the past five centuries.
[645] Mainstream American culture is a Western culture largely derived from the traditions of European immigrants with influences from many other sources, such as traditions brought by slaves from Africa.
[643][646] More recent immigration from Asia and especially Latin America has added to a cultural mix that has been described as both a homogenizing melting pot, and a heterogeneous salad bowl in which immigrants and their descendants retain distinctive cultural characteristics.
Core American culture was established by Protestant British colonists and shaped by the frontier settlement process, with the traits derived passed down to descendants and transmitted to immigrants through assimilation.
Americans have traditionally been characterized by a strong work ethic, competitiveness, and individualism,[647] as well as a unifying belief in an "American creed" emphasizing liberty, equality, private property, democracy, rule of law, and a preference for limited government.
[648] Americans are extremely charitable by global standards.
According to a 2006 British study, Americans gave 1.67% of GDP to charity, more than any other nation studied, more than twice the second place British figure of 0.73%, and around twelve times the French figure of 0.14%.
The American Dream, or the perception that Americans enjoy high social mobility, plays a key role in attracting immigrants.
[651] Whether this perception is realistic has been a topic of debate.
[652][653][654][655][525][656] While mainstream culture holds that the United States is a classless society,[657] scholars identify significant differences between the country's social classes, affecting socialization, language, and values.
[658] Americans' self-images, social viewpoints, and cultural expectations are associated with their occupations to an unusually close degree.
[659] While Americans tend greatly to value socioeconomic achievement, being ordinary or average is generally seen as a positive attribute.
Mainstream American cuisine is similar to that in other Western countries.
Wheat is the primary cereal grain with about three-quarters of grain products made of wheat flour[661] and many dishes use indigenous ingredients, such as turkey, venison, potatoes, sweet potatoes, corn, squash, and maple syrup which were consumed by Native Americans and early European settlers.
[662] These homegrown foods are part of a shared national menu on one of America's most popular holidays, Thanksgiving, when some Americans make traditional foods to celebrate the occasion.
Characteristic dishes such as apple pie, fried chicken, pizza, hamburgers, and hot dogs derive from the recipes of various immigrants.
French fries, Mexican dishes such as burritos and tacos, and pasta dishes freely adapted from Italian sources are widely consumed.
[665] Americans drink three times as much coffee as tea.
[666] Marketing by U.S. industries is largely responsible for making orange juice and milk ubiquitous breakfast beverages.
American eating habits owe a great deal to that of their British culinary roots with some variations.
Although American lands could grow newer vegetables that Britain could not, most colonists would not eat these new foods until accepted by Europeans.
[669] Over time American foods changed to a point that food critic, John L. Hess stated in 1972: "Our founding fathers were as far superior to our present political leaders in the quality of their food as they were in the quality of their prose and intelligence".
The American fast food industry, the world's largest,[671] pioneered the drive-through format in the 1940s.
[672] Fast food consumption has sparked health concerns.
During the 1980s and 1990s, Americans' caloric intake rose 24%;[665] frequent dining at fast food outlets is associated with what public health officials call the American "obesity epidemic".
[673] Highly sweetened soft drinks are widely popular, and sugared beverages account for nine percent of American caloric intake.
In the 18th and early 19th centuries, American art and literature took most of its cues from Europe.
Writers such as Nathaniel Hawthorne, Edgar Allan Poe, and Henry David Thoreau established a distinctive American literary voice by the middle of the 19th century.
Mark Twain and poet Walt Whitman were major figures in the century's second half; Emily Dickinson, virtually unknown during her lifetime, is now recognized as an essential American poet.
[675] A work seen as capturing fundamental aspects of the national experience and character—such as Herman Melville's Moby-Dick (1851), Twain's The Adventures of Huckleberry Finn (1885), F. Scott Fitzgerald's The Great Gatsby (1925) and Harper Lee's To Kill a Mockingbird (1960)—may be dubbed the "Great American Novel".
Twelve U.S. citizens have won the Nobel Prize in Literature, most recently Bob Dylan in 2016.
William Faulkner, Ernest Hemingway and John Steinbeck are often named among the most influential writers of the 20th century.
[677] Popular literary genres such as the Western and hardboiled crime fiction developed in the United States.
The Beat Generation writers opened up new literary approaches, as have postmodernist authors such as John Barth, Thomas Pynchon, and Don DeLillo.
The transcendentalists, led by Thoreau and Ralph Waldo Emerson, established the first major American philosophical movement.
After the Civil War, Charles Sanders Peirce and then William James and John Dewey were leaders in the development of pragmatism.
In the 20th century, the work of W. V. O. Quine and Richard Rorty, and later Noam Chomsky, brought analytic philosophy to the fore of American philosophical academia.
John Rawls and Robert Nozick led a revival of political philosophy, and Martha Nussbaum is its most important figure today.
Cornel West and Judith Butler have led a continental tradition in American philosophical academia.
Chicago school economists like Milton Friedman, James M. Buchanan, and Thomas Sowell have affected various fields in social and political philosophy.
In the visual arts, the Hudson River School was a mid-19th-century movement in the tradition of European naturalism.
The realist paintings of Thomas Eakins are now widely celebrated.
The 1913 Armory Show in New York City, an exhibition of European modernist art, shocked the public and transformed the U.S. art scene.
[681] Georgia O'Keeffe, Marsden Hartley, and others experimented with new, individualistic styles.
Major artistic movements such as the abstract expressionism of Jackson Pollock and Willem de Kooning and the pop art of Andy Warhol and Roy Lichtenstein developed largely in the United States.
The tide of modernism and then postmodernism has brought fame to American architects such as Frank Lloyd Wright, Philip Johnson, and Frank Gehry.
[682] Americans have long been important in the modern artistic medium of photography, with major photographers including Alfred Stieglitz, Edward Steichen, and Ansel Adams.
One of the first major promoters of American theater was impresario P. T. Barnum, who began operating a lower Manhattan entertainment complex in 1841.
The team of Harrigan and Hart produced a series of popular musical comedies in New York starting in the late 1870s.
In the 20th century, the modern musical form emerged on Broadway; the songs of musical theater composers such as Irving Berlin, Cole Porter, and Stephen Sondheim have become pop standards.
Playwright Eugene O'Neill won the Nobel literature prize in 1936; other acclaimed U.S. dramatists include multiple Pulitzer Prize winners Tennessee Williams, Edward Albee, and August Wilson.
Choreographers Isadora Duncan and Martha Graham helped create modern dance, while George Balanchine and Jerome Robbins were leaders in 20th-century ballet.
Although little known at the time, Charles Ives's work of the 1910s established him as the first major U.S. composer in the classical tradition, while experimentalists such as Henry Cowell and John Cage created a distinctive American approach to classical composition.
Aaron Copland and George Gershwin developed a new synthesis of popular and classical music.
The rhythmic and lyrical styles of African-American music have deeply influenced American music at large, distinguishing it from European and African traditions.
Elements from folk idioms such as the blues and what is now known as old-time music were adopted and transformed into popular genres with global audiences.
Jazz was developed by innovators such as Louis Armstrong and Duke Ellington early in the 20th century.
Country music developed in the 1920s, and rhythm and blues in the 1940s.
Elvis Presley and Chuck Berry were among the mid-1950s pioneers of rock and roll.
Rock bands such as Metallica, the Eagles, and Aerosmith are among the highest grossing in worldwide sales.
[687][688][689] In the 1960s, Bob Dylan emerged from the folk revival to become one of America's most celebrated songwriters and James Brown led the development of funk.
More recent American creations include hip hop and house music.
American pop stars such as Elvis Presley, Michael Jackson, and Madonna have become global celebrities,[686] as have contemporary musical artists such as Taylor Swift, Britney Spears, Katy Perry, Beyoncé, Jay-Z, Eminem, Kanye West, and Ariana Grande.
Hollywood, a northern district of Los Angeles, California, is one of the leaders in motion picture production.
[691] The world's first commercial motion picture exhibition was given in New York City in 1894, using Thomas Edison's Kinetoscope.
[692] The next year saw the first commercial screening of a projected film, also in New York, and the United States was in the forefront of sound film's development in the following decades.
Since the early 20th century, the U.S. film industry has largely been based in and around Hollywood, although in the 21st century an increasing number of films are not made there, and film companies have been subject to the forces of globalization.
Director D. W. Griffith, the top American filmmaker during the silent film period, was central to the development of film grammar, and producer/entrepreneur Walt Disney was a leader in both animated film and movie merchandising.
[694] Directors such as John Ford redefined the image of the American Old West and history, and, like others such as John Huston, broadened the possibilities of cinema with location shooting, with great influence on subsequent directors.
The industry enjoyed its golden years, in what is commonly referred to as the "Golden Age of Hollywood", from the early sound period until the early 1960s,[695] with screen actors such as John Wayne and Marilyn Monroe becoming iconic figures.
[696][697] In the 1970s, film directors such as Martin Scorsese, Francis Ford Coppola and Robert Altman were a vital component in what became known as "New Hollywood" or the "Hollywood Renaissance",[698] grittier films influenced by French and Italian realist pictures of the post-war period.
[699] Since, directors such as Steven Spielberg, George Lucas and James Cameron have gained renown for their blockbuster films, often characterized by high production costs, and in return, high earnings at the box office, with Cameron's Avatar (2009) earning more than $2 billion.
Notable films topping the American Film Institute's AFI 100 list include Orson Welles's Citizen Kane (1941), which is frequently cited as the greatest film of all time,[701][702] Casablanca (1942), The Godfather (1972), Gone with the Wind (1939), Lawrence of Arabia (1962), The Wizard of Oz (1939), The Graduate (1967), On the Waterfront (1954), Schindler's List (1993), Singin' in the Rain (1952), It's a Wonderful Life (1946) and Sunset Boulevard (1950).
[703] The Academy Awards, popularly known as the Oscars, have been held annually by the Academy of Motion Picture Arts and Sciences since 1929,[704] and the Golden Globe Awards have been held annually since January 1944.
American football is by several measures the most popular spectator sport;[707] the National Football League (NFL) has the highest average attendance of any sports league in the world, and the Super Bowl is watched by millions globally.
Baseball has been regarded as the U.S. national sport since the late 19th century, with Major League Baseball (MLB) being the top league.
Basketball and ice hockey are the country's next two leading professional team sports, with the top leagues being the National Basketball Association (NBA) and the National Hockey League (NHL).
These four major sports, when played professionally, each occupy a season at different but overlapping, times of the year.
College football and basketball attract large audiences.
[708] In soccer, the country hosted the 1994 FIFA World Cup, the men's national soccer team qualified for ten World Cups and the women's team has won the FIFA Women's World Cup three times; Major League Soccer is the sport's highest league in the United States (featuring 21 American and 3 Canadian teams).
The market for professional sports in the United States is roughly $69 billion, roughly 50% larger than that of all of Europe, the Middle East, and Africa combined.
Eight Olympic Games have taken place in the United States (2028 Summer Olympics will mark the ninth time).
As of 2017[update], the United States has won 2,522 medals at the Summer Olympic Games, more than any other country, and 305 in the Winter Olympic Games, the second most behind Norway.
While most major U.S. sports such as baseball and American football have evolved out of European practices, basketball, volleyball, skateboarding, and snowboarding are American inventions, some of which have become popular worldwide.
Lacrosse and surfing arose from Native American and Native Hawaiian activities that predate Western contact.
[711] The most watched individual sports are golf and auto racing, particularly NASCAR.
[712][713] Rugby union is considered the fastest growing sport in the U.S., with registered players, numbered at 115,000+ and a further 1.2 million participants.
The four major broadcasters in the U.S. are the National Broadcasting Company (NBC), Columbia Broadcasting System (CBS), American Broadcasting Company (ABC), and Fox Broadcasting Company (FOX).
The four major broadcast television networks are all commercial entities.
Cable television offers hundreds of channels catering to a variety of niches.
[715] Americans listen to radio programming, also largely commercial, on average just over two-and-a-half hours a day.
In 1998, the number of U.S. commercial radio stations had grown to 4,793 AM stations and 5,662 FM stations.
In addition, there are 1,460 public radio stations.
Most of these stations are run by universities and public authorities for educational purposes and are financed by public or private funds, subscriptions, and corporate underwriting.
Much public-radio broadcasting is supplied by NPR (formerly National Public Radio).
NPR was incorporated in February 1970 under the Public Broadcasting Act of 1967; its television counterpart, PBS, was also created by the same legislation (NPR and PBS are operated separately from each other).
As of September 30, 2014[update], there are 15,433 licensed full-power radio stations in the U.S. according to the U.S. Federal Communications Commission (FCC).
Well-known newspapers include The Wall Street Journal, The New York Times, and USA Today.
[718] Although the cost of publishing has increased over the years, the price of newspapers has generally remained low, forcing newspapers to rely more on advertising revenue and on articles provided by a major wire service, such as the Associated Press or Reuters, for their national and world coverage.
With very few exceptions, all the newspapers in the U.S. are privately owned, either by large chains such as Gannett or McClatchy, which own dozens or even hundreds of newspapers; by small chains that own a handful of papers; or in a situation that is increasingly rare, by individuals or families.
Major cities often have "alternative weeklies" to complement the mainstream daily papers, for example, New York City's The Village Voice or Los Angeles' LA Weekly, to name two of the best-known.
Major cities may also support a local business journal, trade papers relating to local industries, and papers for local ethnic and social groups.
Early versions of the American newspaper comic strip and the American comic book began appearing in the 19th century.
In 1938, Superman, the comic book superhero of DC Comics, developed into an American icon.
[719] Aside from web portals and search engines, the most popular websites are Facebook, YouTube, Wikipedia, Yahoo!, eBay, Amazon, and Twitter.
More than 800 publications are produced in Spanish, the second most commonly used language in the United States behind English.
The CIA World Factbook lists the United States as the third-largest country (after Russia and Canada) with total area of 9,833,517 sq km,[19] and China as fourth-largest at 9,596,960 sq km.
[20] This figure for the United States is greater than in the Encyclopædia Britannica because it includes coastal and territorial waters.
Internet sources
