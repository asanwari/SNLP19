{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# SNLP - SoSe 2019 - ASSINGMENT v\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt \n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def word_ngrams(tokenized_text, n):\n",
    "    return [tuple(tokenized_text[i:i + n]) for i in range(0, len(tokenized_text)-n+1)]\n",
    "\n",
    "\n",
    "class ngram_LM:\n",
    "    \"\"\"A class to represent a language model.\"\"\"\n",
    "\n",
    "    def __init__(self, n, ngram_counts, vocab, unk=False):\n",
    "        \"\"\"\"Make a n-gram language model, given a vocab and\n",
    "            data structure for n-gram counts.\"\"\"\n",
    "\n",
    "        self.n = n\n",
    "\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.V = len(vocab)\n",
    "\n",
    "        self.ngram_counts = ngram_counts\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # START BY MAKING THE RIGHT COUNTS FOR THIS PARTICULAR self.n\n",
    "        # for unigrams, we only need total word count\n",
    "        if n == 1:\n",
    "            self.total_count = sum(self.ngram_counts.values())\n",
    "        # for bigrams, we need total count wrt each word. In our language, it is history count.\n",
    "        elif n == 2:\n",
    "            self.history_count = Counter()\n",
    "            for k, v in self.ngram_counts.items():\n",
    "                self.history_count[k[0]] = self.history_count[k[0]] + v\n",
    "            # since we only count for the first word in the tuple, we will always\n",
    "            # miss counting </s>. However, since the frequency of </s> is the same\n",
    "            # as the frequency of <s>, we can simply assign it equal to it.\n",
    "            self.history_count['</s>'] = self.history_count['<s>']\n",
    "\n",
    "\n",
    "\n",
    "    def estimate_prob(self, history, word):\n",
    "        \"\"\"Estimate probability of a word given a history.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        if history == '':\n",
    "            # unigram\n",
    "            word_frequency = self.ngram_counts[tuple([word])]\n",
    "            return word_frequency/self.total_count\n",
    "\n",
    "        else:\n",
    "            # bigram\n",
    "            word_frequency = self.ngram_counts[tuple([history, word])]\n",
    "            history_count = self.history_count[history]\n",
    "            if history_count == 0:\n",
    "                return 0\n",
    "            return word_frequency/history_count\n",
    "\n",
    "\n",
    "    def estimate_smoothed_prob(self, history, word, alpha = 0.5):\n",
    "        \"\"\"Estimate probability of a word given a history with Lidstone smoothing.\"\"\"\n",
    "\n",
    "        if history == '':\n",
    "            # unigram\n",
    "            word_frequency = self.ngram_counts[tuple([word])]\n",
    "            return (word_frequency + alpha)/(alpha*self.V +self.total_count)\n",
    "\n",
    "        else:\n",
    "            # bigram\n",
    "            word_frequency = self.ngram_counts[tuple([history, word])]\n",
    "            history_count = self.history_count[history]\n",
    "            return (word_frequency + alpha)/(alpha*self.V + history_count)\n",
    "\n",
    "\n",
    "    def logP(self, history, word):\n",
    "        \"\"\"Return base-2 log probablity.\"\"\"\n",
    "        prob = self.estimate_smoothed_prob(history, word)\n",
    "        log_prob = math.log(prob, 2)\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "    def score_sentence(self, sentence):\n",
    "        \"\"\"Given a sentence, return score.\"\"\"\n",
    "        log_prob_sum = 0\n",
    "        for i in range(len(sentence)):\n",
    "            history = sentence[i][0]\n",
    "            word = sentence[i][1]\n",
    "            log_prob = self.logP(history, word)\n",
    "            log_prob_sum += log_prob\n",
    "        normalized_log_prob_sum = (-1 / len(sentence)) * log_prob_sum\n",
    "        return normalized_log_prob_sum\n",
    "\n",
    "\n",
    "    def test_LM(self):\n",
    "        \"\"\"Test whether or not the probability mass sums up to one.\"\"\"\n",
    "\n",
    "        precision = 10**-8\n",
    "\n",
    "        if self.n == 1:\n",
    "\n",
    "            P_sum = sum(self.estimate_prob('', w) for w in self.vocab)\n",
    "\n",
    "            assert abs(1.0 - P_sum) < precision, 'Probability mass does not sum up to one.'\n",
    "\n",
    "        elif self.n == 2:\n",
    "            histories = ['the', 'in', 'at', 'blue', 'white']\n",
    "\n",
    "            for h in histories:\n",
    "\n",
    "                P_sum = sum(self.estimate_prob(h, w) for w in self.vocab)\n",
    "\n",
    "                assert abs(1.0 - P_sum) < precision, 'Probability mass does not sum up to one for history' + h\n",
    "\n",
    "        print('TEST SUCCESSFUL!')\n",
    "\n",
    "\n",
    "\n",
    "    def test_smoohted_LM(self):\n",
    "        \"\"\"Test whether or not the smoothed probability mass sums up to one.\"\"\"\n",
    "        precision = 10**-8\n",
    "\n",
    "        if self.n == 1:\n",
    "\n",
    "            P_sum = sum(self.estimate_smoothed_prob('', w) for w in self.vocab)\n",
    "\n",
    "            assert abs(1.0 - P_sum) < precision, 'Probability mass does not sum up to one.'\n",
    "\n",
    "        elif self.n == 2:\n",
    "            histories = ['the', 'in', 'at', 'blue', 'white']\n",
    "\n",
    "            for h in histories:\n",
    "\n",
    "                P_sum = sum(self.estimate_smoothed_prob(h, w) for w in self.vocab)\n",
    "\n",
    "                assert abs(1.0 - P_sum) < precision, 'Probability mass does not sum up to one for history' + h\n",
    "\n",
    "        print('TEST SUCCESSFUL!')\n",
    "\n",
    "\n",
    "    def perplexity(self, test_corpus, alpha):\n",
    "\n",
    "        likelihood = 0\n",
    "        for sentence in test_corpus:\n",
    "            try:\n",
    "                if self.n == 1:\n",
    "                    prob = self.estimate_smoothed_prob('', sentence[0], alpha)\n",
    "                elif self.n ==2:\n",
    "                    prob = self.estimate_smoothed_prob(sentence[0], sentence[1], alpha)\n",
    "                likelihood += math.log2(prob)\n",
    "            except:\n",
    "                if alpha == 0:\n",
    "                    continue \n",
    "\n",
    "        perplexity = math.pow(2, (-1*likelihood)/len(test_corpus))\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'sir', 'arthur', 'conan', 'doyle', 'in', 'our', 'series', 'by', 'sir', 'arthur', 'conan', 'doyle', 'copyright', 'laws', 'are', 'changing', 'all', 'over', 'the', 'world', 'be', 'sure', 'to', 'check', 'the', 'copyright', 'laws', 'for', 'your', 'country', 'before', 'downloading', 'or', 'redistributing', 'this', 'or', 'any', 'other', 'project', 'gutenberg', 'ebook', 'this', 'header', 'should', 'be', 'the', 'first', 'thing', 'seen', 'when', 'viewing', 'this', 'project', 'gutenberg', 'file', 'please', 'do', 'not', 'remove', 'it', 'do', 'not', 'change', 'or', 'edit', 'the', 'header', 'without', 'written', 'permission', 'please', 'read', 'the', 'legal', 'small', 'print', 'and', 'other', 'information', 'about', 'the', 'ebook', 'and', 'project', 'gutenberg', 'at', 'the', 'bottom', 'of']\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "filename= 'continuous.corpus.en'\n",
    "with open(filename, encoding='utf-8', errors='replace') as f:\n",
    "    # read entire file\n",
    "    text = f.read() \n",
    "\n",
    "# tokenize it\n",
    "tokenized_text = tokenize(text)\n",
    "# print first 100 tokens to verify\n",
    "print(tokenized_text[0:100])\n",
    "unigrams = Counter(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_PD(w1 , w2 , D, tokenized_text):\n",
    "    count = 0\n",
    "    N = len(tokenized_text)\n",
    "    for i in range(N-D):\n",
    "        if tokenized_text[i] == w1 and tokenized_text[i+D] == w2:\n",
    "            count = count + 1\n",
    "    return count/(N-D)\n",
    "\n",
    "# 1.1 b\n",
    "def correlation(w1 , w2 , D):\n",
    "    total_words = len(tokenized_text)\n",
    "    numerator = calculate_PD(w1, w2, D, tokenized_text) \n",
    "    denominator = (unigrams[w1]*unigrams[w2]) / (total_words * total_words)\n",
    "    return numerator/denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 c\n",
    "\n",
    "inputs = [('you','your'),('he','his'),('he','her'),('she','her'),('they','their'),('she','his'), ('i', 'my'), ('we', 'our')]\n",
    "data_points = {}\n",
    "D = 100\n",
    "for i in range(len(inputs)):\n",
    "    data_points[inputs[i]] = []\n",
    "    for d in range(1,D+1):\n",
    "        input_i = inputs[i]\n",
    "        data_points[input_i].append(correlation(input_i[0],input_i[1],d))\n",
    "        print('the correlation for \"{}\" and \"{}\" for D = {} is : {}'.format(inputi[0], inputi[1], d, data_points[input_i][d-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1.1 d\n",
    "running_avg = {}\n",
    "window_size = 5\n",
    "for input_i in inputs:\n",
    "    running_avg[input_i] = []\n",
    "    running_avg[input_i].append([np.mean(data_points[input_i][j:j+window_size]) for j in range(D - window_size)])\n",
    "\n",
    "\n",
    "Ds = [i for i in range(1, D-window_size+1)]\n",
    "for input_i, run_avg in running_avg.items():\n",
    "    plt.plot(Ds, run_avg[0])\n",
    "plt.legend(running_avg.keys())\n",
    "plt.xlabel('D')\n",
    "plt.ylabel('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
