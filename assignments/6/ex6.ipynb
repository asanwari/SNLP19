{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# SNLP - SoSe 2019 - ASSINGMENT VI\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt \n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def word_ngrams(tokenized_text, n):\n",
    "    return [tuple(tokenized_text[i:i + n]) for i in range(0, len(tokenized_text)-n+1)]\n",
    "\n",
    "\n",
    "class ngram_LM:\n",
    "    \"\"\"A class to represent a language model.\"\"\"\n",
    "\n",
    "    def __init__(self, n, ngram_counts, vocab, unk=False):\n",
    "        \"\"\"\"Make a n-gram language model, given a vocab and\n",
    "            data structure for n-gram counts.\"\"\"\n",
    "\n",
    "        self.n = n\n",
    "\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.V = len(vocab)\n",
    "\n",
    "        self.ngram_counts = ngram_counts\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # START BY MAKING THE RIGHT COUNTS FOR THIS PARTICULAR self.n\n",
    "        # for unigrams, we only need total word count\n",
    "        if n == 1:\n",
    "            self.total_count = sum(self.ngram_counts.values())\n",
    "        # for bigrams, we need total count wrt each word. In our language, it is history count.\n",
    "        elif n == 2:\n",
    "            self.history_count = Counter()\n",
    "            for k, v in self.ngram_counts.items():\n",
    "                self.history_count[k[0]] = self.history_count[k[0]] + v\n",
    "            # since we only count for the first word in the tuple, we will always\n",
    "            # miss counting </s>. However, since the frequency of </s> is the same\n",
    "            # as the frequency of <s>, we can simply assign it equal to it.\n",
    "            self.history_count['</s>'] = self.history_count['<s>']\n",
    "\n",
    "\n",
    "\n",
    "    def estimate_prob(self, history, word):\n",
    "        \"\"\"Estimate probability of a word given a history.\"\"\"\n",
    "        if history == '':\n",
    "            # unigram\n",
    "            word_frequency = self.ngram_counts[tuple([word])]\n",
    "            return word_frequency/self.total_count\n",
    "\n",
    "        else:\n",
    "            # bigram\n",
    "            word_frequency = self.ngram_counts[tuple([history, word])]\n",
    "            history_count = self.history_count[history]\n",
    "            if history_count == 0:\n",
    "                return 0\n",
    "            return word_frequency/history_count\n",
    "\n",
    "\n",
    "    def estimate_smoothed_prob(self, history, word, alpha = 0.5):\n",
    "        \"\"\"Estimate probability of a word given a history with Lidstone smoothing.\"\"\"\n",
    "\n",
    "        if history == '':\n",
    "            # unigram\n",
    "            word_frequency = self.ngram_counts[tuple([word])]\n",
    "            return (word_frequency + alpha)/(alpha*self.V +self.total_count)\n",
    "\n",
    "        else:\n",
    "            # bigram\n",
    "            word_frequency = self.ngram_counts[tuple([history, word])]\n",
    "            history_count = self.history_count[history]\n",
    "            return (word_frequency + alpha)/(alpha*self.V + history_count)\n",
    "\n",
    "    def getN1plus(self, word):\n",
    "        # currently implemented for N1+(.,w)\n",
    "        count = 0\n",
    "        for v in self.vocab:\n",
    "            if self.ngram_counts[tuple([v,word])] > 0:\n",
    "                count = count + 1\n",
    "        return count\n",
    "\n",
    "    \n",
    "    def kneser_Ney_smoothing(self, history, word):\n",
    "        # currently implemented only for history = ''\n",
    "        \n",
    "        # out of vocab case\n",
    "        if word not in self.vocab:\n",
    "            return 1/self.V\n",
    "        numerator = self.getN1plus(word)\n",
    "        denominator = len(self.ngram_counts)\n",
    "        return numerator/denominator\n",
    "    \n",
    "\n",
    "\n",
    "    def logP(self, history, word):\n",
    "        \"\"\"Return base-2 log probablity.\"\"\"\n",
    "        prob = self.estimate_smoothed_prob(history, word)\n",
    "        log_prob = math.log(prob, 2)\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "    def score_sentence(self, sentence):\n",
    "        \"\"\"Given a sentence, return score.\"\"\"\n",
    "        log_prob_sum = 0\n",
    "        for i in range(len(sentence)):\n",
    "            history = sentence[i][0]\n",
    "            word = sentence[i][1]\n",
    "            log_prob = self.logP(history, word)\n",
    "            log_prob_sum += log_prob\n",
    "        normalized_log_prob_sum = (-1 / len(sentence)) * log_prob_sum\n",
    "        return normalized_log_prob_sum\n",
    "\n",
    "\n",
    "    def test_LM(self):\n",
    "        \"\"\"Test whether or not the probability mass sums up to one.\"\"\"\n",
    "\n",
    "        precision = 10**-8\n",
    "\n",
    "        if self.n == 1:\n",
    "\n",
    "            P_sum = sum(self.estimate_prob('', w) for w in self.vocab)\n",
    "\n",
    "            assert abs(1.0 - P_sum) < precision, 'Probability mass does not sum up to one.'\n",
    "\n",
    "        elif self.n == 2:\n",
    "            histories = ['the', 'in', 'at', 'blue', 'white']\n",
    "\n",
    "            for h in histories:\n",
    "\n",
    "                P_sum = sum(self.estimate_prob(h, w) for w in self.vocab)\n",
    "\n",
    "                assert abs(1.0 - P_sum) < precision, 'Probability mass does not sum up to one for history' + h\n",
    "\n",
    "        print('TEST SUCCESSFUL!')\n",
    "\n",
    "\n",
    "\n",
    "    def test_smoohted_LM(self):\n",
    "        \"\"\"Test whether or not the smoothed probability mass sums up to one.\"\"\"\n",
    "        precision = 10**-8\n",
    "\n",
    "        if self.n == 1:\n",
    "\n",
    "            P_sum = sum(self.estimate_smoothed_prob('', w) for w in self.vocab)\n",
    "\n",
    "            assert abs(1.0 - P_sum) < precision, 'Probability mass does not sum up to one.'\n",
    "\n",
    "        elif self.n == 2:\n",
    "            histories = ['the', 'in', 'at', 'blue', 'white']\n",
    "\n",
    "            for h in histories:\n",
    "\n",
    "                P_sum = sum(self.estimate_smoothed_prob(h, w) for w in self.vocab)\n",
    "\n",
    "                assert abs(1.0 - P_sum) < precision, 'Probability mass does not sum up to one for history' + h\n",
    "\n",
    "        print('TEST SUCCESSFUL!')\n",
    "\n",
    "\n",
    "    def perplexity(self, test_corpus, alpha):\n",
    "\n",
    "        likelihood = 0\n",
    "        for sentence in test_corpus:\n",
    "            try:\n",
    "                if self.n == 1:\n",
    "                    prob = self.estimate_smoothed_prob('', sentence[0], alpha)\n",
    "                elif self.n ==2:\n",
    "                    prob = self.estimate_smoothed_prob(sentence[0], sentence[1], alpha)\n",
    "                likelihood += math.log2(prob)\n",
    "            except:\n",
    "                if alpha == 0:\n",
    "                    continue \n",
    "\n",
    "        perplexity = math.pow(2, (-1*likelihood)/len(test_corpus))\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "filename= 'corpus.sent.en.train'\n",
    "with open(filename, encoding='utf-8', errors='replace') as f:\n",
    "    # read entire file\n",
    "    text = f.read() \n",
    "\n",
    "# tokenize it\n",
    "tokenized_text = tokenize(text)\n",
    "unigrams = Counter(word_ngrams(tokenized_text,1))\n",
    "unigram_vocabulary = list(unigrams.keys())\n",
    "unigram_vocabulary = [i[0] for i in unigram_vocabulary]\n",
    "\n",
    "bigrams = Counter(word_ngrams(tokenized_text,2))\n",
    "bigram_vocabulary = [i for i in unigram_vocabulary]\n",
    "bigram_vocabulary.extend(['<s>','</s>'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------ For Word \"york\"------\n",
      "\n",
      "\n",
      "N(w) is 2374\n",
      "N1+(.,w) is 9\n",
      "-log2P_ML(w) is 12.258374283571047\n",
      "-log2PLids(w) is 12.268770360977008\n",
      "-log2Pkn(w) is 18.06307025575544\n",
      "\n",
      "\n",
      "------ For Word \"matter\"------\n",
      "\n",
      "\n",
      "N(w) is 2367\n",
      "N1+(.,w) is 269\n",
      "-log2P_ML(w) is 12.262634512478337\n",
      "-log2PLids(w) is 12.27302879345373\n",
      "-log2Pkn(w) is 13.16153289464113\n"
     ]
    }
   ],
   "source": [
    "words = ['york', 'matter']\n",
    "unigram_LM = ngram_LM(1, unigrams, unigram_vocabulary)\n",
    "bigram_LM = ngram_LM(2, bigrams, bigram_vocabulary)\n",
    "for word in words:\n",
    "    print('\\n\\n------ For Word \"{}\"------\\n\\n'.format(word))\n",
    "    print('N(w) is {}'.format(unigram_LM.ngram_counts[tuple([word])]))\n",
    "    print('N1+(.,w) is {}'.format(bigram_LM.getN1plus(word)))\n",
    "    \n",
    "    P_ML = unigram_LM.estimate_prob('',word)\n",
    "    log2P_ML = -math.log(P_ML, 2)\n",
    "    print('-log2P_ML(w) is {}'.format(log2P_ML))\n",
    "    \n",
    "    P_Lids = unigram_LM.estimate_smoothed_prob('',word,1)\n",
    "    log2P_Lids =  -math.log(P_Lids, 2)\n",
    "    print('-log2PLids(w) is {}'.format(log2P_Lids))\n",
    "    \n",
    "    P_kn = bigram_LM.kneser_Ney_smoothing('',word)\n",
    "    log2P_kn = -math.log(P_kn, 2)\n",
    "    print('-log2Pkn(w) is {}'.format(log2P_kn))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
